<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>pyinstaller编译py文件</title>
      <link href="/2023/06/07/pyinstaller%E7%BC%96%E8%AF%91py%E6%96%87%E4%BB%B6/"/>
      <url>/2023/06/07/pyinstaller%E7%BC%96%E8%AF%91py%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>要将Python脚本编译成可执行工具，可以使用pyinstaller库。pyinstaller可以将Python脚本打包成可执行文件，这样您就可以在没有Python解释器的情况下在其他计算机上运行它。</p><p>以下是使用pyinstaller将Python脚本编译成可执行工具的步骤：</p><p>安装pyinstaller库。在命令行中输入以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install pyinstaller</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">pip install pyinstaller==5.1</span></span><br></pre></td></tr></table></figure><p>在命令行中导航到包含Python脚本的目录。</p><p>使用以下命令将Python脚本编译为可执行文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller your_script.py</span><br></pre></td></tr></table></figure><p>这将在当前目录中创建一个dist文件夹，其中包含可执行文件和其他必要的文件。</p><p>运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./dist/your_script/your_script</span><br></pre></td></tr></table></figure><ol><li>openpyxl无法编译：<br>参考解决方案：<br><a href="https://stackoverflow.com/questions/74926850/how-to-solve-modulenotfounderror-no-module-named-openpyxl-cell-writer">https://stackoverflow.com/questions/74926850/how-to-solve-modulenotfounderror-no-module-named-openpyxl-cell-writer</a></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller YOUR_FILE.py --hidden-import openpyxl.cell._writer</span><br></pre></td></tr></table></figure><p>2.sklearn无法编译：</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zotero配置</title>
      <link href="/2023/06/06/zotero%E9%85%8D%E7%BD%AE/"/>
      <url>/2023/06/06/zotero%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://zhuanlan.zhihu.com/p/390596338">https://zhuanlan.zhihu.com/p/390596338</a></p><h2 id="Zotero的数据文件"><a href="#Zotero的数据文件" class="headerlink" title="Zotero的数据文件"></a>Zotero的数据文件</h2><p>Zotero的默认数据文件路径是”C:\Users\用户名\Zotero”，其中所有文献附件全部存放在storage文件夹中，所以我们只需同步这个文件夹就行。</p><h2 id="创建数据文件间的链接"><a href="#创建数据文件间的链接" class="headerlink" title="创建数据文件间的链接"></a>创建数据文件间的链接</h2><p>首先创建一台电脑上zotero数据文件和onedrive的链接关系，CMD命令（管理员权限打开）如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mklink /j &quot;E:\OneDrive\Zotero_storage&quot; &quot;C:\Users\用户名\Zotero\storage&quot;</span><br></pre></td></tr></table></figure><p>这样就在OneDrive中为storage文件创建了一个名为Zotero_storage的“入口”（该文件夹自动生成，不需要提前新建，否则会报错“当文件已存在时，无法创建该文件”）。该入口文件不占用具体的空间，对该入口文件夹的访问和内容修改都会被链接到对原始storage文件的访问和内容修改（但又不同于普通快捷方式，普通快捷方式在onedrive中无法同步相应内容）。</p><p>接着在另一台电脑上创建onedrive上Zotero_storage文件和的本地zotero数据文件的链接关系。首先需要删除本地Zotero文件中的storage文件（路径”C:\Users\用户名\Zotero\storage” ），然后用Mklink指令创建一个onedrive中数据文件的入口替代。CMD指令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mklink /j &quot;C:\Users\用户名\Zotero\storage&quot; &quot;C:\Users\用户名\OneDrive\Zotero_storage&quot;</span><br></pre></td></tr></table></figure><p>至此实现了两台电脑间的文献库同步</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>R_issues</title>
      <link href="/2023/03/08/R-issues/"/>
      <url>/2023/03/08/R-issues/</url>
      
        <content type="html"><![CDATA[<h3 id="1-In-system-cmd-‘make’-not-found��Error-1-occurred-building-shared-library"><a href="#1-In-system-cmd-‘make’-not-found��Error-1-occurred-building-shared-library" class="headerlink" title="1.In system(cmd) : ‘make’ not found��Error 1 occurred building shared library."></a>1.In system(cmd) : ‘make’ not found��Error 1 occurred building shared library.</h3><p>Warning message:<br>In system(cmd) : ‘make’ not found<br>Error in Rcpp::sourceCpp(“./BeiHang/bayesRB-main/src/BayesRB.cpp”) :<br>  Error 1 occurred building shared library.<br>In addition: Warning messages:<br>1: R graphics engine version 15 is not supported by this version of RStudio. The Plots tab will be disabled until a newer version of RStudio is installed.<br>2: In normalizePath(path.expand(path), winslash, mustWork) :<br>  path[1]=”D:/OneDrive/code/GZU/BeiHang/bayesRB-main/src/../inst/include”: ϵͳ�Ҳ���ָ����·����</p><blockquote><p>Sys.setenv(PATH = paste(“C:\RBuildTools\rtools42\usr\bin”, Sys.getenv(“PATH”), sep=”;”))<br>Sys.setenv(PATH = paste(“C:\RBuildTools\rtools42”, Sys.getenv(“PATH”), sep=”;”))</p></blockquote><h3 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h3><p>install.packages(‘devtools’, repos=’<a href="http://cran.rstudio.com/">http://cran.rstudio.com/</a>‘)</p><h3 id="3-g-����include��λ�a�����gsl"><a href="#3-g-����include��λ�a�����gsl" class="headerlink" title="3.g++����include��λ�ã�����gsl"></a>3.g++����include��λ�ã�����gsl</h3><p><a href="https://blog.csdn.net/weixin_44759449/article/details/124944977">https://blog.csdn.net/weixin_44759449/article/details/124944977</a><br>������C++����ʱ��ÿ�ζ���Ҫ���������ֶ�����-I��������鷳�����ֻ��Ҫ���û���������<br>CPLUS_INCLUDE_PATH=�����include·����,����ʵ��Ϊg++����������Ĭ�ϵ�include·����</p><h3 id="R������ͼ"><a href="#R������ͼ" class="headerlink" title="R������ͼ"></a>R������ͼ</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.jb51.net/article/252887.htm</span></span><br><span class="line">library<span class="punctuation">(</span>installr<span class="punctuation">)</span></span><br><span class="line">install.packages<span class="punctuation">(</span><span class="string">&quot;ggcorrplot&quot;</span><span class="punctuation">)</span></span><br><span class="line">install.package<span class="punctuation">(</span><span class="string">&quot;corrplot&quot;</span><span class="punctuation">)</span></span><br><span class="line">install.packages<span class="punctuation">(</span><span class="string">&quot;openxlsx&quot;</span><span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>openxlsx<span class="punctuation">)</span></span><br><span class="line">xl_data <span class="operator">=</span> read.xlsx<span class="punctuation">(</span><span class="string">&#x27;./xl_data.xlsx&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># �������⣬����һ����Ϊindex</span></span><br><span class="line">rownames<span class="punctuation">(</span>xl_data<span class="punctuation">)</span> <span class="operator">&lt;-</span> xl_data<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">xl_data <span class="operator">&lt;-</span> xl_data<span class="punctuation">[</span><span class="punctuation">,</span> <span class="operator">-</span><span class="number">1</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ����һ</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ���ݳ�index head��ȫ����ֵ���ͣ����Կ�ʼ��ͼ</span></span><br><span class="line">ggcorrplot<span class="punctuation">(</span>xl_data<span class="punctuation">,</span> method <span class="operator">=</span> <span class="string">&quot;circle&quot;</span><span class="punctuation">,</span> lab<span class="operator">=</span><span class="built_in">T</span><span class="punctuation">)</span></span><br><span class="line">ggcorrplot<span class="punctuation">(</span>xl_data<span class="punctuation">,</span> method <span class="operator">=</span> <span class="string">&quot;circle&quot;</span><span class="punctuation">,</span> lab<span class="operator">=</span><span class="built_in">T</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&#x27;upper&#x27;</span><span class="punctuation">,</span> lab_size <span class="operator">=</span> <span class="number">2.5</span><span class="punctuation">,</span>ggtheme <span class="operator">=</span> theme_bw<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">,</span>colors <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;#6D9EC1&quot;</span><span class="punctuation">,</span><span class="string">&quot;white&quot;</span><span class="punctuation">,</span><span class="string">&quot;#E46726&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> hc.order <span class="operator">=</span> <span class="built_in">T</span><span class="punctuation">,</span>hc.method <span class="operator">=</span> <span class="string">&quot;ward.D&quot;</span><span class="punctuation">,</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ������</span></span><br><span class="line">library<span class="punctuation">(</span>corrplot<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># xl_data��ҪתΪmatrix</span></span><br><span class="line">M <span class="operator">&lt;-</span> as.matrix<span class="punctuation">(</span>xl_data<span class="punctuation">)</span></span><br><span class="line">corrplot.mixed<span class="punctuation">(</span>M<span class="punctuation">,</span> order <span class="operator">=</span> <span class="string">&#x27;AOE&#x27;</span><span class="punctuation">)</span></span><br><span class="line">corrplot.mixed<span class="punctuation">(</span>M<span class="punctuation">,</span> lower <span class="operator">=</span> <span class="string">&#x27;number&#x27;</span><span class="punctuation">,</span> upper <span class="operator">=</span> <span class="string">&#x27;circle&#x27;</span><span class="punctuation">)</span></span><br><span class="line">corrplot.mixed<span class="punctuation">(</span>M<span class="punctuation">,</span> lower <span class="operator">=</span> <span class="string">&#x27;number&#x27;</span><span class="punctuation">,</span> upper <span class="operator">=</span> <span class="string">&#x27;pie&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>word目录制表符前导符</title>
      <link href="/2023/02/28/word%E7%9B%AE%E5%BD%95%E5%88%B6%E8%A1%A8%E7%AC%A6%E5%89%8D%E5%AF%BC%E7%AC%A6/"/>
      <url>/2023/02/28/word%E7%9B%AE%E5%BD%95%E5%88%B6%E8%A1%A8%E7%AC%A6%E5%89%8D%E5%AF%BC%E7%AC%A6/</url>
      
        <content type="html"><![CDATA[<p>WORD生成目录只有一级标题或部分标题没有制表符前导符（页码前面的……….）</p><p>直接解决方案：</p><p>点击引用——目录——自定义目录——修改——点击对应级别——格式——选择制表符——全部删除。</p><p>更新目录即可。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>设置mendeley参考文献引用格式</title>
      <link href="/2023/02/08/%E8%AE%BE%E7%BD%AEmendeley%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%BC%95%E7%94%A8%E6%A0%BC%E5%BC%8F/"/>
      <url>/2023/02/08/%E8%AE%BE%E7%BD%AEmendeley%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%BC%95%E7%94%A8%E6%A0%BC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>转至该网站进行设置： <a href="https://csl.mendeley.com/visualEditor/">https://csl.mendeley.com/visualEditor/</a></p><p>几个GB/T7714 csl文件<br><a href="https://github.com/redleafnew/Chinese-std-GB-T-7714-related-csl">https://github.com/redleafnew/Chinese-std-GB-T-7714-related-csl</a></p><p><a href="https://github.com/redleafnew/Chinese-STD-GB-T-7714-related-csl">https://github.com/redleafnew/Chinese-STD-GB-T-7714-related-csl</a></p><p><a href="https://zhuanlan.zhihu.com/p/62396113">https://zhuanlan.zhihu.com/p/62396113</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>生成requirements.txt</title>
      <link href="/2023/01/30/%E7%94%9F%E6%88%90requirements-txt/"/>
      <url>/2023/01/30/%E7%94%9F%E6%88%90requirements-txt/</url>
      
        <content type="html"><![CDATA[<p>方案1 </p><p>pip list &gt;.&#x2F;requirements</p><p>方案2<br>命令行进到你所在的项目根目录</p><p>运行：pipreqs .&#x2F; –encoding&#x3D;utf-8</p><p>然后你就会看到这个项目所有以来环境的requirements.txt了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>android-windows transmit</title>
      <link href="/2023/01/25/android-windows-transmit/"/>
      <url>/2023/01/25/android-windows-transmit/</url>
      
        <content type="html"><![CDATA[<p>1.局域网ip ping通</p><p>2.建立用户访问，设置共享文件夹和本地用户</p><p>3.设置访问权限，打开需要共享文件的属性，选择“安全”，点击“编辑”，点击“添加”，输入“everyone”，确定。一路确定。就可以解决问题了。换句话说，在安全里只要有everyone，就可以用es浏览器访问浏览电脑里的共享文件夹了。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>认识细胞生物学</title>
      <link href="/2022/12/22/%E8%AE%A4%E8%AF%86%E7%BB%86%E8%83%9E%E7%94%9F%E7%89%A9%E5%AD%A6/"/>
      <url>/2022/12/22/%E8%AE%A4%E8%AF%86%E7%BB%86%E8%83%9E%E7%94%9F%E7%89%A9%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<h2 id="认识细胞"><a href="#认识细胞" class="headerlink" title="认识细胞"></a>认识细胞</h2><p>肉眼能分辨的最小值0.2毫米（20N0微米），细胞为20微米，细胞器为200纳米-2微米，光学显微镜能分辨的最小值。分子2nm,原子0.2nm,电子显微镜能分辨的最小值是0.2nm。</p><p>细胞核周围是细胞质。细胞类型中，细菌的结构最简单。细胞中有细胞核的生物被称为真核生物，没有核的成为原核生物。原核生物是最多样化的细胞，大部分原核生物以单细胞形式生活。细菌虽然形态结构简单，但其化学性多样化且具有创造性。真核细胞中的线粒体是由好氧细菌进化而来，而叶绿体是由光合细菌进化而来。原核生物分为真细菌和古细菌。日常生活中存在以及致病的都是真细菌，古细菌是生存于古老地球存在的环境相似的细菌。</p><p>细胞核是真核细胞的信息存储器。细胞核中含有DNA分子，DNA形成染色体。线粒体基本存在于一切真核细胞中，线粒体含有它们自己的DNA，并以一分为二的方式增值，由细菌衍生而来，宿主真核细胞与被吞入细菌建立共生关系，线粒体内膜折叠，包含了担负细胞呼吸作用的大部分蛋白质，高度折叠提供了巨大面积，线粒体通过离心机分离，线粒体是细胞的化学能发生器，产生腺苷二磷酸ATP，ATP是驱动细胞大多数活动的化学燃料，在此过程中，线粒体消耗氧并释放二氧化碳，整过过程被称为细胞呼吸。</p><p>叶绿体只在植物和藻类细胞中出现，具有比线粒体更加复杂的结构，除了周围的两层膜以外，叶绿体内还有含绿色的色素即叶绿素的膜的堆积。叶绿体也含有自己的DNA，通过分裂为二来繁殖自己。</p><p>真核细胞中的膜被细胞器不只有细胞核、线粒体和叶绿体。内质网是由膜包围的具有内部相通空间的不规则迷宫，是制造大部分细胞膜成分以及最后输出细胞物质的地方。</p><p>在内质网、高尔基体、溶酶体以及细胞外围之间不断地进行着物质交换。</p><p>在电子显微镜下可以看到真核细胞内又细又长地蛋白质丝纵横交错地分布在胶质溶液中，这个纤维蛋白质丝构成地系统被称为细胞骨架，最细的丝是肌动蛋白丝，一切真核细胞都具有肌动蛋白丝。最粗的丝称为微管，中间的丝称为中等纤维，三种纤维以及连成的蛋白质控制细胞的形状并且驱动和引导细胞运动。</p><p>细胞主要含有四种有机小分子：糖、氨基酸、核苷酸、脂肪酸。糖是细胞的能源，也是多糖的亚基；脂肪酸是细胞膜的组分；氨基酸是蛋白质的基本构成单位，蛋白质中有20种氨基酸；核苷酸是DNA及RNA的亚基，在细胞中，核苷酸最基本的功能是生物信息的储存和读取。核苷酸是构成核酸的结构单元。</p>]]></content>
      
      
      <categories>
          
          <category> 细胞生物学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UKB影像数据下载</title>
      <link href="/2022/12/22/UKB%E5%BD%B1%E5%83%8F%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD/"/>
      <url>/2022/12/22/UKB%E5%BD%B1%E5%83%8F%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<p>1.使用dnanexous平台<br>需要使用dx工具进行下载。</p><p>1.ntfs盘挂载到linux</p><p>使用rsync命令很多参数都不能使用，因此无法拷贝，能用的参数 -aP</p><p>查看linux下盘的分区、格式命令：</p><p>df -T</p><p>将分区&#x2F;dev&#x2F;sdj1 格式化为ext4格式</p><p>mkfs -t ext4 &#x2F;dev&#x2F;sdj1</p><p>然后就能愉快的使用了。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基因组单倍型组装与应用</title>
      <link href="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
      <url>/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><p>haplotype单倍型 haplotype assembly单倍型组装 decontaminate去污，净化  diploid二倍体</p><ul><li>1.More complete genomics 背景</li><li>2.Evolution of sequencing technologies 那些信息可以让我们进行单倍型组装</li><li>3.Reference-based haplotype reconstruction 单倍型组装策略（有参）</li><li>4.De nove haplotype assembly 单倍型组装策略（无参）</li><li>5.Trio-bining haplotype assembly 最近无参中比较火的策略</li><li>6.Trio-based decontami-nation 基于Trio-binig策略的延申，序列去污染的一种方法</li></ul><h2 id="More-complete-genomics"><a href="#More-complete-genomics" class="headerlink" title="More complete genomics"></a>More complete genomics</h2><h3 id="Basis-of-Genomics-Assembly"><a href="#Basis-of-Genomics-Assembly" class="headerlink" title="Basis of Genomics - Assembly"></a>Basis of Genomics - Assembly</h3><p>基因组组装是生物分析的一个基础，基因组组装取得比较好的质量，才能在下游分析中取得较好的准度和精度，比如比较基因组。人类基因组计划开始就希望通过计算机的算法将测序打断的序列合成一个完整的染色体，时至今日，依旧没有完美的染色体。</p><p>Sequencing打断 ——&gt; Quality control ——&gt; Assembly ——&gt; Annotation ——&gt;Comparison</p><h3 id="More-complete-genomes"><a href="#More-complete-genomes" class="headerlink" title="More complete genomes"></a>More complete genomes</h3><p>随着算法进步，获得完美基因组成为了可能。现在就是希望结合不同的测序数据序列，利用它们的优势，再结合一些算法，去共同解决两大问题，一种是完整性，一种是单倍型，最阻碍我们的两个问题是重复和杂合。这篇文章又指出，如果有了完美单倍型的完美基因组，我们可以做什么，比如结构变异注释、结构变异延伸到复杂稀有结构区等。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/complete.png" alt="complete.png"></p><p>现状<br>T2T基因组，指通过ONT ultra-long N50&gt;100Kb结合HiFi和二代数据进行混合组装，得到的有一条或者多条染色体达到端粒到端粒（Telomere-to-Telomere）的水平基因组，T2T基因组完成图是基因组组装的终极目标。基因组图谱的绘制，包括以前发表的相关研究，解决了人类遗传多样性的各个方面，包括人类与灵长类动物的进化比较。此外，基因组图谱可用于识别着丝粒内和着丝粒之间甲基化丰度的变化如何不同，以及表观遗传学如何影响重复序列的转录。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/%E7%8E%B0%E7%8A%B6.png" alt="现状.png"></p><p>T2T consortium这样说：The first sequence of a complete human genome is imminent, and the next challenge will be applying the methods to fully phase and assemble diploid genomes.即，完美基因组序列已经可以实现，然后新的挑战是组装二倍体基因组。</p><h2 id="Evolution-of-sequencing-technologies"><a href="#Evolution-of-sequencing-technologies" class="headerlink" title="Evolution of sequencing technologies"></a>Evolution of sequencing technologies</h2><p>单倍型定义（Haplotyping or Phasing）,如下图，左为宏观、右为微观。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/haplotype.png" alt="haplotype.png"><br>重要意义：医学上探索致病机理、挖掘致病基因、寻找疾病治疗新方法；群体遗传学上分析等位基因间差异，追踪个体亲缘关系，了解生物迁移模式和进化历史；农作物的遗传发育方面，发掘优异等位基因变异、探索杂种优势理论；基因组组装领域。</p><p>几乎所有的数据都能用于单倍体组装，利用它们的各自优势。根据RNA-seq测序技术的不同，可以分为三种，short-read、long-read、direct RNA-seq。Hi-C方法是全基因组范围内，不限制特定互作蛋白为局限，利用高通量测序（二代或三代）的基于“全对全”模式的染色质互作构象分析方法。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/sequencing.png" alt="sequencing.png"></p><p>单倍体组装从方法上来说，可以分为两类：参考类，有reference；组装的,reference有无均可，对深度有一定的要求比如&gt;50x，根据结果可以分为两种序列，第一种是 一条primary序列是一个比较完整的序列，另一条是含有等位基因的位点把它输出；第二种是同时输出两条完整的基因组序列，这是我们希望的。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/mehods.png" alt="mehods.png"></p><h2 id="Reference-based-haplotype-reconstruction"><a href="#Reference-based-haplotype-reconstruction" class="headerlink" title="Reference-based haplotype reconstruction"></a>Reference-based haplotype reconstruction</h2><p>二倍体型Phasing, 所谓Phasing就是要把一个二倍体（甚至是多倍体）基因组上的等位基因（或者杂合位点），按照其亲本正确地定位到父亲或者母亲的染色体上，最终使得所有来自同一个亲本的等位基因都能够排列在同一条染色体里面。<br>多倍体phasing比较复杂，复杂度为k！阶乘。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/diploid.png" alt="diploid.png"></p><h2 id="De-nove-haplotype-assembly-单倍型组装策略（无参）"><a href="#De-nove-haplotype-assembly-单倍型组装策略（无参）" class="headerlink" title="De nove haplotype assembly 单倍型组装策略（无参）"></a>De nove haplotype assembly 单倍型组装策略（无参）</h2><p>二倍体的单倍型组装按照算法可以分为四大类：collapsed assembly、semi-collapsed assembly、hybrid uncollapsed assembly、Trio long-read assembly</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/denove.png" alt="denove.png"></p><p>二倍体的单倍型组装遇到的难题，多对多的比对，snp分配问题。</p><p>例子：<br>HiFi reads（High Fidelity reads）是2019年由PacBio推出的基于环化共有序列（Circular Consensus Sequencing，CCS）模式产生的既兼顾长读长（10-20kb的长度）又具有高精度（&gt;99%准确率）的测序结果。利用HiFi+Hi-C数据：<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/denove1.png" alt="denove1.png"></p><p>传统单细胞测序是指转录组，下例是指基因组：也是先去组装一个染色体级别的target,然后利用strand-seq提供的长程信息进行组装：<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/denove2.png" alt="denove2.png"></p><h2 id="Trio-bining-haplotype-assembly"><a href="#Trio-bining-haplotype-assembly" class="headerlink" title="Trio-bining haplotype assembly"></a>Trio-bining haplotype assembly</h2><p>该文章以牛与牛的杂交，左边是口感好的牛，右边是婆罗门牛，来自印度，可能由于印度环境比较特殊，因此印度牛体质比较好，但是口感差，因此进行杂交获得口感和抗病性，现在已经实现。</p><p>k-mer 是指将reads分成包含k个碱基的字符串,一般长短为m的reads可以分成m-k+1个k-mers，即长度为k的短序列DNA片段。</p><p>蓝色为安斯牛特有-kmers，红色为婆罗门牛特有kmers，灰色为共有，因为是同一物种，所以共有的kmers很多。根据这些少量的特异亲本kmers，我们就可以标记三代(家系)reads中特异的snp位点，我们就可以将read中的样本标记为父本和母本，按照attribution的snp，划分为父本和母本进行组装，就接下来就是T2T的情况，变成了完全单倍体的情况，这就是trio-bining策略。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/Trio.png" alt="Trio.png"></p><p>什么样的情况可以使用trio binning<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/Trio2.png" alt="Trio2.png"></p><p>stLFR(single tube Long Fragment Read): 单管长片段序列，是由华大制造自主研发的基于DNBSEQ平台的，一种长片段读取技术，可实现读取序列的长度高达10k~300k<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/Trio3.png" alt="Trio3.png"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/Trio4.png" alt="Trio4.png"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/conclusion.png" alt="conclusion.png"></p><p>挑战与未来<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/challenge.png" alt="challenge.png"></p><p>报告作者<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/02/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8D%95%E5%80%8D%E5%9E%8B%E7%BB%84%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/%E6%8A%A5%E5%91%8A%E4%BD%9C%E8%80%85.png" alt="报告作者"></p>]]></content>
      
      
      <categories>
          
          <category> 生物信息学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物 </tag>
            
            <tag> 基因组学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>整合多组学方法研究基因组调控及演化</title>
      <link href="/2022/11/02/%E6%95%B4%E5%90%88%E5%A4%9A%E7%BB%84%E5%AD%A6%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6%E5%9F%BA%E5%9B%A0%E7%BB%84%E8%B0%83%E6%8E%A7%E5%8F%8A%E6%BC%94%E5%8C%96/"/>
      <url>/2022/11/02/%E6%95%B4%E5%90%88%E5%A4%9A%E7%BB%84%E5%AD%A6%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6%E5%9F%BA%E5%9B%A0%E7%BB%84%E8%B0%83%E6%8E%A7%E5%8F%8A%E6%BC%94%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>中山大学副教授 李彩，09-15老华大，哥本哈根博士</p>]]></content>
      
      
      <categories>
          
          <category> 生物信息学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物 </tag>
            
            <tag> 基因组学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>肿瘤基因组学分析</title>
      <link href="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/"/>
      <url>/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>ENCODE项目：由美国国家人类基因组研究所2003年9月发起的公共联合研究项目，旨在找出人类基因组中所有的功能组件。</p><h2 id="变异来源"><a href="#变异来源" class="headerlink" title="变异来源"></a>变异来源</h2><p>比对到参考基因组的序列，检测出的变异来源大致可分为：</p><ul><li>测序错误</li><li>胚系突变</li><li>体细胞突变</li></ul><p>体突变检测要解决的问题实质为：<br>区分检出的变异是胚胎突变，体细胞突变，还是测序错误。</p><p>绝大多数情况下，肿瘤样本同时混有肿瘤细胞和正常细胞；依靠最小reads支持数（minimal mutation reads）可过滤部分测序错误。DOi:10.1186&#x2F;gm524</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E5%8F%98%E5%BC%82%E6%9D%A5%E6%BA%90.png" alt="变异来源"></p><p>体细胞突变（Somatic mutation）与胚系突变（Germline mutation）的区别,主要是看其是否有可遗传性<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/germline.png" alt="germiline"></p><h2 id="变异类型"><a href="#变异类型" class="headerlink" title="变异类型"></a>变异类型</h2><p>Single-nucleotide variants (SNV) and indels are the most common type of variants called by most variant callers, and these variants are prevalent in many important cancer genes. </p><p>变异类型主要分为三种：点突变、多个核酸突变、大片段的突变；SNV、Structural variation(CNV，CNV通常&gt;1kb)、MNV(multiple nucleotide variation),MNV是同一个<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E5%8F%98%E5%BC%82%E7%B1%BB%E5%9E%8B.png" alt="变异类型"></p><p>多核苷酸变体 (MNV) 定义为同一单倍型上同一密码子内的两个或多个变体</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/mnv.png" alt="mnv"></p><p>SNV根据对蛋白质的影响定义为不同的突变类型:No mutation无突变、Silent对蛋白质没有影响的突变、Nonsense突变会导致蛋白质的转录或翻译中止、Missense错义突变，保守错译突变对蛋白质结构没有太大变异，对功能不会有太大影响，比如亲水性等特性，非保守突变会导致蛋白质结构很大的变化，可能有较大危害。</p><p>Indel是突变时，在基因或染色体上有小片段的插入和缺失、或者是两种情况同时产生</p><p>CNV：片段duplication</p><p>Structure Variation</p><p>突变类型详细分类网站：<a href="http://www.sequenceontology.org/browser/obob.cgi">http://www.sequenceontology.org/browser/obob.cgi</a></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/variation.png" alt="variation"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95.png" alt="检测方法"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E5%B7%A5%E5%85%B7.png" alt="工具"><br>该工具示例图：<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E7%BB%93%E6%9E%9C.png" alt="结果"></p><p>软件流程集成，基本是按照投票原则进行集成开发的<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E8%87%AA%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7.png" alt="自构建工具"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C.png" alt="测试结果"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E6%B5%81%E7%A8%8B%E5%8F%82%E6%95%B0.png" alt="流程参数"></p><h2 id="变异位点注释"><a href="#变异位点注释" class="headerlink" title="变异位点注释"></a>变异位点注释</h2><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/Annovar.png" alt="Annovar"></p><h2 id="肿瘤基因组分析"><a href="#肿瘤基因组分析" class="headerlink" title="肿瘤基因组分析"></a>肿瘤基因组分析</h2><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/analyse.jpg" alt="analyse"></p><h3 id="变异特征及致癌机制（TMB-Signature）"><a href="#变异特征及致癌机制（TMB-Signature）" class="headerlink" title="变异特征及致癌机制（TMB, Signature）"></a>变异特征及致癌机制（TMB, Signature）</h3><p>肿瘤突变负荷（tumor mutation burden, TMB）是指肿瘤基因组去除胚系突变后的体细胞突变数量。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/%E7%AA%81%E5%8F%98%E8%B4%9F%E8%8D%B7.png" alt="突变负荷.png"></p><p>Mutational Signature(译为突变特征、突变信号等)</p><p>SNV突变中，有ATCG四种碱基，各自可突变为另外3种碱基，比如C&gt;A, C&gt;G, C&gt;T;因此碱基共$3 \times 4 &#x3D; 12$种，根据双链互补原则，C&gt;A等价于G&gt;T，因此一共有6种基础突变。如果考虑前后碱基，就共有96种：$单碱基突变6种 \times 前1bp的4种碱基 \times 后1bp的4种碱基&#x3D;96$，如果考虑前后两个碱基的话，就有1536种。一般研究96种。</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/signature.png" alt="signature"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/signature_analyse.png" alt="signature_analyse.png"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/indel.png" alt="indel.png"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/cnv.png" alt="cnv.png"></p><h3 id="Drivergene肿瘤驱动基因"><a href="#Drivergene肿瘤驱动基因" class="headerlink" title="Drivergene肿瘤驱动基因"></a>Drivergene肿瘤驱动基因</h3><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/drivegene.png" alt="drivegene.png"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/driversnv.png" alt="driversnv.png"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/snvdriever.png" alt="snvdriever.png"></p><h3 id="通路富集pathway"><a href="#通路富集pathway" class="headerlink" title="通路富集pathway"></a>通路富集pathway</h3><p>找出gene之后，就可以做通路分析<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/pathway.png" alt="pathway.png"></p><h3 id="肿瘤异质性和克隆演化"><a href="#肿瘤异质性和克隆演化" class="headerlink" title="肿瘤异质性和克隆演化"></a>肿瘤异质性和克隆演化</h3><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/clone.png" alt="clone.png"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/clone2.png" alt="clone2.png"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/clone3.png" alt="clone3.png"></p><h3 id="肿瘤免疫基因组和微环境研究"><a href="#肿瘤免疫基因组和微环境研究" class="headerlink" title="肿瘤免疫基因组和微环境研究"></a>肿瘤免疫基因组和微环境研究</h3><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/env.png" alt="env"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/env2.png" alt="env2"></p><h3 id="病毒"><a href="#病毒" class="headerlink" title="病毒"></a>病毒</h3><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/hbv.png" alt="hbv"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/hbv2.png" alt="hbv2"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/hpv.png" alt="hpv"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/epv.png" alt="epv"></p><h3 id="分子分型"><a href="#分子分型" class="headerlink" title="分子分型"></a>分子分型</h3><p>分子分型一般在多组学研究，<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/fenzi.png" alt="fenzi.png"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/fenzi2.png" alt="fenzi2.png"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/fenzi3.png" alt="fenzi3.png"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/01/%E8%82%BF%E7%98%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E5%88%86%E6%9E%90/fenzi4.png" alt="fenzi4.png"></p>]]></content>
      
      
      <categories>
          
          <category> 生物信息学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物 </tag>
            
            <tag> 基因组学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS_learn</title>
      <link href="/2022/10/26/CSS-learn/"/>
      <url>/2022/10/26/CSS-learn/</url>
      
        <content type="html"><![CDATA[<h2 id="CSS-Introduction"><a href="#CSS-Introduction" class="headerlink" title="CSS Introduction"></a>CSS Introduction</h2><p>CSS : Cascading Style Sheet</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/* Css Syntax: </span></span><br><span class="line"><span class="comment">    selector&#123;</span></span><br><span class="line"><span class="comment">    Declaration;  Declaration-&gt; color: blue; Property:value;</span></span><br><span class="line"><span class="comment">    Declaration;</span></span><br><span class="line"><span class="comment">    &#125; */</span></span><br><span class="line">  <span class="comment">/* we can divide CSS selector into five categories:</span></span><br><span class="line"><span class="comment">    Simple selectors(name, id, class)</span></span><br><span class="line"><span class="comment">    Combinator selectors </span></span><br><span class="line"><span class="comment">    Pseudo-class selectors</span></span><br><span class="line"><span class="comment">    Pseudo-elements selectors</span></span><br><span class="line"><span class="comment">    Attribute selectors*/</span></span><br><span class="line"><span class="selector-tag">p</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgb</span>(<span class="number">6</span>, <span class="number">70</span>, <span class="number">110</span>);</span><br><span class="line">    <span class="attribute">font-size</span>: large;</span><br><span class="line">    <span class="attribute">text-align</span>: center;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/* id selector */</span></span><br><span class="line">  <span class="selector-id">#para1</span> &#123;</span><br><span class="line">    <span class="attribute">text-align</span>: left;</span><br><span class="line">    <span class="attribute">color</span>: red;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/* class Selector */</span></span><br><span class="line">  <span class="selector-class">.center</span> &#123;</span><br><span class="line">    <span class="attribute">text-align</span>: left;</span><br><span class="line">    <span class="attribute">color</span>: brown;</span><br><span class="line">  &#125;</span><br><span class="line">   <span class="comment">/* specific class selector,only use in elements p*/</span></span><br><span class="line">  <span class="selector-tag">p</span><span class="selector-class">.center</span> &#123;</span><br><span class="line">    <span class="attribute">text-align</span>: center;</span><br><span class="line">    <span class="attribute">color</span>: red;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="/2022/10/26/CSS-learn/selector.png" alt="selector"></p><p>There are three ways of inserting a style sheet:</p><ul><li>External CSS: The external.css file should not contain any Html tags<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mystyle.css is a file --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">href</span>=<span class="string">&quot;mystyle.css&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>Internal CSS: The internal style is defined inside the  &lt;style&gt;, inside the head section.<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-tag">body</span> &#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">background-color</span>: linen;</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>Inline CSS: An inline style may be used to apply a unique style for a single element.<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">style</span>=<span class="string">&quot;cololr: blue; text-alian: center;&quot;</span>&gt;</span> this is a heading<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><p>If some properties have been defined for the same selector in different style sheets, the value from the last read style sheet will be used.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul><li>background-color</li><li>opacity 不透明度[0,1], the lower value, the more transparent.</li><li>background-image: url(“image.jpg”)（默认整个背景由图像重复填充铺满）</li><li>background-repeat: repeat-x（水平方向repeat） no-repeat（不重复）</li><li>background-position: right top；</li><li>background-attachment: fixed(固定) scroll(滚动)；指定背景图片滚动还是固定，指固定在识图，页面滚动，图片不会滚动。<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">style</span>=<span class="string">&quot;background-color:DodgerBlue;&quot;</span>&gt;</span>Hello World<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">&quot;background-color:Tomato;&quot;</span>&gt;</span>Lorem ipsum...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- text --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">style</span>=<span class="string">&quot;color:Tomato;&quot;</span>&gt;</span>Hello World<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Border Color 边框--solid：实线0，&gt;</span></span><br><span class="line"><span class="comment"> &lt;h1 style=&quot;border:10px solid Blue;&quot;&gt;Hello World&lt;/h1&gt;</span></span><br><span class="line"><span class="comment"> body&#123;</span></span><br><span class="line"><span class="comment">  background-image: url(&quot;img_tree.png&quot;);</span></span><br><span class="line"><span class="comment">  background-repeat: no-repeat;</span></span><br><span class="line"><span class="comment">  background-position: right top;</span></span><br><span class="line"><span class="comment">  background-attachment: fixed; #scorll</span></span><br><span class="line"><span class="comment">   background: #ffffff url(&quot;img_tree.png&quot;) no-repeat right top;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br></pre></td></tr></table></figure><img src= "/img/loading.gif" data-lazy-src="/2022/10/26/CSS-learn/backgrond.png" alt="background"></li></ul><h2 id="Border"><a href="#Border" class="headerlink" title="Border"></a>Border</h2><ul><li>border-style: dotted,dashed(虚线)，solid(实线)，double,groove, ridge,inset,outset,none,hidden</li><li>border-width: The width can be set as a specific size (in px, pt, cm, em, etc) or by using one of the three pre-defined values: thin, medium, or thick:</li><li>border-color</li><li>border</li><li>border-radius: 5px 圆角边框<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p.mix &#123;border-style: dotted dashed solid double;&#125;</span><br><span class="line">p &#123;</span><br><span class="line">  border: 5px solid red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="Margins-边距"><a href="#Margins-边距" class="headerlink" title="Margins 边距"></a>Margins 边距</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">p &#123;</span><br><span class="line">  margin-top: 100px;</span><br><span class="line">  margin-bottom: 100px;</span><br><span class="line">  margin-right: 150px;</span><br><span class="line">  margin-left: 80px;</span><br><span class="line">  margin: 25px 50px 75px 100px; #auto</span><br><span class="line">  margin: 20px 0 0 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">div &#123;</span><br><span class="line">  padding-top: 50px;</span><br><span class="line">  padding-right: 30px;</span><br><span class="line">  padding-bottom: 50px;</span><br><span class="line">  padding-left: 80px;</span><br><span class="line">   padding: 25px 50px 75px 100px; #shorthand property</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Box-Model"><a href="#Box-Model" class="headerlink" title="Box Model"></a>Box Model</h2><p><img src= "/img/loading.gif" data-lazy-src="/2022/10/26/CSS-learn/Box.png" alt="box"></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">div &#123;</span><br><span class="line">  width: 300px;</span><br><span class="line">  border: 15px solid green;</span><br><span class="line">  padding: 50px;</span><br><span class="line">  margin: 20px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Text"><a href="#Text" class="headerlink" title="Text"></a>Text</h2><p>Alignment&#x2F;Direction Properties<br><img src= "/img/loading.gif" data-lazy-src="/2022/10/26/CSS-learn/textalign.png" alt="text align"></p><p>Text Decoration文本装饰</p><ul><li>text-decoration-line</li><li>text-decoration-color</li><li>text-decoration-style</li><li>text-decoration-thickness</li></ul><p>text-transform 指定文本字母大小写</p><p>text Spacing 文本间距</p><ul><li>text-indent : used to specify the indentation（缩进） of the first line of a text,</li><li>letter-spacing</li><li>line-height</li><li>word-spacing</li><li>white-space</li></ul><p>text-shadow 文字阴影</p><h2 id="Fonts字体"><a href="#Fonts字体" class="headerlink" title="Fonts字体"></a>Fonts字体</h2><p><img src= "/img/loading.gif" data-lazy-src="/2022/10/26/CSS-learn/font.png" alt="font"></p><ul><li>font-family 指定文本的字体，应包含多个字体备用，确保浏览器兼容性，可以设置艺术字</li><li>font-style 指定斜体文体</li><li>font-weight 字体粗细</li><li>font-variant 字体大小写</li><li>font-size 字体大小</li><li>font</li></ul><p>Google Fonts 可以免费使用，并且有 1000 多种字体可供选择。只需在 head 部分添加一个特殊的样式表链接，然后参考 CSS 中的字体。可以使用不同的字体突出强调，形成对比。</p><h2 id="Icons"><a href="#Icons" class="headerlink" title="Icons"></a>Icons</h2><p>网站：阿里巴巴iconfont，fontawesome，前端框架内部的icon</p><h2 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h2><p>四个状态：正常，访问过的链接、鼠标悬停、点击时，分别对应a:link、a:visited、a:hover、a:active;同时可以定义color、font、background、text_decoration等<br>C&#96;&#96;&#96;html<br>&#x2F;* unvisited link *&#x2F;<br>a:link {<br>  color: red;<br>}</p><p>&#x2F;* visited link *&#x2F;<br>a:visited {<br>  color: green;<br>}</p><p>&#x2F;* mouse over link *&#x2F;<br>a:hover {<br>  color: hotpink;<br>}</p><p>&#x2F;* selected link *&#x2F;<br>a:active {<br>  color: blue;<br>}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## lists</span><br><span class="line">Lists分为Unordered Lists, Ordered Lists</span><br><span class="line"></span><br><span class="line">```html</span><br><span class="line"># Unordered Lists</span><br><span class="line">ul.a &#123;</span><br><span class="line">  list-style-type: circle;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ul.b &#123;</span><br><span class="line">  list-style-type: square;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Ordered Lists</span><br><span class="line">ol.c &#123;</span><br><span class="line">  list-style-type: upper-roman;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ol.d &#123;</span><br><span class="line">  list-style-type: lower-alpha;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We can also style lists with colors, to make them look a little more interesting.</p><p>Anything added to the ol or ul tag, affects the entire list, while properties added to the li tag will affect the individual list items:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="selector-tag">ol</span> &#123;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">background</span>: <span class="number">#ff9999</span>;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">padding</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="selector-tag">ul</span> &#123;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">background</span>: <span class="number">#3399ff</span>;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">padding</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="selector-tag">ol</span> <span class="selector-tag">li</span> &#123;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">background</span>: <span class="number">#ffe5e5</span>;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">color</span>: darkred;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">padding</span>: <span class="number">5px</span>;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">margin-left</span>: <span class="number">35px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="selector-tag">ul</span> <span class="selector-tag">li</span> &#123;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">background</span>: <span class="number">#cce5ff</span>;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">color</span>: darkblue;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">margin</span>: <span class="number">5px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Styling Lists With Colors<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ol</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>Coffee<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>Tea<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>Coca Cola<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ol</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>Coffee<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>Tea<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>Coca Cola<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="/2022/10/26/CSS-learn/list.png" alt="list"></p><h2 id="Tables"><a href="#Tables" class="headerlink" title="Tables"></a>Tables</h2><h3 id="Borders"><a href="#Borders" class="headerlink" title="Borders"></a>Borders</h3><p>To specify table boders in CSS, use border property.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="selector-tag">table</span>, <span class="selector-tag">th</span>, <span class="selector-tag">td</span> &#123;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">border</span>: <span class="number">1px</span> solid;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">width</span>: <span class="number">100%</span>;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">height</span>: <span class="number">70px</span>;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">text-align</span>: right; 水平对齐，<span class="attribute">left</span> <span class="attribute">right</span> center</span></span><br><span class="line"><span class="language-css">  <span class="attribute">vertical-align</span>: bottom; 垂直对齐，底部，顶部 ，中间</span></span><br><span class="line"><span class="language-css">  boder-collapse: collapse 单边框；不使用该元素会双边框，因为th/td都有独立的边界</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span>&gt;</span>Add a border to a table:<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">th</span>&gt;</span>Firstname<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">th</span>&gt;</span>Lastname<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">td</span>&gt;</span>Peter<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">td</span>&gt;</span>Griffin<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">td</span>&gt;</span>Lois<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">td</span>&gt;</span>Griffin<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Padding-1"><a href="#Padding-1" class="headerlink" title="Padding"></a>Padding</h3><p>为了控制表格中边框与内容的空间，使用padding属性。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">th, td &#123;</span><br><span class="line">  padding: 15px;</span><br><span class="line">  text-align: left;</span><br><span class="line">  border-bottom: 4px solid red; #Horizontal Dividers水平线</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Hoverable-Table"><a href="#Hoverable-Table" class="headerlink" title="Hoverable Table"></a>Hoverable Table</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tr:hover&#123;</span><br><span class="line">  background-color:coral;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Striped-Tables条纹表"><a href="#Striped-Tables条纹表" class="headerlink" title="Striped Tables条纹表"></a>Striped Tables条纹表</h3><p>zebra-striped table斑马条纹表，使用nth-child() selector,在家background-color到 even偶数或者 odd奇数行。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tr: nth-child(even)&#123;</span><br><span class="line">  background-color: #f2f2f2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Color"><a href="#Color" class="headerlink" title="Color"></a>Color</h3><p>表头color:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">th &#123;</span><br><span class="line">  background-color: red;</span><br><span class="line">  color: white; #字体颜色</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Responsive-Table响应表"><a href="#Responsive-Table响应表" class="headerlink" title="Responsive Table响应表"></a>Responsive Table响应表</h3><p>如果表格水平太长，就设置可滚动的响应表</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;overflow-x:auto;&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>content<span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Layout"><a href="#Layout" class="headerlink" title="Layout"></a>Layout</h2><h3 id="display-Property"><a href="#display-Property" class="headerlink" title="display Property"></a>display Property</h3><p>display用来指定元素是否display,所有的html元素都有默认的display value, 通常是block、inline。</p><p>Block-level Elements,块级元素每次开始都是新的一行，占据可用的全部宽度，占满左右宽度,比如div， h1-h6, p, form. header, footer, section都是块级元素。</p><p>Inline Elements，内联元素只会占据必要的宽度。比如 span img等</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display: none 常与 JavaScript 一起使用来隐藏和显示元素，而无需删除和重新创建它们; <span class="tag">&lt;<span class="name">script</span>&gt;</span> display:none是默认值。</span><br></pre></td></tr></table></figure><p>每个元素都有默认的显示值，但是可以去覆盖，比如常见的例子是使用li为水平菜单制作内联元素。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">li&#123;display : inline&#125;</span><br><span class="line">span&#123;display: block&#125;</span><br></pre></td></tr></table></figure><p>隐藏元素可以display:none,这样其不占有空间，visibility:hidden则是占据位置不显示。</p><h3 id="width-and-max-width"><a href="#width-and-max-width" class="headerlink" title="width and max-width"></a>width and max-width</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">div.ex1 &#123;</span><br><span class="line">  width: 500px; # 如果小型设备没有500px，下方就会出现水平条，这时使用max-width可以改善体验。</span><br><span class="line">  margin: auto;</span><br><span class="line">  border: 3px solid #73AD21;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">div.ex2 &#123;</span><br><span class="line">  max-width: 500px;</span><br><span class="line">  margin: auto;</span><br><span class="line">  border: 3px solid #73AD21;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="position"><a href="#position" class="headerlink" title="position"></a>position</h3><p>The position property specifies the type of positioning method used for an element (static, relative, fixed, absolute or sticky).</p><p>一共有5个不同的值：</p><ul><li>static 即没有定位，static元素不受到top、bottom、left、right影响</li><li>relative 设置相对定位元素的 top、right、bottom 和 left 属性将导致它被调整远离其正常位置，移动相对定位元素，但它原本所占的空间不会改变。</li><li>fixed 元素的位置相对于浏览器窗口是固定位置。即使窗口是滚动的它也不会移动：</li><li>absolute 绝对定位的元素的位置相对于最近的已定位父元素，如果元素没有已定位的父元素，那么它的位置相对于<html>:</html></li><li>sticky 粘性定位的元素是依赖于用户的滚动，在 position:relative 与 position:fixed 定位之间切换。它的行为就像 position:relative; 而当页面滚动超出目标区域时，它的表现就像 position:fixed;，它会固定在目标位置。</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">div.static &#123;</span><br><span class="line">    position: static;</span><br><span class="line">    border: 3px solid #73AD21;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">p.pos_fixed</span><br><span class="line">&#123;</span><br><span class="line">    position:fixed;</span><br><span class="line">    top:30px;</span><br><span class="line">    right:5px;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">h2.pos_left</span><br><span class="line">&#123;</span><br><span class="line">    position:relative;</span><br><span class="line">    left:-20px;</span><br><span class="line">&#125;</span><br><span class="line">h2.pos_right</span><br><span class="line">&#123;</span><br><span class="line">    position:relative;</span><br><span class="line">    left:20px;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">h2</span><br><span class="line">&#123;</span><br><span class="line">    position:absolute;</span><br><span class="line">    left:100px;</span><br><span class="line">    top:150px;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">div.sticky &#123;</span><br><span class="line">    position: -webkit-sticky; /* Safari */</span><br><span class="line">    position: sticky;</span><br><span class="line">    top: 0;</span><br><span class="line">    background-color: green;</span><br><span class="line">    border: 2px solid #4CAF50;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 前端开发 </tag>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/09/22/result/"/>
      <url>/2022/09/22/result/</url>
      
        <content type="html"><![CDATA[<p>metabric 32223<br>xgboost_rna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6768847590765399 0.7994428969359332 0.8853503184713375 0.972027972027972 0.8767123287671232 0.8128654970760234 0.5294117647058824 278 64 8 9</p><p>xgboost_snp<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.5245713190918672 0.7827298050139275 0.8773584905660378 0.9755244755244755 0.9726027397260274 0.7971428571428572 0.2222222222222222 279 71 7 2</p><p>xgboost_cna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.5807548615767795 0.7715877437325905 0.8651315789473685 0.9195804195804196 0.8082191780821918 0.8167701863354038 0.3783783783783784 263 59 23 14</p><p>xgboost_mut<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.654420921544209 0.7966573816155988 0.8857589984350548 0.9895104895104895 0.958904109589041 0.8016997167138811 0.5 283 70 3 3</p><p>brca_tcga<br>xgboost_rna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.7941176470588235 0.7837837837837838 0.8749999999999999 0.8235294117647058 0.6666666666666666 0.9333333333333333 0.14285714285714285 28 2 6 1</p><p>xgboost_snp<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.5 0.918918918918919 0.9577464788732395 1.0 1.0 0.918918918918919 nan 34 3 0 0</p><p>xgboost_cna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.5588235294117647 0.6486486486486487 0.7868852459016393 0.7058823529411765 1.0 0.8888888888888888 0.0 24 3 10 0</p><p>xgboost_mut<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6470588235294117 0.8648648648648649 0.9275362318840579 0.9411764705882353 1.0 0.9142857142857143 0.0 32 3 2 0</p><p>brca_tcga_pan_can_atlas_2018<br>xgboost_rna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6982248520710059 0.7384615384615385 0.8468468468468469 0.9038461538461539 0.9230769230769231 0.7966101694915254 0.16666666666666666 47 12 5 1</p><p>xgboost_snp<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.4223372781065089 0.8 0.888888888888889 1.0 1.0 0.8 nan 52 13 0 0</p><p>xgboost_cna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6190828402366864 0.7076923076923077 0.8256880733944955 0.8653846153846154 0.9230769230769231 0.7894736842105263 0.125 45 12 7 1</p><p>xgboost_mut<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.7226331360946745 0.8153846153846154 0.890909090909091 0.9423076923076923 0.6923076923076923 0.8448275862068966 0.5714285714285714 49 9 3 4</p><p>metabric 32233 rna&#x3D;3</p><p>xgboost_all<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6860810422454259 0.7910863509749304 0.8772504091653028 0.9370629370629371 0.7808219178082192 0.8246153846153846 0.47058823529411764 268 57 18 16</p><p>catboost_all<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6719992336430692 0.7994428969359332 0.8875000000000001 0.993006993006993 0.958904109589041 0.8022598870056498 0.6 284 70 2 3</p><p>bayes<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6343758980745282 0.6462395543175488 0.7465069860279441 0.6538461538461539 0.3835616438356164 0.8697674418604651 0.3125 187 28 99 45</p><p>logistic<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.5862630520164767 0.7409470752089137 0.839378238341969 0.8496503496503497 0.684931506849315 0.8293515358361775 0.3484848484848485 243 50 43 23</p><p>Random Forest<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6637129993294377 0.7966573816155988 0.8868217054263565 1.0 1.0 0.7966573816155988 nan 286 73 0 0</p><p>SVM<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6357888686655812 0.7966573816155988 0.8868217054263565 1.0 1.0 0.7966573816155988 nan 286 73 0 0</p><p>xgboost_rna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.647044736085832 0.766016713091922 0.8618421052631579 0.916083916083916 0.821917808219178 0.8136645962732919 0.35135135135135137 262 60 24 13</p><p>catboost_rna<br>AUC, ACC, F1, TPR, FPR, PPV, NPV, TP, FP, FN, TN<br>0.6940798927100297 0.7883008356545961 0.8789808917197452 0.965034965034965 0.9041095890410958 0.8070175438596491 0.4117647058823529 276 66 10 7</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Java</title>
      <link href="/2022/09/13/Java/"/>
      <url>/2022/09/13/Java/</url>
      
        <content type="html"><![CDATA[<h2 id="Java面向对象三大特性"><a href="#Java面向对象三大特性" class="headerlink" title="Java面向对象三大特性"></a>Java面向对象三大特性</h2><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p><strong>概念</strong>：尽可能隐藏对象内部实现细节，控制对象地修改及访问权限</p><p><strong>访问修饰符</strong>：private（可将属性修饰为私有，仅限本类可见）</p><p>外部访问只可以访问公共空间，不可以直接访问属性设置为private的私有空间，而get、set方法是外界访问对象私有属性的唯一通道，方法内部可对数据进行检测和过滤</p><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><ul><li>功能越精细，重合点越多，越接近直接父类</li><li>功能越粗糙，重合点越少，越接近Object类（所谓万物皆对象）</li></ul><p>用法：</p><ul><li>语法<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class 子类 extends 父类&#123;&#125; <span class="comment">//定义子类时，显示继承父类</span></span><br></pre></td></tr></table></figure></li><li>应用：产生继承关系后，子类可以使用父类中的属性和方法，也可以定义子类独有的属性和方法。</li><li>好处：即提高了代码的复用性，又提高了代码的可扩展性</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/2022/09/13/Java/power.png" alt="访问修饰符"></p><h3 id="4-1-方法重写"><a href="#4-1-方法重写" class="headerlink" title="4.1 方法重写"></a>4.1 方法重写</h3><p>当父类提供的方法无法满足子类需求时，可在子类中定义和父类相同的方法进行覆盖（Override）</p><h3 id="4-2-方法的覆盖"><a href="#4-2-方法的覆盖" class="headerlink" title="4.2 方法的覆盖"></a>4.2 方法的覆盖</h3><ul><li>方法的覆盖原则：</li><li>方法名称、参数列表、返回值类型必须与父类相同</li><li>访问修饰符可与父类相同或是比父类更宽泛</li><li>方法覆盖的执行：</li><li>子类覆盖父类方法后，调用时优先执行子类覆盖后的方法</li></ul><h3 id="4-3-super关键字"><a href="#4-3-super关键字" class="headerlink" title="4.3 super关键字"></a>4.3 super关键字</h3><ul><li><p>第一种用法：</p><blockquote><ul><li>在子类的方法中使用“super.”的形式访问父类的属性和方法</li><li>例如：super.父类属性、super.父类方法();</li></ul></blockquote></li><li><p>第二种用法</p><blockquote><ul><li>在子类的构造方法的首行，使用“super()”或“super(参数)”，调用父类构造方法</li></ul></blockquote></li><li><p>注意：</p><blockquote><ul><li>如果子类构造方法中，没有显示定义super()或super(实参)，则默认提供super()</li><li>同一个子类构造方法中，super()、this()不可同时存在（因为都是要声明在首行）</li></ul></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Springboot初认识</title>
      <link href="/2022/09/13/Springboot%E5%88%9D%E8%AE%A4%E8%AF%86/"/>
      <url>/2022/09/13/Springboot%E5%88%9D%E8%AE%A4%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h2 id="SpringBoot介绍"><a href="#SpringBoot介绍" class="headerlink" title="SpringBoot介绍"></a>SpringBoot介绍</h2><h3 id="Spring-与-SpringBoot对比"><a href="#Spring-与-SpringBoot对比" class="headerlink" title="Spring 与 SpringBoot对比"></a>Spring 与 SpringBoot对比</h3><p><strong>Spring</strong>: Spring框架的主要功能是<strong>依赖注入</strong>和<strong>控制反转（loC）</strong>。借助Spring Framework，可以开发<strong>松耦合</strong>的应用程序。</p><p><strong>Spring Boot</strong>: Spring Boot是Spring Framework的模块。</p><h3 id="Spring-Boot体系结构"><a href="#Spring-Boot体系结构" class="headerlink" title="Spring Boot体系结构"></a>Spring Boot体系结构</h3><p>Spring 是Spring框架的模块，它用于轻松创建独立的生产基于Spring的应用程序。它是在核心Spring框架的顶部开发的。</p><p>Spring Boot遵循一个分层的体系结构。SpringBoot四个层：</p><ul><li>Presentation Layer展示层</li><li>Business Layer业务层</li><li>Persistence Layer持久层</li><li>DataBase 数据库层</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/2022/09/13/Springboot%E5%88%9D%E8%AE%A4%E8%AF%86/Architect.png" alt="Archetect"></p><p><strong>展示层</strong>：representation layer负责处理HTTP请求，将JSON参数转换为对象，并对请求进行身份验证并将其传输到Business Layer。简而言之，它由View视图即前端部分组成。</p><p><strong>业务层</strong>：业务层处理所有的业务逻辑。它由服务类组成，并使用数据访问层提供的服务，它还执行授权和验证。</p><p><strong>持久层</strong>：持久层包括所有存储逻辑，并将业务对象与数据库进行相互转换。</p><p><strong>数据库层</strong>：在数据库层种，CRUD（增删改查等）</p><h3 id="Spring-Boot-Flow-Architecture"><a href="#Spring-Boot-Flow-Architecture" class="headerlink" title="Spring Boot Flow Architecture"></a>Spring Boot Flow Architecture</h3><p><img src= "/img/loading.gif" data-lazy-src="/2022/09/13/Springboot%E5%88%9D%E8%AE%A4%E8%AF%86/flow.png" alt="Spring Boot flow Architecture"></p><ul><li>现在我们有验证器类，视图类和实用程序类。</li><li>Spring Boot使用类似于Spring MVC，Spring Data等的所有模块。SpringBoot的体系结构与Spring MVC的体系结构相同，不同之处在于: 不需要 DAO 和 DAOImpl 类在Spring启动中。</li><li>创建数据访问层并执行CRUD操作。</li><li>客户端发出HTTP请求(PUT或GET)。</li><li>请求发送到控制器，然后控制器映射该请求并进行处理。之后，如果需要，它将调用服务逻辑。</li><li>在服务层中，所有业务逻辑都将执行。它对通过类映射到JPA的数据执行逻辑。</li><li>如果没有发生错误，则会将JSP页面返回给用户。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Springboot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客迁移</title>
      <link href="/2022/05/31/hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"/>
      <url>/2022/05/31/hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/</url>
      
        <content type="html"><![CDATA[<ol><li><p>安装git</p></li><li><p>安装nodejs<br>npm -v<br>node -v</p></li><li><p>安装hexo<br>npm install -g hexo-cli</p></li></ol><p>4.cd ~&#x2F;new_blog<br>hexo init</p><p>5.删除node moudules</p><p>6.复制文件过来，然后npm install</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>normalize</title>
      <link href="/2022/02/28/normalize/"/>
      <url>/2022/02/28/normalize/</url>
      
        <content type="html"><![CDATA[<h2 id="NLP任务中，layer-norm比BatchNorm好在哪里"><a href="#NLP任务中，layer-norm比BatchNorm好在哪里" class="headerlink" title="NLP任务中，layer-norm比BatchNorm好在哪里"></a>NLP任务中，layer-norm比BatchNorm好在哪里</h2><p>本文主要是讲一下，为什么NLP任务中，比如Transformer，使用LayerNorm而不是使用BatchNorm</p><p>这个问题其实很有意思，理解的最核心的点在于：为什么LayerNorm单独对一个样本的所有单词做缩放可以起到效果。</p><p>大家往下慢慢看，我说一下我自己的理解，欢迎大佬拍砖，如果觉得我说的还行，点个在看鼓励一下。</p><h3 id="为啥BN在NLP中效果差"><a href="#为啥BN在NLP中效果差" class="headerlink" title="为啥BN在NLP中效果差"></a>为啥BN在NLP中效果差</h3><p>上一个文章有说 BN的使用场景，不适合 RNN这种动态文本模型，有一个原因是因为batch中的长度不一致，导致有的靠后面的特征的均值和方差不能估算。</p><p>这个问题其实不是个大问题，可以缓解。我们可以在数据处理的时候，使句子长度相近的在一个batch，就可以了。所以这不是为啥NLP不用BN的核心原因。</p><p>回忆一下上个文章中，BN在MLP中的应用。 BN是对每个特征在batch_size上求的均值和方差。记住，是每个特征。比如说身高，比如说体重等等。这些特征都有明确的含义。</p><p>但是我们想象一下，如果BN应用到NLP任务中，对应的是对什么做处理？</p><p>是对每一个单词！</p><p>也就是说，我现在的每一个单词是对应到了MLP中的每一个特征。</p><p>也就是默认了在同一个位置的单词对应的是同一种特征，比如:“我&#x2F;爱&#x2F;中国&#x2F;共产党”和“今天&#x2F;天气&#x2F;真&#x2F;不错”</p><p>如何使用BN，代表着认为 “我”和“今天”是对应的同一个维度特征，这样才可以去做BN。</p><p>大家想一下，这样做BN，会有效果吗？</p><p>不会有效果的，每个单词表达的特征是不一样的，所以按照位置对单词特征进行缩放，是违背直觉的。</p><h3 id="layner-norm-的特点"><a href="#layner-norm-的特点" class="headerlink" title="layner-norm 的特点"></a>layner-norm 的特点</h3><p>layner-norm 的特点是什么？layner-norm 做的是针对每一个样本，做特征的缩放。换句话讲，保留了N维度，在C&#x2F;H&#x2F;W维度上做缩放。</p><p>也就是，它认为“我&#x2F;爱&#x2F;中国&#x2F;共产党”这四个词在同一个特征之下，所以基于此而做归一化。</p><p>这样做，和BN的区别在于，一句话中的每个单词都可以归到一个名字叫做“语义信息”的一个特征中（我自己瞎起的名字，大家懂就好），也就是说，layner-norm也是在对同一个特征下的元素做归一化，只不过这里不再是对应N（或者说batch size），而是对应的文本长度。</p><p>上面这个解释，有一个细节点，就是，为什么每个单词都可以归到“语义信息”这个特征中。大家这么想，如果让你表达一个句子的语义信息，你怎么做？</p><p>最简单的方法就是词语向量的加权求和来表示句子向量，这一点没问题吧。（当然你也可以自己基于自己的任务去训练语义向量，这里只是说最直觉的办法）</p><p>上面这个方法就是出于每个单词都是语义信息的一部分这个insight。</p><p>引申-为啥BN在CNN可以而在NLP不可以</p><p>但是，我还想问一个问题，CNN中证明BN效果是很好的，NLP中的文本可以类比为图像，为什么BN在图像中效果好，在文本上效果差。</p><p>我是这样理解的。还是回到刚才，BN是对单词做缩放，在NLP中，单词由词向量来表达，本质上是对词向量进行缩放。词向量是什么？是我们学习出来的参数来表示词语语义的参数，不是真实存在的。</p><p>这就是NLP和图像的一个区别，图像的像素是真实存在的，像素中包含固有的信息。比如说，一张图像，最上面的一行像素，可以归为背景这个特征（这里只是为了理解，CNN做BN是基于整个feature map，而不是单独某一行像素）。</p><p>这个理解不确保正确，只是我自己的理解（记得是从一个知乎答案看到的，改天好好找一找）</p><p>简答说一下</p><p>写到这里，我写文章不是为了推导公式，因为这种推导文章太多了，而是想让大家看了我的文章之后再去看这些推导公式能够更加容易理解。</p><p>然后大家有问题的话，私信和我说，我也知道我自己写的哪里有问题，好改进。</p><p>点个在看再走呗，老弟</p><p>列一下参考资料：</p><p>各种Normalization - Mr.Y的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/86765356">https://zhuanlan.zhihu.com/p/86765356</a></p><p>这个文章关于BN和LN如何应用讲解的比较好，就是CNHW</p><p>NLP中 batch normalization与 layer normalization - 秩法策士的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/74516930">https://zhuanlan.zhihu.com/p/74516930</a></p><p>这个文章也还行，我在看的时候，看到中间那个图给了我点启发，就是在理解BN的时候，仅仅是在这个时候啊，我们的C，在CNN中是通道数，在理解BN的时候，理解为句子长度，这样”，每个样本通道数为 C，高为 H，宽为 W。对其求均值和方差时，将在 N、H、W上操作，而保留通道 C 的维度。具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道 …… 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值“这句话才比较好理解。</p><p>一般NLP来说，C为1吧。</p><p>模型优化之Layer Normalization - 大师兄的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/54530247">https://zhuanlan.zhihu.com/p/54530247</a></p><p>推荐一下这个文章，总结了对比实验：”这里我们设置了一组对照试验来对比普通网络，BN以及LN在MLP和RNN上的表现“，我还没细看，之后看。</p><p>transformer 为什么使用 layer normalization，而不是其他的归一化方法？ - pymars的回答 - 知乎 <a href="https://www.zhihu.com/question/395811291/answer/1260290120">https://www.zhihu.com/question/395811291/answer/1260290120</a></p><p>推荐这个答案，很好</p><p>转载自：<a href="https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/NLP%E4%BB%BB%E5%8A%A1%E4%B8%AD-layer-norm%E6%AF%94BatchNorm%E5%A5%BD%E5%9C%A8%E5%93%AA%E9%87%8C.md">https://github.com/DA-southampton/NLP_ability/blob/master/深度学习自然语言处理/Transformer/NLP任务中-layer-norm比BatchNorm好在哪里.md</a></p><p><a href="https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E7%AD%94%E6%A1%88%E5%90%88%E8%BE%91.md">https://github.com/DA-southampton/NLP_ability/blob/master/深度学习自然语言处理/Transformer/答案合辑.md</a></p>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch 之 CNN(Conv1d Conv2d)</title>
      <link href="/2022/02/16/pytorch-%E4%B9%8B-CNN-Conv1d-Conv2d/"/>
      <url>/2022/02/16/pytorch-%E4%B9%8B-CNN-Conv1d-Conv2d/</url>
      
        <content type="html"><![CDATA[<h2 id="Conv1d"><a href="#Conv1d" class="headerlink" title="Conv1d"></a>Conv1d</h2><p>$Input_size:(N, C_{in}, L)$</p><p>$Output_size:(N, C_{out}, L_{out})$</p><p>$N$ is a batch size, $C$ denotes a number of channels, $L$ is a length of signal sequence.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.Conv1d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">in_channels(<span class="built_in">int</span>) – 输入信号的通道。在文本分类中，即为词向量的维度</span><br><span class="line">out_channels(<span class="built_in">int</span>) – 卷积产生的通道。有多少个out_channels，就需要多少个<span class="number">1</span>维卷积</span><br><span class="line">kernel_size(<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>) - 卷积核的尺寸，卷积核的大小为(k,)，第二个维度是由in_channels来决定的，所以实际上卷积大小为kernel_size*in_channels</span><br><span class="line">stride(<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) - 卷积步长</span><br><span class="line">padding (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional)- 输入的每一条边补充<span class="number">0</span>的层数</span><br><span class="line">dilation(<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) – 卷积核元素之间的间距</span><br><span class="line">groups(<span class="built_in">int</span>, optional) – 从输入通道到输出通道的阻塞连接数</span><br><span class="line">bias(<span class="built_in">bool</span>, optional) - 如果bias=<span class="literal">True</span>，添加偏置</span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv1 = nn.Conv1d(in_channels=<span class="number">256</span>，out_channels=<span class="number">100</span>,kernel_size=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">32</span>,<span class="number">35</span>,<span class="number">256</span>)</span><br><span class="line"><span class="comment"># batch_size x text_len x embedding_size -&gt; batch_size x embedding_size x text_len</span></span><br><span class="line"><span class="built_in">input</span> = <span class="built_in">input</span>.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">out = conv1(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out.size()</span><br></pre></td></tr></table></figure><p>这里32为batch_size，35为句子最大长度，256为词向量</p><p>再输入一维卷积的时候，需要将32<em>35</em>256变换为32<em>256</em>35，因为一维卷积是在最后维度上扫的，最后out的大小即为：32<em>100</em>（35-2+1）&#x3D;32<em>100</em>34</p><p>计算：</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/02/16/pytorch-%E4%B9%8B-CNN-Conv1d-Conv2d/Conv1d.png" alt="Conv1d"></p><h3 id="channel的作用"><a href="#channel的作用" class="headerlink" title="channel的作用"></a>channel的作用</h3><p>input_channel&#x3D;256，output_channel&#x3D;100，channel是怎样变化的呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input_shape: (<span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>) (<span class="built_in">len</span>, dim, input_channel)</span><br><span class="line">input_channel: <span class="number">3</span></span><br><span class="line">out_channel: <span class="number">100</span> </span><br><span class="line">one_filter: (kernel_size=<span class="number">2</span>)-&gt; (<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>) (<span class="built_in">len</span>, dim, input_channel)</span><br><span class="line">one_filter_out:(<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>) -&gt;in_channel_sum -&gt;(<span class="number">5</span>, <span class="number">5</span>)  <span class="comment"># 5=6-2+1</span></span><br><span class="line">all_filter: (<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>) * <span class="number">100</span>(out_channel)</span><br></pre></td></tr></table></figure><p>input_channel相当于有n个维度的词向量，从n个角度解释词向量，有各自的权重，所以要input_channel个单层filter，然后累加。</p><p>output_channel是输出的channel维度，即一共有output_channel个(len, dim, input_channel)大小的filter。</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/02/16/pytorch-%E4%B9%8B-CNN-Conv1d-Conv2d/channel.png" alt="channel示意图"></p><h2 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h2><p>$Input_size:(N, C_{in}, H, W)$</p><p>$Output_size:(N, C_{out}, H_{out}, W_{out})$</p><p>$N$ is a batch size, $C$ denotes a number of channels, $H$ is a height of input planes in pixels, and $W$ is width in pixels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d( in_channels , out_channels , kernel_size , stride = <span class="number">1</span> , padding = <span class="number">0</span> , dilation = <span class="number">1</span> , groups = <span class="number">1</span> , bias = <span class="literal">True</span> , padding_mode = <span class="string">&#x27;zeros&#x27;</span> , device = <span class="literal">None</span> , dtype = <span class="literal">None</span> )</span><br><span class="line"></span><br><span class="line">in_channels ( <span class="built_in">int</span> ) – 输入图像中的通道数</span><br><span class="line">out_channels ( <span class="built_in">int</span> ) – 卷积产生的通道数</span><br><span class="line">kernel_size ( <span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span> ) – 卷积核的大小</span><br><span class="line">stride ( <span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span> , optional ) -- 卷积的步幅。默认值：<span class="number">1</span></span><br><span class="line">padding ( <span class="built_in">int</span> , <span class="built_in">tuple</span>或<span class="built_in">str</span> , optional ) – 添加到输入的所有四个边的填充。默认值：<span class="number">0</span></span><br><span class="line">padding_mode (字符串,可选) – <span class="string">&#x27;zeros&#x27;</span>, <span class="string">&#x27;reflect&#x27;</span>, <span class="string">&#x27;replicate&#x27;</span>或<span class="string">&#x27;circular&#x27;</span>. 默认：<span class="string">&#x27;zeros&#x27;</span></span><br><span class="line">dilation ( <span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span> , optional ) -- 内核元素之间的间距。默认值：<span class="number">1</span></span><br><span class="line">groups ( <span class="built_in">int</span> , optional ) -- 从输入通道到输出通道的阻塞连接数。默认值：<span class="number">1</span></span><br><span class="line">bias ( <span class="built_in">bool</span> , optional ) – If <span class="literal">True</span>，向输出添加可学习的偏差。默认：<span class="literal">True</span></span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># With square kernels and equal stride</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># non-square kernels and unequal stride and with padding and dilation</span></span><br><span class="line">m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>), dilation=(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure><p>计算：</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/02/16/pytorch-%E4%B9%8B-CNN-Conv1d-Conv2d/Conv2d.png" alt="Conv2d"></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AttentionPool</title>
      <link href="/2022/02/16/AttentionPool/"/>
      <url>/2022/02/16/AttentionPool/</url>
      
        <content type="html"><![CDATA[<p>将tensor最后一维($len_{dim&#x3D;-1}&#x3D;n$)划分成$p$段，每段分别做$x \times softmax(x)$，然后各段相加，总维度不变，最后一维($len_{dim&#x3D;-1}&#x3D;n&#x2F;p$)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionPool</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    pooling: b d n -&gt; b d n/p</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    input: x.shape : b d n -&gt; b d s p  s=n/p</span></span><br><span class="line"><span class="string">           to_att_logits.shape : b d s p -&gt; x.softmax(dim=-1)</span></span><br><span class="line"><span class="string">    :return: b d s -&gt; (x * to_att_logits).sum(dim=-1)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, pool_size=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.pool_size = pool_size</span><br><span class="line">        self.pool_fn = Rearrange(<span class="string">&#x27;b d (n p) -&gt; b d n p&#x27;</span>, p=<span class="number">2</span>)</span><br><span class="line">        self.to_attn_logits = nn.Parameter(torch.eye(dim)) </span><br><span class="line">            <span class="comment"># torch.eye(dim): shape(dim, dim)对角线为1的矩阵</span></span><br><span class="line">            <span class="comment"># nn.Parameter(): 封装成net的Parameter, requires_grad=True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, _, n = x.shape</span><br><span class="line">        remainder = n % self.pool_size</span><br><span class="line">        needs_padding = remainder &gt; <span class="number">0</span>  <span class="comment"># True or False (if remainder &gt; 0: needs_padding=True else: False)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> needs_padding:</span><br><span class="line">            x = F.pad(x, (<span class="number">0</span>, remainder), value=<span class="number">0</span>)</span><br><span class="line">            mask = torch.zeros((b, <span class="number">1</span>, n), dtype=torch.<span class="built_in">bool</span>, device=x.device)</span><br><span class="line">            mask = F.pad(mask, (<span class="number">0</span>, remainder), value=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        attn_logits = einsum(<span class="string">&#x27;b d n, d e -&gt; b e n&#x27;</span>, x, self.to_attn_logits)  <span class="comment"># x * to_attn_logits</span></span><br><span class="line">        x = self.pool_fn(x)</span><br><span class="line">        logits = self.pool_fn(attn_logits)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> needs_padding:</span><br><span class="line">            mask_value = -torch.finfo(logits.dtype).<span class="built_in">max</span></span><br><span class="line">            logits = logits.masked_fill(self.pool_fn(mask), mask_value)</span><br><span class="line"></span><br><span class="line">        attn = logits.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> (x * attn).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>logging</title>
      <link href="/2022/02/15/logging/"/>
      <url>/2022/02/15/logging/</url>
      
        <content type="html"><![CDATA[<p>Use logging.getLogger(name) to create a named global logger.</p><h2 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a>main.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> log</span><br><span class="line">logger = log.setup_custom_logger(<span class="string">&#x27;root&#x27;</span>)</span><br><span class="line">logger.debug(<span class="string">&#x27;main message&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> submodule</span><br></pre></td></tr></table></figure><h2 id="log-py"><a href="#log-py" class="headerlink" title="log.py"></a>log.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_custom_logger</span>(<span class="params">name</span>):</span><br><span class="line">    formatter = logging.Formatter(fmt=<span class="string">&#x27;%(asctime)s - %(levelname)s - %(module)s - %(message)s&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    handler = logging.StreamHandler()</span><br><span class="line">    handler.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(<span class="string">&#x27;./Output/PRS.log&#x27;</span>):</span><br><span class="line">        <span class="built_in">open</span>(<span class="string">&quot;./Output/PRS.log&quot;</span>, <span class="string">&quot;w&quot;</span>).close()</span><br><span class="line">    fhlr = logging.FileHandler(<span class="string">&#x27;./Output/PRS.log&#x27;</span>)  <span class="comment"># file handler</span></span><br><span class="line">    fhlr.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line">    logger = logging.getLogger(name)</span><br><span class="line">    logger.setLevel(logging.DEBUG)</span><br><span class="line">    logger.addHandler(handler)</span><br><span class="line">    logger.addHandler(fhlr)</span><br><span class="line">    <span class="keyword">return</span> logger</span><br></pre></td></tr></table></figure><h2 id="submodule-py"><a href="#submodule-py" class="headerlink" title="submodule.py"></a>submodule.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(<span class="string">&#x27;root&#x27;</span>)</span><br><span class="line">logger.debug(<span class="string">&#x27;submodule message&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2011-10-01 20:08:40,049 - DEBUG - main - main message</span><br><span class="line">2011-10-01 20:08:40,050 - DEBUG - submodule - submodule message</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> logging </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VC维</title>
      <link href="/2022/01/19/VC%E7%BB%B4/"/>
      <url>/2022/01/19/VC%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<h2 id="可学习的条件"><a href="#可学习的条件" class="headerlink" title="可学习的条件"></a>可学习的条件</h2><h3 id="学习的过程"><a href="#学习的过程" class="headerlink" title="学习的过程"></a>学习的过程</h3><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/19/VC%E7%BB%B4/1.1.png" alt="图1.1"></p><p>$A$表示学习算法，$f$表示学习的目标假设(可以是一个函数，也可以是一个分布)，$H$表示假设空间，$g$表示我们求解的用来预测的假设，$g$是属于$H$的。</p><p>机器学习的过程就是：通过算法$A$，在假设空间$H$中，根据训练样本$D$，选择最好的假设作为$g$，使$g$近似于$f$。</p><p>图1.1中$E_{out}(g)$和$E_{in}(g)$是两个重要概念：</p><ul><li>$E_{out}(g)$到的假设$g$在除了训练样本外的其他所有样本(out-of-sample)上的损失，称为期望误差，也称泛化误差。（外部验证误差）</li><li>$E_{in}(g)$学到的假设$g$在训练样本(in-of-sample)上的损失，称为经验误差。（训练误差）</li></ul><p>我们的目标是选择最好的假设作为$g$, 使$g$近似于$f$, 而$E_{out}(g) &#x3D; 0$, 即使$E_{out}(g) \approx 0$。但我们是没法获得训练样本外的其他所有样本的，那也就没法计算 $E_{out}(g)$，这该怎么办呢？</p><h3 id="Hoeffding不等式"><a href="#Hoeffding不等式" class="headerlink" title="Hoeffding不等式"></a>Hoeffding不等式</h3><p>在回答1.1最后的问题之前，先来看看Hoeffding不等式。</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/19/VC%E7%BB%B4/1.2.png" alt="图1.2(可以将v$看作是样本期望,$\mu$则是总体期望)"></p><p>来看图1.2所述的问题: 设瓶子里的橙色球占比为$\mu$，从瓶子中随机抽取出N个样本，这N个样本中橙色球占比是$v$，我们可以将$\mu$看作1.1节中的$E_{out}(g)$，将$v$看作$E_{in}(g)$，我们现在并不知道$\mu$和$E_{out}(g)$，只知道$v$和$E_{in}(g)$，我们可以从$v$推断出关于$\mu$的什么吗？</p><p>直觉上，如果我们有更多的样本(抽出很多的球)，则$v$应该越来越接近总体期望$\mu$。事实上，这里可以用hoeffding不等式表示如下：</p><p>$$ P[|v - \mu| &gt; \epsilon ] \leq 2 \ exp(-2\epsilon^2N)$$</p><p>从hoeffding不等式可以看出，当N越来越大时，$v$和$\mu$之差大于$\epsilon$的概率的上界越来越接近0，所以样本期望$v$越来越接近总体期望$\mu$，即$v$和$\mu$概率近似相等(probably approximately correct, PAC)了。</p><h3 id="可学习的条件-1"><a href="#可学习的条件-1" class="headerlink" title="可学习的条件"></a>可学习的条件</h3><p>有了1.2节的铺垫，我们就可以来回答1.1节的问题了。</p><p>对于任意固定的假设$h$，训练样本量N足够大，类比于1.2节的例子，可以通过样本集上的经验误差$E_{in}(h)$推测总体的期望误差$E_{out}(h)$。基于hoeffding不等式，我们得到下面式子：<br>$$P[|E_{in}(h) - E_{out}(h)| &gt; \epsilon ] \leq 2 \ exp(-2\epsilon^2N)$$</p><p>根据上面不等式，我们可以推断，当N足够大时，$E_{in}(h)$和$E_{out}(h)$将非常接近。</p><p>注意在上面推导中，我们是针对某一个特定的假设$h$。在我们的假设空间$H$中，往往有很多个假设(甚至于无穷多个)。那么对于假设空间$H$ 中的任意的假设$h$，$E_{in}(h)$和$E_{out}(h)$ 之差大于$\epsilon$的概率的上界会是什么呢？</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Papers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper ADVERSARIAL TRAINING METHODS FOR SEMI-SUPERVISED TEXT CLASSIFICATION</title>
      <link href="/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/"/>
      <url>/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/</url>
      
        <content type="html"><![CDATA[<p>ICLR 2017</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>对抗式训练(Adversarial training)提供了一种规范化监督学习算法的方法，而虚拟对抗式训练能够将监督学习算法扩展到半监督环境。然而，这两种方法都需要对输入向量的多个条目进行小扰动，这不适用于稀疏的高维输入(sparse high-dimensional inputs)，例如One-hot representations.本文将对抗式和虚拟对抗式训练扩展到文本域，通过对递归神经网络(recurrent neural network)中嵌入的单词施加扰动(perturbations)，而不是对原始输入本身(original input itself)施加扰动。所提出的方法在多个基准半监督和纯监督任务上实现了最先进(state of the art)的结果。我们提供的可视化和分析表明，所学单词嵌入的质量有所提高，并且在培训过程中，模型不太容易过度拟合。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>对抗性示例(Adversarial example)是通过对设计的输入进行小扰动来显著增加机器学习模型造成的损失的示例（Szegedy et al.，2014；Goodfello et al.，2015）。一些模型，包括最先进的卷积神经网络，缺乏正确分类对抗性示例的能力，有时甚至当对抗性扰动被限制为非常小以至于人类观察者无法感知时。对抗性训练是训练一个模型的过程，以正确地将修改的例子和通用的例子分类。它不仅提高了对抗性示例的鲁棒性，而且提高了原始示例的泛化性能。对抗性训练要求在训练使用监督成本的模型时使用标签，因为标签出现在对抗性干扰设计为最大化的成本函数中。虚拟对抗训练（Miyato等人，2016年）将对抗训练的思想扩展到半监督区域和未标记的示例。这是通过对模型进行规范化来实现的，因此给定一个示例，该模型将产生与该示例的对抗性扰动相同的输出分布。虚拟对抗训练在有监督和半监督学习任务中都具有良好的泛化性能.</p><p>以前的工作主要将对抗性和虚拟对抗性训练应用于图像分类任务。在这项工作中，我们将这些技术扩展到文本分类任务和序列模型。对抗性干扰通常包括对许多实际价值投入进行微小修改。对于文本分类，输入是离散的，通常表示为一系列高维一维热向量(high-dimensional one-hot vectors)。由于高维一维热向量集不允许有限的符号扰动(not admit infinitesimal perturbation)，我们定义了连续词嵌(continuous word embeddings)入上的扰动，而不是离散(discrete)词输入上的扰动。传统对抗式和虚拟对抗式训练既可以解释为规则化策略（Szegedy等人，2014年；Goodfellowet等人，2015年；Miyato等人，2016年），也可以解释为对能够提供恶意输入(malicious inputs)的对手的防御（Szegedy等人，2014年；Goodfello等人，2015年）。由于扰动嵌入不映射到任何单词，而且对手可能无法访问单词嵌入层，因此我们提出的训练策略不再是对对手(adversary)的防御.</p><p>因此，我们提出这种方法作为一种通过稳定分类函数来正则化文本分类器的方法. (We thus propose this approach exclusively as a means of regularizing a text classifier by stabilizing the classification function).</p><p>我们表明，我们采用Dai&amp;Le（2015）提出的神经语言模型无监督预训练的方法在多个半监督文本分类任务（包括情感分类和主题分类）中实现了最先进的性能。我们强调，仅优化一个额外的超参数ǫ，即限制adversarial扰动大小的范数约束，就实现了这种最先进的性能。这些结果有力地鼓励了我们所提出的方法在其他文本分类任务中的应用。我们认为文本分类是半监督学习的理想环境，因为有大量的未标记语料库可供半监督学习算法利用。这项工作是我们所知的第一项使用对抗式和虚拟对抗式训练来改进文本或RNN模式的工作。</p><p>我们还分析了训练模型，定性地描述了对抗性和虚拟对抗性训练的效果。我们发现，与基线方法相比，对抗式和虚拟对抗式训练改善了单词嵌入</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>We denote a sequence of T words as ${w(t)|t&#x3D; 1, . . . , T}$ , and a corresponding target as $y$ .<br>To transform a discrete word input to a continuous vector, we define the word embedding matrix $V∈R^{(K+1)×D}$ where $K$ is the number of words in the vocabulary and each row $v_k$ corresponds to the word embedding of the $i-th$ word. Note that the $(K+ 1)-th$ word embedding is used as an embedding of an ‘end of sequence (eos)’ token, $v_{eos}$. As a text classification model, we used a simple LSTM-based neural network model, shown in Figure 1a. At time step t, the input is the discrete word $w(t)$, and the corresponding word embedding is $v(t)$. We additionally tried the bidirectional LSTM architecture (Graves &amp; Schmidhuber, 2005) since this is used by the current state of theart method (Johnson &amp; Zhang, 2016b). For constructing the bidirectional LSTM model for textclassification, we add an additional LSTM on the reversed sequence to the unidirectional LSTMmodel described in Figure 1. The model then predicts the label on the concatenated LSTM outputsof both ends of the sequence.</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/1.png" alt="Figure 1"></p><p>在对抗式和虚拟对抗式训练中，我们训练分类器对嵌入扰动具有鲁棒性，如图1b所示。第3节详细描述了这些扰动。目前，理解扰动是有界范数(bounded norm)就足够了。该模型可以通过学习具有非常大范数的嵌入使扰动变得无关紧要。为了防止这种病态的解决方案(pathological solution)，当我们将对抗性和虚拟对抗性训练应用于我们上面定义的模型时，我们用规范化嵌入(normalized embeddings)$\bar{v}_k$替换嵌入$v_k$，定义为</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/2.png" alt="公式1"></p><p>其中$f_i$是第$i$个单词的频率，在所有训练集中计算。</p><h2 id="ADVERSARIAL-AND-VIRTUAL-ADVERSARIAL-TRAINING-对抗与虚拟对抗训练"><a href="#ADVERSARIAL-AND-VIRTUAL-ADVERSARIAL-TRAINING-对抗与虚拟对抗训练" class="headerlink" title="ADVERSARIAL AND VIRTUAL ADVERSARIAL TRAINING 对抗与虚拟对抗训练"></a>ADVERSARIAL AND VIRTUAL ADVERSARIAL TRAINING 对抗与虚拟对抗训练</h2><p>对抗性训练（Goodfello et al.，2015）是一种新的正则化方法(novel regularization method)，用于分类器，以提高对较小、近似最坏情况扰动的鲁棒性(robustness to small, approximately worst case perturbations).</p><p>Let us denote $x$ as the input and $θ$ as the parameters of a classifier. When applied to a classifier, adversarial training adds the following term to the cost function:</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/g2.png" alt="公式2"></p><p>其中，r是输入上的扰动，$\hat{\theta}$ 是分类器当前参数的常数集。使用常量copy$\hat{\theta}$而不是θ,表明不应使用反向传播算法通过对抗性示例构建过程传播梯度( indicates that the back propagation algorithm should not be used to propagate gradients through the adversarial example construction process)。在训练的每个步骤中，我们根据公式（2）中的当前模型$P(y|x; \hat{\theta})$确定最坏情况扰动$r_{adv}$，并通过最小化公式（2）中关于θ的值来训练模型对此类扰动具有鲁棒性。然而，我们一般不能精确计算这个值，因为对于许多有趣的模型，如神经网络，精确的环面最小化是很难实现的。古德费罗等人（2015年）建议通过在x附近线性化$P(y|x; \hat{\theta})$来近似该值。利用线性近似和等式（2）中的$L_2$norm约束，产生的对抗性扰动为:</p><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/g3.png" alt="公式3 4"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/g4.png" alt="公式567"></p><p><img src= "/img/loading.gif" data-lazy-src="/2022/01/13/Paper-ADVERSARIAL-TRAINING-METHODS-FOR-SEMI-SUPERVISED-TEXT-CLASSIFICATION/g5.png" alt="公式8"></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pandas两表合并</title>
      <link href="/2021/12/29/pandas%E4%B8%A4%E8%A1%A8%E5%90%88%E5%B9%B6/"/>
      <url>/2021/12/29/pandas%E4%B8%A4%E8%A1%A8%E5%90%88%E5%B9%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="pandas删除特定列"><a href="#pandas删除特定列" class="headerlink" title="pandas删除特定列"></a>pandas删除特定列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.drop(<span class="number">5</span>, axis=<span class="number">1</span>) <span class="comment"># 行 axis=0</span></span><br></pre></td></tr></table></figure><h2 id="pandas-merge"><a href="#pandas-merge" class="headerlink" title="pandas merge"></a>pandas merge</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time: 2021/12/29 2:27 下午</span></span><br><span class="line"><span class="comment"># @Author: cgshuo</span></span><br><span class="line"><span class="comment"># @Email: cgshuo@163.com</span></span><br><span class="line"><span class="comment"># @File: get_newfam.py</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">fam_path = <span class="string">&quot;heart.QC.fam&quot;</span></span><br><span class="line">fam_new_path = <span class="string">&quot;../heart.QC.fam.new2&quot;</span></span><br><span class="line">pheno_path = <span class="string">&quot;../s1.train_pheno.txt&quot;</span></span><br><span class="line"></span><br><span class="line">pheno_df = pd.read_csv(pheno_path, sep=<span class="string">&#x27; &#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">fam_df = pd.read_csv(fam_path, sep=<span class="string">&#x27; &#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">pheno_df[<span class="number">2</span>] = pheno_df[<span class="number">2</span>].replace(<span class="number">1</span>, <span class="number">2</span>).replace(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># new_df = pd.concat([fam_df, pheno_df], join=&#x27;inner&#x27;, axis=1)</span></span><br><span class="line">new_df = pd.merge(fam_df, pheno_df, on=[<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># on 指定列</span></span><br><span class="line">new_df = new_df.drop(<span class="number">5</span>, axis=<span class="number">1</span>)</span><br><span class="line">new_df.to_csv(fam_new_path, sep=<span class="string">&#x27; &#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bootstrapped replications</title>
      <link href="/2021/12/28/bootstrapped-replications/"/>
      <url>/2021/12/28/bootstrapped-replications/</url>
      
        <content type="html"><![CDATA[<p>The bootstrap is done by taking your sample and resampling from it by sampling with replacement. Here is a small example.</p><p>Let’s say you have a sample of 6 observations {1,4,6,7,8,8.5}.</p><p>A bootstrap sample is a sample obtained by sampling with replacement 6 times. So a bootstrap sample could look like {1,4,4,7,7,8.5}. There are many possibilities. For each bootstrap sample you get a sample estimate. For a simple example let the estimate be for the mean of the distribution for the original sample that would be:</p><p>(1+4+6+7+8+8.5)&#x2F;6&#x3D;34.5&#x2F;6&#x3D;5.75</p><p>For the given bootstrap sample it is (1+4+4+7+7+8.5)&#x2F;6&#x3D;5.25.</p><p>Now there is a bootstrap distribution obtained by enumerating all possible distinct bootstrap samples and compute the estimate for each. But this is not usually done. So we simulate taking bootstrap samples at random many times.</p><p>Each bootstrap is called a replication. In your case you do 1000 replications. That many replications are needed for good confidence interval estimation. After taking the 1000 replications you have 1000 values for the sample mean (called the bootstrap distribution for the mean). From this sample take the appropriate percentiles of the bootstrap distribution to obtain the percentile method confidence interval.</p><p>For a 95% confidence interval you would take the 2.5th value from the bottom for the lower endpoint and the 97.5th value from the bottom to get the upper endpoint. Graphically you can make a histogram of the distribution and shade in the middle 95% of this distribution.</p><p>Efron named it bootstrap because it makes no special assumptions about the data other than iid. He thought of it as following the adage of picking yourself up by your own bootstraps.</p><p>假设您有一个包含 6 个观察值的样本 {1,4,6,7,8,8.5}.</p><p>bootstrap 样本是通过替换采样 6 次获得的样本。所以引导样本可能看起来像 {1,4,4,7,7,8.5}. 有很多可能性。对于每个引导样本，您都会得到一个样本估计。对于一个简单的例子，让估计是原始样本的分布均值，即：</p><p>(1+4+6+7+8+8.5)&#x2F;6&#x3D;34.5&#x2F;6&#x3D;5.75</p><p>对于给定的引导程序样本，它是 (1+4+4+7+7+8.5)&#x2F;6&#x3D;31.5&#x2F;6&#x3D;5.25.</p><p>现在通过枚举所有可能的不同 bootstrap 样本并计算每个样本的估计值，可以得到一个 bootstrap 分布。但通常不会这样做。所以我们模拟多次随机抽取引导样本。</p><p>每个引导程序称为一个复制。在您的情况下，您进行 1000 次复制。良好的置信区间估计需要多次重复。进行 1000 次重复后，样本均值有 1000 个值（称为均值的 bootstrap 分布）。从该样本中取自举分布的适当百分位数，以获得百分位数方法置信区间。</p><p>对于95% 的置信区间，您将从底部取第2.5 个值作为下端点，从底部取第97.5 个值来获得上端点。您可以在图形上绘制分布的直方图，并在该分布的中间95%处绘制阴影。</p><p>Efron 将其命名为 bootstrap，因为它对 iid 以外的数据没有任何特殊假设。他认为这是遵循自力更生的格言。</p>]]></content>
      
      
      <categories>
          
          <category> 生信123 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
            <tag> 统计学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生物知识积累</title>
      <link href="/2021/12/24/%E7%94%9F%E7%89%A9%E7%9F%A5%E8%AF%86%E7%A7%AF%E7%B4%AF/"/>
      <url>/2021/12/24/%E7%94%9F%E7%89%A9%E7%9F%A5%E8%AF%86%E7%A7%AF%E7%B4%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="PRS相关"><a href="#PRS相关" class="headerlink" title="PRS相关"></a>PRS相关</h2><p>SNP</p><p>loci</p><p>LD</p><p>GWAS</p><p>SNP</p><p>Palindromic SNP:<br>A sequence of DNA is a palindrome if the 5’ to 3’ (sense or forward) sequence is the same as the 3’ to 5’ (antisense or reverse). For example, the sequence ACCTAGGT (reading 5’ to 3’) is a palindrome because its complement (3’ to 5’) is TGGATCCA. <a href="https://mr-dictionary.mrcieu.ac.uk/term/palindrome/">参考链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 生信123 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper Parkinson’s disease determinants, prediction and gene–environment interactions in the UK Biobank</title>
      <link href="/2021/12/24/Paper-Parkinson%E2%80%99s-disease-determinants-prediction-and-gene%E2%80%93environment-interactions-in-the-UK-Biobank/"/>
      <url>/2021/12/24/Paper-Parkinson%E2%80%99s-disease-determinants-prediction-and-gene%E2%80%93environment-interactions-in-the-UK-Biobank/</url>
      
        <content type="html"><![CDATA[<p>Parkinson’s disease determinants, prediction and gene–environment interactions in the UK Biobank.</p><p>期刊：BMJ 时间：2020 IF&gt;30</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>帕金森病（PD）是全球第二常见的神经退行性疾病。当一个人被诊断为帕金森病时，黑质纹状体神经元的实质部分已经丢失。高危个体的识别和早期发现可能是开发有效治疗以预防或逆转PD进展的最佳机会。在过去十年中，帕金森病的大规模全基因组关联研究（GWAS）建立在罕见的帕金森病家族形式的连锁研究基础上。从最新的PD GWAs中，识别出90个独立信号，这些信号共同解释了约16%的总体PD遗传力。另外，流行病学研究已经确定了潜在的可改变暴露、各种共病和前驱症状。4.5一直在努力将这些非遗传风险因素纳入预测算法中，以识别PD风险较高的个体。6-9由遗传因素和PD环境风险因素的小个体效应大小解释的适度总体责任表明，它们之间的相互作用可以解释一些缺失的风险。对相互作用进行建模可以深入了解帕金森病的病理生物学，进一步改进预测算法，并提出通过干预基因突变组来改变风险的潜在方法。10–12在此，我们使用英国生物银行（UKB）队列和最新PD GWAS数据来评估环境和前驱因素与PD事件的关联，探索帕金森病多基因风险评分（PRS）如何提高结合这些因素的预测算法的性能，并探索PRS如何与环境&#x2F;前驱因素相互作用。</p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>数据：UKB</p><p>基因型数据：基因型数据使用Axiom（英国生物银行Axiom阵列，ThermoFisher）和英国BiLEVE阵列进行基因分型。基因分型、插补和质量控制程序在别处描述。19个遗传主成分由UKB提供（数据字段22 009）</p><h3 id="构建PRS"><a href="#构建PRS" class="headerlink" title="构建PRS"></a>构建PRS</h3><p>A variety of PRS were created using the clumping-and- thresholding approach:</p><ol><li><p>We extracted variant associations with PD from the most recent GWAS but not including the UKB participants from that GWAS</p></li><li><p>We excluded palindromic variants(回文变异) and variants without an rsID.</p></li><li><p>We excluded variants associated with PD above an arbitrary p value threshold (0.00005, 0.0005, 0.005, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, and 1).</p></li><li><p>We clumped using several r2 thresholds (0.1, 0.2, 0.4, 0.6, 0.8) and a clumping distance of 250kb, with the 1000 genomes EUR samples as the reference genome.</p></li></ol><p>参考基因组数据来自欧洲血统的503名1000基因组项目参与者。21只有在PD GWAS和target（UKB）数据集中通过质量控制的常染色体、双等位基因变异体被包括在内。我们排除了所有重复RSID、重复位置、偏离Hardy-Weinberg平衡的变异（p&lt;1e-06）、少数等位基因频率&lt;0.01的罕见变异、基因型缺失&gt;10%的变异和插补质量低的变异（马赫数R2&lt;0.3）。SNP质控后，参考数据集和目标数据集之间共有4490455个标记重叠。对于遗传分析，缺失基因型&gt;10%的个体被排除在外，仅包括自我报告的“英国白人”种族和遗传主成分定义的欧洲血统的个体。我们排除了亲属关系系数截止值为0.0442的每对个体中的一个，相当于三级亲属。亲属关系系数由UKB计算，并在“亲属关系”文件（263类）中提供.</p><p>作为确定尚未发现的基因组风险位点是否解释了对 PD 的额外责任的敏感性分析，我们使用表现最佳的 PRS（ in terms of Nagelkerke’s pseudo-R2)）创建了一个额外的 PRS。 对于最新的国际帕金森病基因组学联盟 (IPDGC) GWAS 中发现的 90 个风险位点，我们排除了主要 SNP 两侧 1MB 内的所有变体</p><p>每个位点的效应等位基因剂量乘以β系数，得出该位点的风险评分。每个SNP的得分标准化为平均值0和1单位方差。对于缺失基因型，该位点的得分被定义为该位点所有得分的平均值。将整个基因组的风险分数相加，以计算个体的分数。包括UKB中所有诊断为PD的个体，无论是流行还是偶发。分析是在PLINK（V.2.00aLM 64位英特尔）中使用“–score”标志执行的</p><h2 id="Statistical-methods"><a href="#Statistical-methods" class="headerlink" title="Statistical methods"></a>Statistical methods</h2><h3 id="incident-case-control-study"><a href="#incident-case-control-study" class="headerlink" title="incident case-control study"></a>incident case-control study</h3><p>使用整个UKB队列作为对照，并调整年龄、性别、种族和剥夺状态( age,  sex,  ethnicity  and  deprivation  status)，为每个风险因素建立多变量逻辑回归模型。模型的形式为：  PD  status  ~Age+Sex+Ethnici-ty+Townsend Deprivation status+risk factor。接下来，建立事件性帕金森病的多变量逻辑回归模型，包括所有与帕金森病风险密切相关的环境因素（错误发现率（FDR）&lt;0.05），包括上述混杂因素。似然比检验用于评估FDR阈值为0.05时模型拟合的改善情况，也就是说，对于每个风险因素，将上述模型与以下形式的空模型进行比较：  PD  status  ~Age+Sex+Eth-nicity+Townsend Deprivation status。对于性别特异性协变量(sex-specific covari-ates)，性别不作为( confounding  covariate.)混杂协变量。我们进行了以下敏感性分析：排除60岁以下的个体，排除非白人个体，排除仅通过自我报告和匹配病例对照分析得出PD诊断的个体（每个病例与四名参与者的年龄、种族和性别完全匹配）.</p><h3 id="Application-of-PREDICT-PD-algorithm"><a href="#Application-of-PREDICT-PD-algorithm" class="headerlink" title="Application of PREDICT-PD algorithm"></a>Application of PREDICT-PD algorithm</h3><p>我们将PREDICT-PD算法应用于UKB参与者，以外部验证该风险评分，并确定其预测性能是否通过添加遗传风险评分而得到增强。该算法使用已发表的相对风险和OR的估计值，这些估计值来自于对PD早期非运动特征和风险因素的大型荟萃分析(large meta-analyses)。4帕金森病的基线风险（在优势量表上(odds scale)）由以下等式确定<a href="#refer-anchor"><sup>1</sup></a>:</p><p>$$odds(PD)&#x3D;\frac{Pr(PD)}{1-Pr(PD)}&#x3D;\frac{1}{1+28.53049+73.67057 \times exp(-0.165308(Age-60))}$$</p><p>使用PREDICT-PD算法，根据是否存在以下缺陷对个体的基线年龄调整风险进行以下调整：女性（除以1.5）、当前吸烟（乘以0.44）、先前吸烟（乘以0.78）、PD家族史（乘以4.45），每天超过一杯咖啡（乘以0.67）、每周超过一杯酒精饮料（乘以0.9）、便秘（乘以2.34）、焦虑或抑郁（乘以1.86）和勃起功能障碍（乘以3.8）。females  (divided by 1.5), current smoking (multiplied by 0.44), previous smoking  (multiplied  by  0.78),  family  history  of  PD  (multiplied  by  4.45),  more  than  one  cup  of  coffee  per  day  (multiplied  by  0.67),  more  than  one  alcoholic  drink  per  week  (multiplied  by  0.9),  constipation  (multiplied  by  2.34),  anxiety  or  depression  (multiplied by 1.86) and erectile dysfunction (multiplied by 3.8).<br>The  final  odds  for  PD  was  converted  to  the  probability  scale  using the equation:</p><p>$$Pr(PD)&#x3D;\frac{Odds(PD)}{1+Odds(PD)}$$</p><p>To   assess   model   performance   as   a   covariate   in   logistic   regression  models,  the  PREDICT-PD  estimate  for  liability  was  converted to the log odds scale.</p><h3 id="Evaluation-of-model-performance"><a href="#Evaluation-of-model-performance" class="headerlink" title="Evaluation of model performance"></a>Evaluation of model performance</h3><p>为了确定PRS是否解释了UKB中的PD风险，以及它是否改进了现有的风险预测模型，我们首先将欧洲无关UKB参与者的子集分为一个训练集（30%）和一个测试集（70%）。训练集用于选择解释最大PD风险的PRS.</p><p>To evaluate PRS performance in the training set comprising 451 prevalent PD cases and 100446 controls, we used Nagelkerke’s pseudo-R2 metric comparing a full model (PD status ~Age+Sex+Townsend+PCs 1–4+PRS) to a null model (PD status ~Age+Sex+Townsend+PCs 1–4+PRS). Ninety-five per cent CIs were derived from 1000 bootstrap replicates using normal approximations as test statistics were approximately normally distributed. We selected the PRS with the highest absolute Nagelkerke’s pseudo-R2 for further validation.</p><p>To evaluate predictive model performance in the testing set, we calculated discrimination statistics (area under the curve (AUC)), calibration statistics and Nagelkerke’s pseudo-R2 using a variety of models.</p><h3 id="Gene-environment-interactions"><a href="#Gene-environment-interactions" class="headerlink" title="Gene-environment interactions"></a>Gene-environment interactions</h3><p>所有检测基因与环境相互作用的分析均在测试集中进行，以减轻因PRS过度拟合而产生的偏差。在加性和乘性尺度上评估相互作用。通过计算交互作用引起的归因比例（attributable proportion, AP），评估加性量表上的交互作用。加性交互作用分析基于多变量逻辑回归模型，将招募年龄、性别、剥夺和前四个遗传主成分作为混杂因素.</p><blockquote><p>All analyses examining gene–environment interactions were conducted in the testing set to mitigate against bias from overfit-ting of the PRS. Interactions were assessed on both the additive and the multiplicative scales. Interaction on the additive scale was assessed by calculating the attributable proportion (AP) due to interaction. Additive interaction analyses were based on multivariable logistic regression models incorporating age at recruitment, sex, deprivation and the first four genetic principal components as confounders.</p></blockquote><p>For a logistic regression model of the form:</p><p>$$log(\frac{p}{1-p})&#x3D;{\beta}<em>0 + {\beta}</em>{RF1}x + {\beta}<em>{RF2}y + {\beta}</em>{RF1+RF2}x \times y$$</p><p>in  which  (p&#x2F;(1-p))  is  the  log  odds  of  PD  and  are  the  values  of exposure variables (e.g. childhood body size, smoking, PRS) and is the interaction term, then the Relative Excess Risk due to Interaction (RERI) can be calculated as:<br>$$RERI &#x3D; exp({\beta}<em>{RF1}+{\beta}</em>{RF2}+{\beta}<em>{RF1+RF2}) - exp({\beta}</em>{RF1})-exp({\beta}_{RF2}) + 1$$</p><p>The AP can be conceived of as the proportion of the disease in  the  doubly  exposed  group  attributable  to  the  interaction  between the risk factors, (AP可被认为是由于风险因素之间的相互作用而导致的双重暴露组中疾病的比例),即,</p><p>$$AP&#x3D;\frac{RERI}{exp({\beta}<em>{RF1}+{\beta}</em>{RF2}+{\beta}_{RF1+RF2})}$$</p><p>This model can be expanded to include confounding covariates, in which case the beta coefficients are adjusted for confounders. Derivation  and  further  discussion  of  the  advantages  of  this  method over Rothman’s initial description can be found in Knol et  al.22  We  restricted  this  analysis  to  participants  with  geneti-cally European ancestry determined by both self-report (‘Cauca-sian’ in UKB data) and genetic ethnic grouping. For interaction analyses  using  the  PRS,  the  covariates  were  age,  sex,  current  deprivation  and  the  first  four  genetic  principal  components.  The PRS was transformed using the inverse normal transforma-tion and treated as a continuous variable for these analyses. For the  menarche  analysis,  age  at  menarche  was  also  transformed  using  the  inverse  normal  transformation.  CIs  for  the  AP  were  estimated using bootstrap resampling of the entire dataset with replacement for 5000 iterations.22 Ninety-five per cent CIs were derived from the 2.5th and 97.5th percentile values. Interactions on the multiplicative scale were assessed using a logistic regres-sion model incorporating an interaction term. The presence of multiplicative interaction was assessed using the likelihood ratio test, with an overall FDR threshold at 5%.</p><div id="refer-anchor"></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Noyce AJ, Bestwick JP, Silveira-Moriyama L, et al. PREDICT-PD: identifying risk of Parkinson’s disease in the community: methods and baseline results. J Neurol Neurosurg Psychiatry 2014;85:31–7</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> PRS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
            <tag> PRS </tag>
            
            <tag> Papers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>loss下降问题</title>
      <link href="/2021/12/23/loss%E4%B8%8B%E9%99%8D%E9%97%AE%E9%A2%98/"/>
      <url>/2021/12/23/loss%E4%B8%8B%E9%99%8D%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="loss的下降与上升"><a href="#loss的下降与上升" class="headerlink" title="loss的下降与上升"></a>loss的下降与上升</h2><ul><li>train loss 下降，val loss下降，说明网络仍在学习；~~ 奈斯，继续训练</li><li>train loss 下降，val loss上升，说明网络开始过拟合了；~~ 赶紧停止，然后数据增强、正则</li><li>train loss 不变，val loss不变，说明学习遇到瓶颈；~~ 调小学习率或批量数目</li><li>train loss 不变，val loss下降，说明数据集100%有问题；~~ 检查数据集标注有没有问题</li><li>train loss 上升，val loss上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。~~ 没有遇到过</li></ul><p>loss震荡？<br>轻微震荡一般是正常的，在一定范围内，一般来说 Batch Size 越大，其确定的下降方向越准，引起训练震荡越小，如果震荡十分剧烈，那估计是Batch Size设置的太小了。</p><p>学习率：0.0002</p><h2 id="batch-size与learning-rate"><a href="#batch-size与learning-rate" class="headerlink" title="batch_size与learning_rate"></a>batch_size与learning_rate</h2><p>从优化本身来看它们是影响模型性能收敛最重要的参数。学习率直接影响模型的收敛状态，batch_size则影响模型的泛化性能，两者又是分子分母的直接关系，相互也可影响。</p><p>可能存在的问题：</p><ul><li>学习率设置不合理(大多数情况),学习率设置太大,会造成不收敛,如果太小,会造成收敛速度非常慢</li><li>Batchsize太大,陷入到局部最优，无法达到全局最优，故而无法继续收敛；</li><li>网络容量，浅层网络完成复杂的任务loss不下降是肯定的，网络设计太简单，一般情况下，网络的层数和节点数量越大，拟合能力就越强，如果层数和节点不够多，无法拟合复杂的情况，也会造成不收敛</li></ul><p>过拟合存在的可能原因：</p><ul><li>出现过拟合的原因训练集的数量级和模型的复杂度不匹配</li><li>训练集的数量级要小于模型的复杂度；</li><li>训练集和测试集特征分布不一致；</li><li>样本里的噪音数据…</li></ul><p>解决方案：simpler model structure、 data augmentation、 regularization、 dropout、early stopping、ensemble、重新清洗数据</p><h2 id="batch-size"><a href="#batch-size" class="headerlink" title="batch_size"></a>batch_size</h2><p>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小</p><ul><li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li><li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li></ul><p>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。</p><p>大的batch_size性能下降是因为训练时间不够长，本质上并不是batch_size的问题，在同样的epochs下的参数更新变少了，因此需要更长的迭代次数。</p><ul><li>大的batchsize收敛到sharp minimum，而小的batchsize收敛到flat minimum，后者具有更好的泛化能力</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/2021/12/23/loss%E4%B8%8B%E9%99%8D%E9%97%AE%E9%A2%98/1.png" alt="1"></p><p>两者的区别就在于变化的趋势，一个快一个慢，如上图，造成这个现象的主要原因是小的batchsize带来的噪声有助于逃离sharp minimum</p><ul><li><p>Batchsize增加，学习率应随者增加<br>通常当我们增加batchsize为原来的N倍时，要保证经过同样的样本后更新的权重相等，按照线性缩放规则，学习率应该增加为原来的N倍。但是如果要保证权重的方差不变，则学习率应该增加为原来的sqrt(N)倍，目前这两种策略都被研究过，使用前者的明显居多。</p></li><li><p>提升Batchsize大小等同 添加学习率衰减</p></li></ul><p>总之，如果增加了学习率，那么batch size最好也跟着增加，这样收敛更稳定。</p><p>尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch size，学习率对模型的收敛影响真的很大，慎重调整。</p><p>用bn的坏处就是不能用太小的batch size，要不然mean和variance就偏了。所以现在一般是显存能放多少就放多少。而且实际调起模型来，真的是数据分布和预处理更为重要，数据不行的话 玩再多花招也没用</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>torch 常用数学分布</title>
      <link href="/2021/12/23/torch-%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E5%88%86%E5%B8%83/"/>
      <url>/2021/12/23/torch-%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h2 id="torch-tensor-uniform"><a href="#torch-tensor-uniform" class="headerlink" title="torch.tensor.uniform_()"></a>torch.tensor.uniform_()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensor.uniform_(from=0, to=1) → Tensor</span></span><br><span class="line">a = torch.empty(<span class="number">2</span>,<span class="number">3</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensor([[0.1526, 0.6311, 0.8407],</span></span><br><span class="line"><span class="comment">#        [0.4741, 0.5123, 0.1231]])</span></span><br></pre></td></tr></table></figure><p>Fills self tensor with numbers sampled from the continuous uniform distribution(连续均匀分布):<br>$$P(x)&#x3D;\frac{1}{to - from}$$</p><h2 id="torch-bernoulli"><a href="#torch-bernoulli" class="headerlink" title="torch.bernoulli()"></a>torch.bernoulli()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.bernoulli(input, *, generator=None, out=None) → Tensor</span></span><br><span class="line">a = torch.empty(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">a = torch.bernoulli(a)</span><br><span class="line"><span class="comment"># tensor([[ 1.,  0.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 0.,  0.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  1.,  1.]])</span></span><br></pre></td></tr></table></figure><p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.But, all values in inputs have to be in the range: $0 &lt; input_i &lt; 1$.</p><p>The $i^{th}$ element of the output tensor will draw a value 1 according to the $i^{th}$ probability value given in input.<br>$$ out_i &#x3D; Bernoulli(p&#x3D;input_i)$$<br>The returned out tensor only has values 0 or 1 and is of the same shape as input.</p><p>out can have integral dtype, but input must have floating point dtype.</p>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bgi log</title>
      <link href="/2021/12/23/bgi-log/"/>
      <url>/2021/12/23/bgi-log/</url>
      
        <content type="html"><![CDATA[<h2 id="端口转发"><a href="#端口转发" class="headerlink" title="端口转发"></a>端口转发</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L 8888:cngb-gpu-e15-1:9218 caoguangshuo@192.168.61.10</span><br></pre></td></tr></table></figure><h2 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h2><p>超大内存队列任务投递：<br>qsub -cwd -l vf&#x3D;300g,num_proc&#x3D;14 -P st_supermem -binding linear:14 -q st_supermem.q step1_generate_GRMinpart_20.sh</p><p>linear:14 每14行分成一个子任务</p><p>普通计算节点任务队列投递：<br>qsub -clear -cwd -l vf&#x3D;85g,num_proc&#x3D;10 -q st.q -P P20Z10200N0170 step1_generate_GRMinpart_20.sh</p><p>qsub -cwd -l vf&#x3D;50g,num_proc&#x3D;3 -P P20Z10200N0170 -q st.q merge_GRM_bin.sh<br>qsub -cwd -l vf&#x3D;30g,num_proc&#x3D;3 -P P20Z10200N0170 -q st.q merge_GRM_N_bin.sh</p><h2 id="投递任务"><a href="#投递任务" class="headerlink" title="投递任务"></a>投递任务</h2><p>qsub [-a date_time] [-c interval] [-C directive_prefix]<br>[-e path] [-I] [-j join] [-k keep] [-l resource_list] [-m mail_options] [-M user_list][-N name] [-o path] [-p priority] [-q destination]<br>[-r c][-S path_list] [-u user_list][-v variable_list] [-V] [-W additional_attributes] [-z]<br>[script]</p><p>qstat —— 用于查询作业状态信息</p><p>qdel —— 用于删除已提交的作业</p><p>qhold &amp; qrls —— 作业挂起 &amp; 作业释放</p><h2 id="shell提交任务"><a href="#shell提交任务" class="headerlink" title="shell提交任务"></a>shell提交任务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python train.py</span><br><span class="line">nohup python train.py</span><br><span class="line">nohup python train.py &gt;&gt; ./Output/epoch200.log &amp;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">&amp;:后台运行</span></span><br><span class="line"></span><br><span class="line">ssh -N -f -L 8888:cngb-gpu-e15-1:9218 caoguangshuo@192.168.61.10</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Paper Neural Distance Embeddings for Biological Sequences</title>
      <link href="/2021/12/21/Paper-Neural-Distance-Embeddings-for-Biological-Sequences/"/>
      <url>/2021/12/21/Paper-Neural-Distance-Embeddings-for-Biological-Sequences/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在进化过程中，生物序列不断发生变异，生物研究的很大一部分是基于对这些变异的分析。 生物学家开发了准确的统计模型，根据编辑距离 D(s1,s2) 估计序列对之间的进化距离：将字符串 s1 转换为另一个字符串 s2 所需的（加权）插入、删除或替换的最小数量。</p><p>然而，使用传统方法计算此编辑距离核 D 必然具有二次复杂性且难以并行化，使其计算成为大规模分析（例如微生物组研究）中的瓶颈。此外，在诸如层次聚类和多序列比对等关键任务的基础上，准确计算多个序列之间的相似性，即使对于相对少量的序列，在计算上也是难以处理的。在其他空间中相对简单的问题在由编辑距离定义的序列空间中变得难以组合。例如，找到 Steiner 串，生物信息学中的一个经典问题，可以被认为是计算序列空间中的几何中位数，是 NP 完全的(NP-complete)。</p><p>常见的自监督学习方法在 NLP 中非常成功，但在生物环境中效果较差，因为关系往往在序列之间而不是碱基之间。<br>在这项工作中，我们提出了神经距离嵌入 (NeuroSEED)，这是一个生成生物序列表示的通用框架，其中嵌入空间中的距离与序列之间的进化距离 D 相关。 NeuroSEED 提供距离核 D 的快速近似、生物序列的低维表示、嵌入几何中多个序列之间关系的易处理分析以及解码新序列的方法。</p><p>首先，我们将几种现有方法重新表述为 NeuroSEED，突出了它们的贡献和局限性。然后，我们检查嵌入序列的任务，以保留作为框架基础的编辑距离。该分析揭示了依赖于数据的方法以及使用与基础数据分布匹配良好的几何结构的重要性。双曲线空间能够捕捉生物进化给出的隐含层次结构，并在嵌入 RMSE 时与最佳竞争几何体相比平均减少 22%。<br>我们通过分析生物信息学中涉及多个序列之间关系的两个基本任务来展示该框架的潜力及其广泛的适用性：层次聚类和多序列比对。对于这两项任务，使用 NeuroSEED 编码器的无监督方法能够匹配常见启发式算法的准确性，同时速度要快几个数量级。对于层次聚类，我们还探索了一种基于双曲线空间中 Dasgupta 成本的连续松弛的方法，该方法在类似的质量水平下提供 15 倍的运行时间减少。最后，对于多序列比对，我们设计了一种基于变分自动编码器的原始方法，该方法与竞争基线的性能相匹配，同时显着降低了运行时复杂性.</p><p>作为总结，我们的贡献是：（i）我们引入了 NeuroSEED，这是一个在几何向量空间中映射序列的通用框架，并将现有方法重新表述为它。 (ii) 我们展示了双曲线空间如何为生物序列的数据相关分析带来显着改进。 (iii) 我们针对经典生物信息学问题提出了几种启发式方法，这些方法可以构建在 NeuroSEED 嵌入之上，并显着减少经典基线的运行时间。</p><h2 id="Bioinformatics-tasks"><a href="#Bioinformatics-tasks" class="headerlink" title="Bioinformatics tasks"></a>Bioinformatics tasks</h2><p>Edit distance approximation.编辑距离近似值,在这项工作中，我们总是处理经典的编辑距离，其中每个字符串操作都具有相同的权重，但所有开发的方法都可以应用于任何选择的距离函数（作为预言机给出）。例如，当使用氨基酸序列时，经典替代矩阵的不同度量变体之一，如 mPAM250 [15] 将是一个不错的选择。作为基线近似启发式，我们采用 k-mer [5]，这是最常用的无对齐方法，通过一定长度的子序列的频率向量表示序列，以及 FFP [16]，另一种无对齐方法，查看 k-mers 分布之间的 Jensen-Shannon 散度。</p><p>Hierarchical clustering (HC).层次聚类 (HC)发现进化史给出的内在层次结构是许多生物学分析的关键步骤。分层聚类 (HC) 包括，在给定成对距离函数的情况下，定义一棵树，其中内部点对应于集群，叶子对应于数据点。 Dasgupta 的成本 [17] 衡量生成的树对数据点之间相似性的尊重程度。作为基线，我们考虑经典的凝聚聚类算法（Single [18]、Complete [19] 和 Average Linkage [6]）和最近的技术 [20]，该技术在双曲线空间中使用 Dasgupta 成本的连续松弛。</p><p>Multiple sequence alignment (MSA).多序列比对 (MSA),比对三个或更多序列用于识别活性和结合位点以及保守的蛋白质结构，但找到其最佳解决方案是 NP 完全的。 MSA 的一个相关任务是 Steiner 字符串的近似，它最小化集合中序列的距离总和（共识误差）。 数据集 为了评估启发式，我们选择了三个包含 16S rRNA 基因不同部分的数据集微生物组分析 [21]，这是我们方法最有前途的应用之一。第一个是 Qiita [21]，包含超过 6M 长达 152 bp 的序列，覆盖 V4 高变区。第二个，RT988 [11]，只有 6.7k 公开可用的序列，长度可达 465 bp，覆盖 V3-V4 区域。两个数据集均由 Illumina MiSeq [22] 生成，包含长度大致相同的序列。 Qiita 来自皮肤、唾液和粪便样本，而 RT988 来自口腔斑块。第三个数据集是 Greengenes 全长 16S rRNA 数据库 [23]，其中包含超过 1M 长度在 1,111 到 2,368 之间的序列。此外，我们使用合成生成序列的数据集来测试依赖于数据的方法的重要性。附录 B.4 中提供了每个任务的数据拆分的完整描述。</p><p>数据集, 为了评估启发式方法，我们选择了三个包含 16S rRNA 基因不同部分的数据集，这在微生物组分析中至关重要 [21]，这是我们方法最有前途的应用之一。第一个是 Qiita [21]，包含超过 6M 长达 152 bp 的序列，覆盖 V4 高变区。第二个，RT988 [11]，只有 6.7k 公开可用的序列，长度可达 465 bp，覆盖 V3-V4 区域。两个数据集均由 Illumina MiSeq [22] 生成，包含长度大致相同的序列。 Qiita 来自皮肤、唾液和粪便样本，而 RT988 来自口腔斑块。第三个数据集是 Greengenes 全长 16S rRNA 数据库 [23]，其中包含超过 1M 长度在 1,111 到 2,368 之间的序列。此外，我们使用合成生成序列的数据集来测试依赖于数据的方法的重要性。附录 B.4 中提供了每个任务的数据拆分的完整描述。</p><h2 id="Neural-Distance-Embedding"><a href="#Neural-Distance-Embedding" class="headerlink" title="Neural Distance Embedding"></a>Neural Distance Embedding</h2><p>NeuroSEED 框架背后的基本思想（如图 1 所示）是在连续空间中映射序列，以便嵌入点之间的距离与序列之间的距离相关联。 给定序列的分布和它们之间的距离函数 D，任何 NeuroSEED 方法都由四个主要组件构成：嵌入几何、编码器模型、解码器模型和损失函数</p><p>Embedding geometry. 嵌入点之间的距离函数$d$定义了嵌入空间的几何形状。 虽然之前的工作[11、24、25、26、27]大多忽略了这个因素，但我们表明，这种几何形状反映域中序列之间的关系至关重要。 在我们的实验中，我们提供了欧几里得、曼哈顿、余弦、平方欧几里得（称为 Square）和双曲距离（详见附录 D）之间的比较。</p><p>Encoder model. 编码器模型 $f_θ$ 将序列映射到嵌入空间中的点。 在这项工作中，我们测试了各种模型作为编码器功能：线性层、MLP、CNN、GRU [28] 和具有局部和全局注意力的转换器 [29]。 附录 C 中提供了有关模型如何适应序列的详细信息。 Chen 等人 [24] 提出了 CSM，一种基于子序列卷积的编码器架构。 然而，正如 Koide 等人所指出的那样。 [12]，当各种层堆叠时，该模型表现不佳，并且由于动态编程例程中单元的相互依赖性，它无法在 GPU 上有效地并行化。</p><p>Decoder model. 对于某些任务，从嵌入空间解码序列也很有用。 这个想法在第 7.2 节中使用，并且在与 NeuroSEED 相关的作品中很新颖，能够将框架应用于更广泛的问题</p><p>Loss function. 训练 NeuroSEED 编码器的最简单方法是直接将序列距离与其近似值之间的 MSE 最小化为嵌入之间的距离：</p><p>$$L(\theta, S)&#x3D;\Sum_{s_1, s_2 \in S}(D(s_1,s_2)-\alpha d(f_{\theta}(s_1), f_{\theta}(s_2)))^2$$</p><p>其中 α 是一个常数或可学习的标量。 根据学习嵌入的应用程序，MSE 损失可以组合或替换为其他损失函数，例如最近字符串检索的三元组损失（附录 F），放宽 Dasgupta 的分层聚类成本（第 7.1 节） 或多序列比对的序列重建损失（第 7.2 节）。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/12/21/Paper-Neural-Distance-Embeddings-for-Biological-Sequences/1.png" alt="1"></p><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>Hyperbolic embeddings, 双曲几何是一种具有恒定负截面曲率的非欧几何，通常被称为树的连续版本，因为它能够以任意低的失真嵌入树。 在双曲空间中映射具有隐式或显式层次结构的对象的优势也已在其他领域中得到体现 [30, 31, 32, 10]。 相比之下，这项工作处理由生物序列中的编辑距离定义的非常不同的空间，并且与之前的大多数工作不同，我们不仅为特定的一组数据点导出嵌入，而且训练编码器从 空间中的域。</p><p>Sequence Distance Embeddings , 在更易于计算的空间中工作的明显优势激发了对序列距离嵌入 [33]（也称为低失真嵌入(low-distortion embeddings)）方法的重要研究，用于编辑距离的变体 [34, 35, 36, 37, 38, 39, 40]。 然而，它们都与数据无关，并且在“不受约束”的编辑距离上表现不佳。</p><p>Hashing and metric learning , NeuroSEED 也非常适合学习散列 [41] 的更广泛研究，其目标通常是将高维向量空间映射到更小的保留距离的空间中。 与 NeuroSEED 相关的另一个领域是度量学习 [42, 43]，其中训练模型以从相似和不同对的示例中学习嵌入.</p><p>Locality-sensitive hashing ,局部敏感哈希,最后，所提出的工作是对局部敏感哈希方法研究路线的补充。 研究人员开发了一系列启发式方法，特别是针对序列聚类 [44, 45] 和局部对齐 [46] 的任务。 这些用作基于对齐或无对齐方法的子程序嵌入&#x2F;特征，因此，属于我们在论文中强调的局限性。 未来的工作可以分析如何使用 NeuroSEED 嵌入来改进这些启发式方法</p><h2 id="Edit-distance-approximation"><a href="#Edit-distance-approximation" class="headerlink" title="Edit distance approximation"></a>Edit distance approximation</h2><p>双曲线空间 最有趣和新颖的结果来自对嵌入空间几何形状的分析。在这些生物数据集中，存在隐含的层次结构，双曲空间很好地反映了这一点。由于这种密切的对应关系，即使是相对简单的模型在双曲线距离上也表现得非常好。在卷积模型中，双曲线空间与每个模​​型的最佳竞争几何相比，平均 RMSE 减少了 22%。在分析所需的维度时，使用双曲线空间的好处是显而易见的（图 4）。双曲线空间提供了显着更有效的嵌入，该模型在维度 32 处达到了“肘部”，并且与其他维度为 128 的空间的性能相匹配，只有 4 到 16。鉴于存储嵌入的空间和时间计算它们之间的距离与维度成线性比例，这为下游任务提供了比其他 NeuroSEED 方法的显着改进.</p><h2 id="Supervised-heuristics-启发式"><a href="#Supervised-heuristics-启发式" class="headerlink" title="Supervised heuristics(启发式)"></a>Supervised heuristics(启发式)</h2><p>在本节中，我们提出并评估了两种使 NeuroSEED 适应层次聚类和具有定制损失函数的多序列比对任务的方法。</p><p>Relaxed approach to hierarchical clustering. 层次聚类的另一种方法是使用 Dasgupta 成本 [17] 的连续松弛 [20] 作为损失函数，将序列嵌入双曲线空间中（有关更多详细信息，请参见附录 B.2 和 [20]）。 与 Chami 等人相比。 [20]，我们表明可以通过将序列直接映射到空间来显着减少所需的成对距离的数量。 这可以大大加快构建速度，尤其是在不需要任何预训练模型的情况下处理大量序列时。</p><p>Steiner string approach to multiple sequence alignment<br>多序列比对的另一种方法是使用向量空间中的解码器在连续优化任务中转换 Steiner 字符串近似问题（附录 B.3）。</p><p>这种方法在图 8 中进行了总结并在附录 G 中进行了详细说明，它包括训练一个自动编码器来将序列映射到一个连续空间并保留距离，一次只使用一对序列（其中计算距离是可行的）。这是通过在损失函数中结合一个用于潜在空间编辑距离近似的组件和一个用于解码器的重建精度的组件来实现的。第一个表示为潜在空间中编辑距离和向量距离之间的 MSE。第二个由解码器输出与实际序列的平均元素交叉熵损失组成。在测试时，编码器将所有序列嵌入集合中，通过相对简单的优化程序找到几何中点（最小化嵌入空间中的距离总和），然后使用解码器找到 Steiner 字符串的近似值.在训练期间，高斯噪声被添加到潜在空间中的嵌入点，迫使解码器对不是由编码器直接产生的点具有鲁棒性。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/12/21/Paper-Neural-Distance-Embeddings-for-Biological-Sequences/2.png" alt="2"></p><h2 id="conclusion"><a href="#conclusion" class="headerlink" title="conclusion"></a>conclusion</h2><p>在这项工作中，我们提出并探索了神经距离嵌入，这是一个利用表征学习的最新进展将生物序列嵌入几何向量空间的框架。通过研究近似序列之间进化编辑距离的能力，我们展示了反映生物层次结构的双曲线空间所提供的强大优势。</p><p>然后，我们证明了 NeuroSEED 在层次聚类和多序列比对问题上的有效性和广泛适用性。对于每项任务，我们尝试了两种不同的方法：一种是无监督的将 NeuroSEED 嵌入绑定到现有的启发式算法中，另一种是通过定制的损失函数直接利用嵌入空间的几何形状。在所有情况下，所提出的方法与现有基线相当或优于现有基线，同时速度明显更快。</p><p>最后，NeuroSEED 提供了非常适合人类交互的表示，因为生成的嵌入可以可视化并易于解释。为了实现这一目标，双曲空间的非常紧凑的表示至关重要 [10]。这项工作还为未来的研究方向提供了许多机会，包括不同类型的序列、标签、架构和任务。我们在附录中提出并激发这些方向</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Corso, Gabriele, et al. “Neural Distance Embeddings for Biological Sequences.” Advances in Neural Information Processing Systems 34 (2021).</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> NeurIPS </tag>
            
            <tag> 生物信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper PRS Chronic obstructive pulmonary disease</title>
      <link href="/2021/12/14/Paper-PRS-Chronic-obstructive-pulmonary-disease/"/>
      <url>/2021/12/14/Paper-PRS-Chronic-obstructive-pulmonary-disease/</url>
      
        <content type="html"><![CDATA[<h2 id="Chronic-obstructive-pulmonary-disease-and-related-phenotypes-polygenic-risk-scores-in-population-based-and-case-control-cohorts"><a href="#Chronic-obstructive-pulmonary-disease-and-related-phenotypes-polygenic-risk-scores-in-population-based-and-case-control-cohorts" class="headerlink" title="Chronic obstructive pulmonary disease and related phenotypes: polygenic risk scores in population-based and case-control cohorts"></a>Chronic obstructive pulmonary disease and related phenotypes: polygenic risk scores in population-based and case-control cohorts</h2><p>慢性阻塞性肺疾病和相关表型：基于人群和病例对照队列的多基因风险评分</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="OR、CI、SD"><a href="#OR、CI、SD" class="headerlink" title="OR、CI、SD"></a>OR、CI、SD</h3><p>OR值的全称是odd ratio, 又称比值比，对于发病率很低的疾病来说，OR值即是相对危险度的精确估计值。<br>logistic回归中，OR值&#x3D;1，表示该因素对疾病的发生不起作用</p><ul><li>OR值大于1，表示该因素是一个危险因素</li><li>OR值小于1，表示该因素是一个保护因素</li></ul><p>CI全称是Confidence Interval,又称可信区间，区间估计是指按预先给定的概率，计算出一个区间，使它能够包含未知的总体均数。事先给定的概率1-α称为可信度（通常取0.95或0.99），计算得到的区间称为可信区间</p><p>SD,Standard Deviation标准差</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Background：Genetic factors influence chronic obstructive pulmonary disease (COPD) risk, but the individual variants that have been identified have small effects. We hypothesised that a polygenic risk score using additional variants would predict COPD and associated phenotypes.</p><p>Methods: We constructed a polygenic risk score using a genome-wide association study of lung function(肺功能) (FEV1 and FEV1&#x2F;forced vital capacity [FVC]) from the UK Biobank and SpiroMeta. We tested this polygenic risk score in nine cohorts(队列) of multiple ethnicities(多种族) for an association with moderate-to-severe(中重症) COPD (defined as FEV1&#x2F;FVC &lt;0·7 and FEV1 &lt;80% of predicted). Associations were tested using logistic regression models, adjusting for age, sex, height, smoking pack-years, and principal components of genetic ancestry. We assessed predictive performance of models by area under the curve. In a subset of studies, we also studied quantitative and qualitative CT imaging phenotypes that reflect parenchymal(实质的) and airway(呼吸道) pathology(病理学), and patterns of reduced lung growth. </p><p>注：FEV1&#x2F;FVC简称一秒率，是指第一秒用力呼气量占所有呼气量的比例,临床上常以FEV1&#x2F;用力肺活量FVC的比值（一秒率）做判定，正常值为83%. </p><p>Findings: The polygenic risk score was associated with COPD in European (odds ratio [OR] per SD 1·81 [95% CI 1·74–1·88] and non-European (1·42 [1·34–1·51]) populations. Compared with the first decile(十分位数), the tenth decile of the polygenic risk score was associated with COPD, with an OR of 7·99 (6·56–9·72) in European ancestry and 4·83 (3·45–6·77) in non-European ancestry cohorts. The polygenic risk score was superior to previously described genetic risk scores and, when combined with clinical risk factors (ie, age, sex, and smoking pack-years), showed improved prediction for COPD compared with a model comprising clinical risk factors alone (AUC 0·80 [0·79–0·81] vs 0·76 [0·75–0·76]). The polygenic risk score was associated with CT imaging phenotypes, including wall area percent, quantitative and qualitative measures of emphysema, local histogram emphysema patterns, and destructive emphysema subtypes. The polygenic risk score was associated with a reduced lung growth pattern. 多基因风险评分优于先前描述的遗传风险评分，并且当与临床风险因素（即年龄、性别和吸烟包年）相结合时，显示出更好的预测COPD 与仅包含临床风险因素的模型相比（AUC 0·80 [0·79–0·81] vs 0·76 [0·75-0·76]），多基因风险评分与 CT 成像表型相关，包括壁面积百分比、肺气肿的定量和定性测量、局部直方图肺气肿模式和破坏性肺气肿亚型。多基因风险评分与肺生长模式降低有关。</p><p>Interpretation: A risk score comprised of genetic variants can identify a small subset of individuals at markedly increased risk for moderate-to-severe COPD, emphysema subtypes associated with cigarette smoking, and patterns of reduced lung growth. 解释 由遗传变异组成的风险评分可以识别出中度至重度 COPD、与吸烟相关的肺气肿亚型以及肺生长减缓模式显着增加的一小部分个体</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>慢性阻塞性肺疾病 (COPD) 的特征是不可逆的气流受限。慢性阻塞性肺疾病全球倡议 (GOLD) 根据低 FEV1&#x2F;用力肺活量 (FVC) 比值，通过存在持续性呼吸道症状并伴有气流阻塞来定义 COPD，并根据支气管扩张剂用量的减少对肺功能严重程度进行分级预测 FEV1 的百分比（% 预测）。COPD 主要发生在有毒环境暴露的背景下，包括吸烟和生物燃料燃烧。然而，并非所有暴露的个体都会出现气流阻塞，这表明某些个体可能对该疾病具有遗传易感性。</p><p>COPD 的遗传力估计值通常在 37% 到 50% 之间。 COPD 和肺功能的全基因组关联研究 (GWAS) 已经确定了许多与 COPD 风险相关的遗传变异。这些 GWAS 变体中的每一个的效果大小通常都很小。然而，虽然每个个体变异只解释了一小部分 COPD 风险，但将许多遗传变异组合成一个单一的遗传风险评分可以解释更大比例的风险。已经为肺功能开发了遗传风险评分，对 COPD 具有预测能力 基于更大的 GWAS 的遗传风险评分，包括更多变体，往往表现出更高的预测性能</p><p>尽管肺功能的遗传风险评分可以预测 COPD，但遗传风险评分可以在多大程度上反映 COPD 异质性尚不清楚。COPD 患者的气道和肺实质受累程度差异很大，个体 COPD GWAS 变异与定量成像特征相关，例如气道壁厚和肺气肿。Oelsner 及其同事 16 从先前的 79 个变异遗传风险评分中得出GWAS 用于肺功能并确定了与胸部 CT 定量成像特征的关联。然而，由大量肺功能变异组成的遗传风险评分是否会导致与更广泛的定量和定性 CT 成像特征的更强关联尚不清楚。</p><p>遗传风险评分代表一组精心挑选的变体，这些变体要么未加权，要么在回归模型中的其他变体的背景下加权。我们使用术语多基因风险评分来指代包含整个基因组变异的风险评分，权重来自 GWAS。在心血管疾病中，包括未达到全基因组显着性的变异的多基因风险评分具有更高的效力，并确定了疾病风险显着增加的大部分人群。因此，肺功能的多基因风险评分可能包括变异在预测复杂性状（如 COPD）方面，未达到全基因组显着性将比遗传风险评分更准确。此外，一些在生命早期肺部生长减慢的个体有患 COPD 的风险。COPD 和肺功能 GWAS 变异与人体测量特征（例如，身高）相关，并且富含肺发育途径。 尚不清楚肺功能遗传变异的风险评分是否与肺生长模式相关</p><p>我们假设使用最大的可用肺功能全基因组遗传研究的完整结果开发的多基因风险评分将改善 COPD 的预测并识别疾病风险显着增加的个体。因为在个体满足 COPD 的 GOLD 肺活量测定标准之前，肺功能下降可能会连续发生，我们开发了基于肺功能的多基因风险评分（即 FEV1 和 FEV1&#x2F;FVC 比值），然后测试了多基因风险的预测能力COPD 评分。我们还试图确定评分是否与特定的定量和定性 CT 成像表型和肺生长模式有关。为了验证这一假设，我们根据 FEV1 和 FEV1&#x2F;FVC 比率制定了个体多基因风险评分，并将这些评分合并为一个组合的多基因风险评分。我们在另外九个独立队列中测试了这种组合风险评分的影响，包括基于人群和病例对照设计、多个种族和族裔群体以及哮喘儿童。</p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>研究人群</p><p>FEV1 和 FEV1&#x2F;FVC 的 GWAS 是为英国生物银行和 SpiroMeta 的参与者完成的。7 我们使用来自挪威卑尔根的 GenKOLS 病例对照研究 20-22 来调整超参数。我们在一系列种族的病例对照和基于人群的研究中计算了多基因风险评分。病例对照研究包括 COPDGene（非西班牙裔白人和非洲裔美国人的参与者）、ECLIPSE、NETT 和规范老化研究 (NAS)、SPIROMICS 和肺健康研究 (LHS)。29,30 基于人群的研究包括 MESA（非洲裔美国人）美国、非西班牙裔白人、西班牙裔和中国参与者）、31,32 心血管健康研究（CHS；非裔美国人和欧洲血统参与者）、33 鹿特丹研究（所有三个队列）、34 和江原大学的一项研究.35 对于肺功能生长模式，我们使用儿童哮喘管理计划 (CAMP) 中的个人进行了分析。 CAMP 是一项针对 1041 名轻度至中度哮喘儿童（入组时年龄为 5-12 岁）进行抗炎治疗的随机安慰剂对照试验，随访时间为 13 年，流失率低（≤20%）。给予知情同意，研究方案得到当地研究伦理委员会和机构审查委员会的批准。附录（第 2-5 页）中提供了研究人群的其他详细信息。</p><p>结果</p><p>主要结局指标是中度至重度 COPD（FEV &#x2F; FVC &lt;0·7 和 FEV &lt;80% 的预测值）。作为次要结局指标，我们评估了联合多基因风险评分与吸烟、急性加重的发生、GOLD 肺活量测定分级、临床 COPD 表型、成像表型和肺生长模式的关联。我们还评估了多基因风险评分（单独和结合临床 COPD 风险因素）在预测 COPD 方面的表现，并将其与临床风险评分的表现进行了比较。</p><p>多基因风险评分的推导</p><p>为了开发 FEV1 和 FEV1&#x2F;FVC 的个体多基因风险评分，我们根据英国生物银行和 SpiroMeta 中 FEV1 和 FEV1&#x2F;FVC 的 GWAS 的效应大小生成了权重。 7 为了减少研究之间遗传变异退出的机会，我们包括在四个队列中基因分型或推算良好 (R2&gt;0·5) 的变异：COPDGene、GenKOLS、ECLIPSE 和 NETT&#x2F;NAS。然后，我们应用了惩罚回归框架，解释了连锁不平衡（lassosum v0.4.4），38 其中使用英国生物银行中的欧洲血统个体计算了连锁不平衡。 39 为了确定 lassosum 的超参数（λ 和收缩），我们使用了GenKOLS 病例对照研究。我们选择 GenKOLS 是为了避免在 COPDGene 研究中训练任何模型参数以保留 COPDGene 以进行测试，并且因为 GenKOLS 是一项强有力且平衡的病例对照研究。根据 GOLD 标准，中度至重度 COPD 的主要结局指标需要降低 FEV1 和降低 FEV1&#x2F;FVC 以进行诊断。因此，我们使用 FEV1 和 FEV1&#x2F;FVC 的两个单独多基因风险评分的加权总和创建了一个单一的组合多基因风险评分。为此，我们建立了 COPD 的逻辑回归模型，其中包括 GenKOLS 中 FEV1 和 FEV1&#x2F;FVC 的个体多基因风险评分，并使用它们的回归系数作为权重来计算组合评分。</p><p>为了测试评分对用于参数调整的队列的敏感性，我们使用四个队列之一（COPDGene 非西班牙裔白人个体、ECLIPSE、NETT&#x2F;NAS 或 GenKOLS）调整了多基因风险评分，然后测试了预测能力其他三个队列。</p><p>统计分析<br>我们使用得到的回归模型计算来自九项研究（COPDGene、CHS、ECLIPSE、江原大学、LHS、MESA、NETT&#x2F;NAS、鹿特丹研究和 SPIROMICS）的参与者的多基因风险评分。对于每个队列，FEV1 和 FEV1&#x2F;FVC 多基因风险评分均以平均值和标准差为中心并进行缩放。我们使用 Pearson 相关系数检查了组合多基因风险评分与吸烟包年的相关性。为了估计吸烟的影响，我们计算了吸烟暴露的人群归因风险 (PAR%) 和暴露人群归因风险 (AR%) 的比例（按 20 包年划分（&gt;20 包年 vs ≤20 包年） -年）使用 COPDGene 非西班牙裔白人参与者。</p><p>为了评估主要结果指标，我们使用逻辑回归模型测试了多基因风险评分与 COPD（中度至重度，除非另有说明）之间的关联，调整了年龄、性别、身高、吸烟包年数和主要成分遗传血统，以及 CHS 的研究诊所。我们测试了多基因风险评分与频繁加重（12 个月内加重 &gt;1 次；加重定义为自我报告的呼吸系统健康恶化，需要用皮质类固醇、抗生素或两者同时治疗）和严重加重（需要急诊室COPDGene 和 ECLIPSE 队列中的访问或住院），调整饲料、性别和包装年；然后再次测试这些模型，调整基线 FEV1 和 FEV1&#x2F;FVC。</p><p>为了评估 COPD 多基因风险评分的预测性能，我们使用 R 版本 3.5.1 中的 pROC 估计了曲线下面积 (AUC)。我们评估了以下模型：1）单独的多基因风险评分； 2) 仅传统 COPD 临床危险因素（年龄、性别和吸烟包年）； 3) COPD临床危险因素和多基因危险评分。我们在十个亚群中比较了这些模型，得到 Bonferroni 校正的 p 值为 0.005。我们还从吸烟 10 包年或以上的英国生物银行参与者中得出临床风险评分，并估计了 COPDGene 和 ECLIPSE 研究中的 AUC。根据约登指数选择临床风险评分和多基因风险评分的临界值，40 并计算性能特征。所有荟萃分析均使用 R (v4.9-7) 中的元软件包进行。41 由于多基因风险评分是在欧洲血统队列中开发的，我们分别检查了欧洲和非欧洲血统队列。我们通过反方差加权和有效样本量加权对 AUC 进行了荟萃分析；42 这些荟萃分析对欧洲人使用了固定效应方法，但我们对非欧洲人进行了随机效应分析以解释种族的多样性祖先。我们通过组合多基因风险评分十分位数和三分位数对每项研究中的参与者进行分组，并将最高和最低十分位数相互比较并与中间三分位数进行比较。</p><p>我们评估了联合多基因风险评分预测 COPD 的性能，发现 AUC 为 0·67（95% CI 0·66–0·68）。仅包含多基因风险评分的模型的预测能力低于仅包含临床COPD危险因素（年龄、性别和吸烟包年）的模型；然而，同时包含多基因风险评分和 COPD 风险因素的模型比单独包含临床风险因素的模型表现更好（多基因风险评分加临床因素的 AUC 0·80 [0·79–0·81] vs 0·76 [ 0·75-0·76] 仅针对临床因素；p&#x3D;1·3 × 10-42；图 4，附录 14-15 页）。无论我们使用反方差加权还是有效样本量加权对 AUC 进行元分析，都获得了类似的结果</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/12/14/Paper-PRS-Chronic-obstructive-pulmonary-disease/prs1.png" alt="fig1"></p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>在这项分析中，我们使用来自超过 40 万名参与者的 GWAS 汇总统计数据开发了一个多基因风险评分，并用它来预测九个基于人群和病例对照队列的 COPD 诊断。与之前的研究相比，该评分使用了更多的变体和更大的样本量，并在更多的验证队列中进行了测试。7,11,14,16 该评分还与 CT 成像表型和肺生长减少的模式有关，这可能会使个体易患到慢阻肺。</p><p>我们结合 FEV1 和 FEV1&#x2F;FVC 多基因风险评分对 COPD 的预测性能优于之前的研究。大约 12 000 人的 GWAS 被用于开发一个未加权的 30 变体遗传风险评分，该评分对 COPD 具有中等 (AUC 0·58) 预测能力。 14 在包括近 49 000 名英国生物银行参与者的 GWAS 中，95 变体风险当比较风险评分的最高和最低十分位数时，评分有 3·7 倍的 COPD 风险。11 在一个更大的 GWAS7 中，包括超过 400000 个人，在多个外部测试中测试了 279 个变量的肺功能加权遗传风险评分。在验证队列中，与最低十分位数相比，最高十分位数与 COPD 的几率高 4·73 倍。这些数据表明，具有更准确权重和更多变体的更大 GWAS 可以提高遗传风险评分的预测性能。与该建议一致，当比较欧洲人群风险评分的最高和最低十分位数时，我们的多基因风险评分与 COPD 几率增加 7·99 倍相关。此外，我们多基因风险评分的荟萃分析 AUC（COPDGene、ECLIPSE、NETT&#x2F;NAS 多基因风险评分 AUC 0·68 [95% CI 0·65–0·70]）高于 279 变体的 AUC Shrine 及其同事报告的遗传风险评分（AUC 0·58 [0·56–0·61]，p&#x3D;2·7 × 10-41）。</p><p>这种改进的性能可能反映了几个因素：包含的变体数量（大约 1·2-1·7 百万个）、包含未达到全基因组显着性的变体、大 GWAS 样本量（&gt; 400000 个人）和 GWAS 衍生的变体重量。通过应用正则化回归方法来包括未达到全基因组意义的变异并结合两个肺功能参数，我们的研究使用了与 Shrine 及其同事相同的 GWAS，并在预测方面取得了实质性的改进。据我们所知，我们的研究是第一个将全基因组多基因评分应用于呼吸系统疾病的研究。这种方法在其他人群中的应用还有待评估</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Moll, Matthew, et al. “Chronic obstructive pulmonary disease and related phenotypes: polygenic risk scores in population-based and case-control cohorts.” The Lancet Respiratory Medicine 8.7 (2020): 696-708.<br>柳叶刀呼吸医学杂志 IF30·700</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> 生物信息 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
            <tag> PRS </tag>
            
            <tag> Papers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch.nn.Linear</title>
      <link href="/2021/12/13/torch-nn-Linear/"/>
      <url>/2021/12/13/torch-nn-Linear/</url>
      
        <content type="html"><![CDATA[<h2 id="torch-nn-Linear"><a href="#torch-nn-Linear" class="headerlink" title="torch.nn.Linear"></a>torch.nn.Linear</h2><blockquote><p>torch.nn.Linear(in_features, out_features, bias&#x3D;True, device&#x3D;None, dtype&#x3D;None)</p></blockquote><ul><li>in_features – size of each input sample</li><li>out_features – size of each output sample</li><li>bias – If set to False, the layer will not learn an additive bias. Default: True</li></ul><p>$$y&#x3D;xA^T + b$$</p><p>weight:(outdim, inputdim)</p><p>bias:(outdim)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NNLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, out_dim</span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(input_dim, out_dim)</span><br><span class="line">        self.linear1.weight = nn.Parameter(torch.ones(out_dim, input_dim))</span><br><span class="line">        self.linear1.bias = nn.Parameter(torch.zeros(out_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear1(x)</span><br><span class="line"></span><br><span class="line">input_dim = <span class="number">3</span></span><br><span class="line">out_dim = <span class="number">2</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">x = torch.randn((<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">model = NNLinear(input_dim, out_dim)</span><br><span class="line"><span class="keyword">for</span> parameter <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="built_in">print</span>(parameter, parameter.size())</span><br><span class="line">y = model(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.Parameter() 将一个固定不可训练的tensor转换成可以训练的类型parameter，并将这个parameter绑定到这个module里面(net.parameter()中就有这个绑定的parameter，所以在参数优化的时候可以进行优化的)，成为模型的一部分，能随着网络训练一直调优</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sklearn评价指标</title>
      <link href="/2021/12/13/sklearn%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
      <url>/2021/12/13/sklearn%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h2 id="二分类评价指标"><a href="#二分类评价指标" class="headerlink" title="二分类评价指标"></a>二分类评价指标</h2><h3 id="准确度（Accuracy）"><a href="#准确度（Accuracy）" class="headerlink" title="准确度（Accuracy）"></a>准确度（Accuracy）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="comment"># accuracy_score(y_true, y_pred, normalize, sample_weight=None)</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">score = accuracy_score(y_true=y_true, y_pred=y_pred)</span><br><span class="line"><span class="built_in">print</span>(score) <span class="comment"># 0.5</span></span><br><span class="line">score1 = accuracy_score(y_true, y_pred, normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(score1) <span class="comment"># 3</span></span><br><span class="line"></span><br><span class="line">accuracy_score(np.array([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]]), np.ones((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="comment"># 0.5</span></span><br></pre></td></tr></table></figure><h3 id="混淆矩阵（Confusion-Matrix）"><a href="#混淆矩阵（Confusion-Matrix）" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="comment">#sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)</span></span><br><span class="line">y_true = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">y_pred = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">confusion_matrix = confusion_matrix(y_true,y_pred)</span><br><span class="line">confusion_matrix</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]], dtype=int64)</span><br></pre></td></tr></table></figure><p>如图所示，这样看上去就很直观了，中间的格子4表示1预测为1的个数有4个。<br><img src= "/img/loading.gif" data-lazy-src="/2021/12/13/sklearn%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/confusion_matrix.png" alt="confusion_matrix"></p><h3 id="ROC-AUC"><a href="#ROC-AUC" class="headerlink" title="ROC_AUC"></a>ROC_AUC</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="comment"># roc_auc_score(y_true, y_score, average=’macro’, sample_weight=None, max_fpr=None)</span></span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">y_score = np.array([<span class="number">0.85</span>,<span class="number">0.78</span>,<span class="number">0.69</span>,<span class="number">0.54</span>])</span><br><span class="line"><span class="built_in">print</span>(roc_auc_score(y_true,y_score))</span><br><span class="line"><span class="comment"># 0.5</span></span><br></pre></td></tr></table></figure><p>如下图所示，横轴表示false positive rate，纵轴表示true positive rate，我们希望false positive rate的值越小越好，true positive rate的值越大越好，希望曲线往左上角偏。那么如何衡量Roc曲线，我们用Roc的曲线面积来衡量即Auc（Area under curve）值，取值范围：[0, 1]<br><img src= "/img/loading.gif" data-lazy-src="/2021/12/13/sklearn%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/roc_auc.png" alt="roc_auc"></p><p>ROC空间将伪阳性率FPR定义为X轴，将真阳性率TPR定义为Y轴</p><p>TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率。<br>$$TPR &#x3D; \frac{TP}{TP+FN}$$</p><p>FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率。<br>$$FPR &#x3D; \frac{FP}{FP+TN}$$</p><h3 id="Precision和Recall"><a href="#Precision和Recall" class="headerlink" title="Precision和Recall"></a>Precision和Recall</h3><ul><li>True positive(TP): 真正例，将正类正确预测为正类数；</li><li>False positive(FP): 假正例，将负类错误预测为正类数；</li><li>False negative(FN):假负例，将正类错误预测为负类数；</li><li>True negative(TN): 真负例，将负类正确预测为负类数。</li></ul><p>Precision(精确率)：分类正确的正样本个数占分类器判定为正样本的样本个数的比例</p><p>分类正确的正样本个数：即真正例(TP)。</p><p>分类器判定为正样本的个数：包括真正例(TP)和假正例(FP)。<br>$$Precesion &#x3D; \frac{TP}{TP+FP}$$</p><p>Recall(召回率)：分类正确的正样本个数占真正的正样本个数的比例。</p><p>分类正确的正样本个数：即真正例(TP)。</p><p>真正的正样本个数：包括真正例(TP)和假负例(FN)。</p><p>$$Recall &#x3D; \frac{TP}{TP+FN}$$</p><p>$$F1score &#x3D; \frac{2TP}{2TP+FN+FP}&#x3D;\frac{2 * Precision * Recall}{Precision + Recall}$$</p><p>recall 体现了分类模型 H H H对正样本的识别能力，recall 越高，说明模型对正样本的识别能力越强，precision 体现了模型对负样本的区分能力，precision越高，说明模型对负样本的区分能力越强。F1-score 是两者的综合。F1-score 越高，说明分类模型越稳健。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">score = accuracy_score(y_true=y_true, y_pred=y_pred)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line">score1 = accuracy_score(y_true, y_pred, normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(score1)</span><br><span class="line"></span><br><span class="line">a = classification_report(y_true, y_pred, output_dict=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="string">&#x27;1&#x27;</span>])</span><br><span class="line"><span class="comment"># &#123;&#x27;precision&#x27;: 0.6666666666666666, &#x27;recall&#x27;: 0.5, &#x27;f1-score&#x27;: 0.5714285714285715, &#x27;support&#x27;: 4&#125;</span></span><br><span class="line"></span><br><span class="line">b = classification_report(y_true, y_pred) <span class="comment"># output_dict默认false</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           <span class="number">0</span>       <span class="number">0.33</span>      <span class="number">0.50</span>      <span class="number">0.40</span>         <span class="number">2</span></span><br><span class="line">           <span class="number">1</span>       <span class="number">0.67</span>      <span class="number">0.50</span>      <span class="number">0.57</span>         <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.50</span>         <span class="number">6</span></span><br><span class="line">   macro avg       <span class="number">0.50</span>      <span class="number">0.50</span>      <span class="number">0.49</span>         <span class="number">6</span></span><br><span class="line">weighted avg       <span class="number">0.56</span>      <span class="number">0.50</span>      <span class="number">0.51</span>         <span class="number">6</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> sklearn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sklearn常用函数</title>
      <link href="/2021/12/13/sklearn%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
      <url>/2021/12/13/sklearn%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> imblearn</span><br></pre></td></tr></table></figure><p>针对数据不平衡问题，有两种策略，过采样(over-sampling)和下采样(under-sampling)。</p><h3 id="under-sampling"><a href="#under-sampling" class="headerlink" title="under_sampling"></a>under_sampling</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">rus = RandomUnderSampler(random_state = <span class="number">1234</span>)</span><br><span class="line">x_rus, y_rus = rus.fit_resample(x, y) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x_rus:&#x27;</span>, Counter(x_rus), <span class="string">&#x27;y_rus&#x27;</span>, Counter(y_rus))</span><br></pre></td></tr></table></figure><p>under_sample<a href="https://imbalanced-learn.org/stable/references/under_sampling.html">合集</a></p><h3 id="over-sampling"><a href="#over-sampling" class="headerlink" title="over_sampling"></a>over_sampling</h3><p>over_sampling<a href="https://imbalanced-learn.org/stable/references/over_sampling.html">方法api</a></p>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> sklearn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两数相加</title>
      <link href="/2021/12/09/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"/>
      <url>/2021/12/09/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。</p><p>请你将两个数相加，并以相同形式返回一个表示和的链表。</p><p>你可以假设除了数字 0 之外，这两个数都不会以 0 开头</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/12/09/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/fig.jpg" alt="示例"></p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><p>注意最后一位进位。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">self, l1: ListNode, l2: ListNode</span>) -&gt; ListNode:</span><br><span class="line">         <span class="comment"># 当前指针，结果链表</span></span><br><span class="line">        result = curr = ListNode()</span><br><span class="line">        <span class="comment"># 进位项</span></span><br><span class="line">        remainder = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 非空满足循环</span></span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">or</span> l2:</span><br><span class="line">            x = l1.val <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            y = l2.val <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            <span class="built_in">sum</span> = x + y + remainder</span><br><span class="line"></span><br><span class="line">            curr.<span class="built_in">next</span> = ListNode(<span class="built_in">sum</span> % <span class="number">10</span>)</span><br><span class="line">            remainder = <span class="built_in">sum</span> // <span class="number">10</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> l1:</span><br><span class="line">                l1 = l1.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">if</span> l2:</span><br><span class="line">                l2 = l2.<span class="built_in">next</span></span><br><span class="line">            curr = curr.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只在最后创建新节点  </span></span><br><span class="line">        <span class="keyword">if</span> remainder:</span><br><span class="line">            curr.<span class="built_in">next</span> = ListNode(remainder)  </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result.<span class="built_in">next</span></span><br></pre></td></tr></table></figure><p>时间复杂度：O(max(m,n))，其中m 和 n 分别为两个链表的长度。我们要遍历两个链表的全部位置，而处理每个位置只需要 O(1) 的时间。</p><p>空间复杂度：O(1)。注意返回值不计入空间复杂度</p><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol><li>链接：<a href="https://leetcode-cn.com/problems/add-two-numbers">https://leetcode-cn.com/problems/add-two-numbers</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两数之和</title>
      <link href="/2021/12/08/%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/"/>
      <url>/2021/12/08/%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。</p><p>你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</p><p>你可以按任意顺序返回答案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：nums = [<span class="number">2</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">15</span>], target = <span class="number">9</span></span><br><span class="line">输出：[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">解释：因为 nums[<span class="number">0</span>] + nums[<span class="number">1</span>] == <span class="number">9</span> ，返回 [<span class="number">0</span>, <span class="number">1</span>] 。</span><br></pre></td></tr></table></figure><h2 id="python解答"><a href="#python解答" class="headerlink" title="python解答"></a>python解答</h2><h3 id="暴力解法"><a href="#暴力解法" class="headerlink" title="暴力解法"></a>暴力解法</h3><p>错误解1:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums, target</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i, value_1 <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            <span class="keyword">for</span> j, value_2 <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">                <span class="keyword">if</span> value_1 + value_2 == target:</span><br><span class="line">                    <span class="keyword">return</span> [i,j]</span><br></pre></td></tr></table></figure><p>双重循环暴力解，但是均从第一个开始计算，题目要求同一个元素在答案里不能重复出现。比如target&#x3D;6，nums&#x3D;[3,1,3],则return:[0,0]。</p><p>更正解:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -- enumerate遍历 --</span></span><br><span class="line"><span class="keyword">for</span> i, value_1 <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">    <span class="keyword">for</span> j, value_2 <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">        <span class="keyword">if</span> i == j : <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> value_1 + value_2 == target:</span><br><span class="line">            <span class="keyword">return</span> [i,j]</span><br><span class="line"><span class="comment"># 直接跳过i==j即可</span></span><br><span class="line"><span class="comment"># 运行时长4060 ms，内存13.5 MB</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -- 下标遍历 --</span></span><br><span class="line">n = <span class="built_in">len</span>(nums)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (n):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> nums[i] + nums[j] == target:</span><br><span class="line">                <span class="keyword">return</span> i, j</span><br><span class="line"><span class="comment"># 运行时长1908 ms，内存13.9 MB</span></span><br></pre></td></tr></table></figure><p>时间复杂度：$O(N^2)$,其中 N 是数组中的元素数量。最坏情况下数组中任意两个数都要被匹配一次。</p><p>空间复杂度：$O(1)$</p><h3 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h3><p>注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。</p><p>使用哈希表，可以将寻找 target - x 的时间复杂度从<br>$O(N)$降低到$O(1)$。散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。python中的dict类型就是哈希表的原理，存储方式是key-value，通过键来快速的访问value，字典在访问操作上时间复杂度为O(1)。</p><p>这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -- 官方 --</span></span><br><span class="line"> hashtable = <span class="built_in">dict</span>() <span class="comment"># hashtable = &#123;&#125;</span></span><br><span class="line">    <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">        <span class="keyword">if</span> target - num <span class="keyword">in</span> hashtable:</span><br><span class="line">            <span class="keyword">return</span> [hashtable[target - num], i]</span><br><span class="line">        hashtable[num] = i <span class="comment">#这句不能放在if语句之前，解决list中有重复值或target-num=num的情况</span></span><br><span class="line">    <span class="keyword">return</span> []</span><br><span class="line"><span class="comment"># 运行时长12ms, 内存13.7MB</span></span><br></pre></td></tr></table></figure><p>时间复杂度：$O(N)$，其中N是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x</p><p>空间复杂度：$O(N)$，其中 N 是数组中的元素数量。主要为哈希表的开销</p><h3 id="python-list解法"><a href="#python-list解法" class="headerlink" title="python list解法"></a>python list解法</h3><p>解题关键主要是想找到 num2 &#x3D; target - num1，是否也在 list 中，那么就需要运用以下两个方法：</p><ul><li>if num2 in nums 返回 True 说明有戏</li><li>nums.index(num2) 查找 num2 的索引</li></ul><p>错误解法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="built_in">len</span>(nums)</span><br><span class="line">    j = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i, num2 <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">        <span class="keyword">if</span> target - num2 <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> nums.index(num2) != nums.index(target-num2) :</span><br><span class="line">                <span class="keyword">return</span> [i,nums.index(target-num2)]</span><br><span class="line">    <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure><p>错误样例：[3,3] return [ ] ; 正确解：[0,1]</p><p>更正解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -- 判断自身 --</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">nums, target</span>):</span><br><span class="line">    lens = <span class="built_in">len</span>(nums)</span><br><span class="line">    j=-<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(lens):</span><br><span class="line">        <span class="keyword">if</span> (target - nums[i]) <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> (nums.count(target - nums[i]) == <span class="number">1</span>)&amp;(target - nums[i] == nums[i]):<span class="comment">#如果num2=num1,且nums中只出现了一次，说明找到是num1本身。</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                j = nums.index(target - nums[i],i+<span class="number">1</span>) <span class="comment">#index(x,i+1)是从num1后的序列后找num2                </span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> j&gt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [i,j]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line"><span class="comment"># -- 优化 -- </span></span><br><span class="line">lens = <span class="built_in">len</span>(nums)</span><br><span class="line">j=-<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,lens):</span><br><span class="line">    temp = nums[:i] <span class="comment"># 不包含nums[i]</span></span><br><span class="line">    <span class="keyword">if</span> (target - nums[i]) <span class="keyword">in</span> temp:</span><br><span class="line">        j = temp.index(target - nums[i])</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">if</span> j&gt;=<span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> [j,i]</span><br><span class="line"><span class="comment"># 执行时间312ms, 内存消耗13.9MB</span></span><br></pre></td></tr></table></figure><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol><li>题目链接：<a href="https://leetcode-cn.com/problems/two-sum">https://leetcode-cn.com/problems/two-sum</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>einops 优雅的tensor操作</title>
      <link href="/2021/12/08/einops-%E4%BC%98%E9%9B%85%E7%9A%84tensor%E6%93%8D%E4%BD%9C/"/>
      <url>/2021/12/08/einops-%E4%BC%98%E9%9B%85%E7%9A%84tensor%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from einops import rearrange, repeat, reduce</span><br><span class="line">from einops.layers.torch import Rearrange</span><br></pre></td></tr></table></figure><p>einops主要是rearrange, reduce, repeat这3个方法，下面介绍如何通过这3个方法如何来起到 stacking, reshape, transposition, squeeze&#x2F;unsqueeze, repeat, tile, concatenate, view 以及各种reduction操作的效果)。</p><h2 id="rearrange-重新安排维度"><a href="#rearrange-重新安排维度" class="headerlink" title="rearrange 重新安排维度"></a>rearrange 重新安排维度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rearrange(ims[<span class="number">0</span>], <span class="string">&#x27;h w c -&gt; w h c&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># or compose a new dimension of batch and width</span></span><br><span class="line">rearrange(ims, <span class="string">&#x27;b h w c -&gt; h (b w) c&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length of newly composed axis is a product of components</span></span><br><span class="line"><span class="comment"># [6, 96, 96, 3] -&gt; [96, (6 * 96), 3]</span></span><br><span class="line">rearrange(ims, <span class="string">&#x27;b h w c -&gt; h (b w) c&#x27;</span>).shape <span class="comment"># (96, 576, 3)</span></span><br></pre></td></tr></table></figure><p>Decomposition of axis：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we can compose more than two axes. </span></span><br><span class="line"><span class="comment"># let&#x27;s flatten 4d array into 1d, resulting array has as many elements as the original</span></span><br><span class="line">rearrange(ims, <span class="string">&#x27;b h w c -&gt; (b h w c)&#x27;</span>).shape <span class="comment">#(2, 3, 96, 96, 3)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># finally, combine composition and decomposition:</span></span><br><span class="line">rearrange(ims, <span class="string">&#x27;(b1 b2) h w c -&gt; (b1 h) (b2 w) c &#x27;</span>, b1=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># slightly different composition: b1 is merged with width, b2 with height</span></span><br><span class="line"><span class="comment"># ... so letters are ordered by w then by h</span></span><br><span class="line">rearrange(ims, <span class="string">&#x27;(b1 b2) h w c -&gt; (b2 h) (b1 w) c &#x27;</span>, b1=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>height 变为2倍，width变为一半：先把Width拆成width&#x3D;w*2,然后分解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rearrange(ims, <span class="string">&#x27;b h (w w2) c -&gt; (h w2) (b w) c&#x27;</span>, w2=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><p>einops.reduce provides combination of reordering and reduction using reader-friendly notation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.random.randn(<span class="number">100</span>, <span class="number">32</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># perform max-reduction on the first axis</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = reduce(x, <span class="string">&#x27;t b c -&gt; b c&#x27;</span>, <span class="string">&#x27;max&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># same as previous, but with clearer axes meaning</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = reduce(x, <span class="string">&#x27;time batch channel -&gt; batch channel&#x27;</span>, <span class="string">&#x27;max&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.random.randn(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2d max-pooling with kernel size = 2 * 2 for image processing</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y1 = reduce(x, <span class="string">&#x27;b c (h1 h2) (w1 w2) -&gt; b c h1 w1&#x27;</span>, <span class="string">&#x27;max&#x27;</span>, h2=<span class="number">2</span>, w2=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># if one wants to go back to the original height and width, depth-to-space trick can be applied</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y2 = rearrange(y1, <span class="string">&#x27;b (c h2 w2) h1 w1 -&gt; b c (h1 h2) (w1 w2)&#x27;</span>, h2=<span class="number">2</span>, w2=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">assert</span> parse_shape(x, <span class="string">&#x27;b _ h w&#x27;</span>) == parse_shape(y2, <span class="string">&#x27;b _ h w&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adaptive 2d max-pooling to 3 * 4 grid</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(x, <span class="string">&#x27;b c (h1 h2) (w1 w2) -&gt; b c h1 w1&#x27;</span>, <span class="string">&#x27;max&#x27;</span>, h1=<span class="number">3</span>, w1=<span class="number">4</span>).shape</span><br><span class="line">(<span class="number">10</span>, <span class="number">20</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Global average pooling</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(x, <span class="string">&#x27;b c h w -&gt; b c&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>).shape</span><br><span class="line">(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Subtracting mean over batch for each channel</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x - reduce(x, <span class="string">&#x27;b c h w -&gt; () c () ()&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Subtracting per-image mean for each channel</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x - reduce(x, <span class="string">&#x27;b c h w -&gt; b c () ()&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="repeat"><a href="#repeat" class="headerlink" title="repeat"></a>repeat</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a grayscale image (of shape height x width)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>image = np.random.randn(<span class="number">30</span>, <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># change it to RGB format by repeating in each channel</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>repeat(image, <span class="string">&#x27;h w -&gt; h w c&#x27;</span>, c=<span class="number">3</span>).shape</span><br><span class="line">(<span class="number">30</span>, <span class="number">40</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># repeat image 2 times along height (vertical axis)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>repeat(image, <span class="string">&#x27;h w -&gt; (repeat h) w&#x27;</span>, repeat=<span class="number">2</span>).shape</span><br><span class="line">(<span class="number">60</span>, <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># repeat image 2 time along height and 3 times along width</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>repeat(image, <span class="string">&#x27;h w -&gt; h (repeat w)&#x27;</span>, repeat=<span class="number">3</span>).shape</span><br><span class="line">(<span class="number">30</span>, <span class="number">120</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert each pixel to a small square 2x2. Upsample image by 2x</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>repeat(image, <span class="string">&#x27;h w -&gt; (h h2) (w w2)&#x27;</span>, h2=<span class="number">2</span>, w2=<span class="number">2</span>).shape</span><br><span class="line">(<span class="number">60</span>, <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pixelate image first by downsampling by 2x, then upsampling</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>downsampled = reduce(image, <span class="string">&#x27;(h h2) (w w2) -&gt; h w&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>, h2=<span class="number">2</span>, w2=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>repeat(downsampled, <span class="string">&#x27;h w -&gt; (h h2) (w w2)&#x27;</span>, h2=<span class="number">2</span>, w2=<span class="number">2</span>).shape</span><br><span class="line">(<span class="number">30</span>, <span class="number">40</span>)</span><br></pre></td></tr></table></figure><h2 id="reference"><a href="#reference" class="headerlink" title="reference:"></a>reference:</h2><ol><li><a href="https://einops.rocks/api/rearrange/">https://einops.rocks/api/rearrange/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
            <tag> einops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python高阶函数map reduce lambda</title>
      <link href="/2021/12/08/python%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0map-reduce-lambda/"/>
      <url>/2021/12/08/python%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0map-reduce-lambda/</url>
      
        <content type="html"><![CDATA[<h3 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]))</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>]</span><br></pre></td></tr></table></figure><p>lambda x: x * x实际上就是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * x</span><br></pre></td></tr></table></figure><p>关键字lambda表示匿名函数，冒号前面的x表示函数参数。</p><p>匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = <span class="keyword">lambda</span> x: x * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f</span><br><span class="line">&lt;function &lt;<span class="keyword">lambda</span>&gt; at <span class="number">0x101c6ef28</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f(<span class="number">5</span>)</span><br><span class="line"><span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span>: x * x + y * y</span><br></pre></td></tr></table></figure><h3 id="map"><a href="#map" class="headerlink" title="map()"></a>map()</h3><p>map(f, Iterable)</p><p>map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。</p><p>举例说明，比如我们有一个函数f(x)&#x3D;x2，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x * x</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = <span class="built_in">map</span>(f, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(r)</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>]</span><br></pre></td></tr></table></figure><p>map()传入的第一个参数是f，即函数对象本身。由于结果r是一个Iterator，Iterator是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。</p><p>你可能会想，不需要map()函数，写一个循环，也可以计算出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]:</span><br><span class="line">    L.append(f(n))</span><br><span class="line"><span class="built_in">print</span>(L)</span><br></pre></td></tr></table></figure><p>再比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]))</span><br><span class="line">[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;8&#x27;</span>, <span class="string">&#x27;9&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce()"></a>reduce()</h3><p>reduce(f, [x1, x2, x3, x4]) &#x3D; f(f(f(x1, x2), x3), x4)</p><h3 id="filter"><a href="#filter" class="headerlink" title="filter()"></a>filter()</h3><p>filter(f, Iterable)</p><p>f：return true or false</p><p>和map()类似，filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">is_odd</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">return</span> n % <span class="number">2</span> == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">filter</span>(is_odd, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>]))</span><br><span class="line"><span class="comment"># 结果: [1, 5, 9, 15]</span></span><br></pre></td></tr></table></figure><p>注意到filter()函数返回的是一个Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。</p><h3 id="sorted"><a href="#sorted" class="headerlink" title="sorted()"></a>sorted()</h3><p>sorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">sorted</span>([<span class="number">36</span>, <span class="number">5</span>, -<span class="number">12</span>, <span class="number">9</span>, -<span class="number">21</span>], key=<span class="built_in">abs</span>)</span><br><span class="line">[<span class="number">5</span>, <span class="number">9</span>, -<span class="number">12</span>, -<span class="number">21</span>, <span class="number">36</span>]</span><br></pre></td></tr></table></figure><h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ol><li><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017404530360000">https://www.liaoxuefeng.com/wiki/1016959663602400/1017404530360000</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Vit</title>
      <link href="/2021/12/08/Vit/"/>
      <url>/2021/12/08/Vit/</url>
      
        <content type="html"><![CDATA[<h2 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h2><p>从2020年，transformer开始在CV领域大放异彩：图像分类（ViT, DeiT），目标检测（DETR，Deformable DETR），语义分割（SETR，MedT），图像生成（GANsformer）等。而从深度学习暴发以来，CNN一直是CV领域的主流模型，而且取得了很好的效果，相比之下transformer却独霸NLP领域，transformer在CV领域的探索正是研究界想把transformer在NLP领域的成功借鉴到CV领域。对于图像问题，卷积具有天然的先天优势（inductive bias）：平移等价性（translation equivariance）和局部性（locality）。而transformer虽然不并具备这些优势，但是transformer的核心self-attention的优势不像卷积那样有固定且有限的感受野，self-attention操作可以获得long-range信息（相比之下CNN要通过不断堆积Conv layers来获取更大的感受野），但训练的难度就比CNN要稍大一些。</p><p>ViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。ViT模型原理如下图所示，其实ViT模型只是用了transformer的Encoder来提取特征（原始的transformer还有decoder部分，用于实现sequence to sequence，比如机器翻译）。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/12/08/Vit/fig1.png" alt="Model overview"></p><h3 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a>Patch Embedding</h3><p>ViT的首要任务是将图转换成词的结构，这里采取的方法是如上图左下角所示，将图片分割成小块，每个小块就相当于句子里的一个词。这里把每个小块称作Patch，而Patch Embedding就是把每个Patch再经过一个全连接网络压缩成一定维度的向量。</p><p>输入的$2-D$图像记为$x \in R^{H \times W \times C}$, 要将图像划分为$P \times P$的patchs（上图中的小块为一个patch）,通过reshape得到长度为N的a sequence of patchs:$x_p \in R^{N \times (P^2 \cdot C)}$，长度为$N &#x3D; HW&#x2F;P^2$（即图像分割成N个patch）。将patch拉平为$1-D$的sequence，即：<br>$$2-D \ picture \ (B \times C \times H \times W) \Rightarrow 1-D \ a \ sequence \ of \ patch \ (B \times N \times D) \ 其中N&#x3D;HW&#x2F;P^2,D&#x3D;P^2C $$</p><p>等价于对图像做了卷积核为PxP，stride为P的卷积操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        num_patches = (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]) * (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>])</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.num_patches = num_patches</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h3><p>ViT中默认采用学习（训练的）的1-D positional embedding，在输入transformer的encoder之前直接将patch embeddings和positional embedding相加:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 这里多1是为了后面要说的class token，embed_dim即patch embed_dim</span><br><span class="line">self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) </span><br><span class="line"></span><br><span class="line"># patch emded + pos_embed</span><br><span class="line">x = x + self.pos_embed</span><br></pre></td></tr></table></figure><p>这里额外要注意的一点，如果改变图像的输入大小，ViT不会改变patchs的大小，那么patchs的数量[公式]会发生变化，那么之前学习的pos_embed就维度对不上了，ViT采用的方案是通过插值来解决这个问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resize_pos_embed</span>(<span class="params">posemb, posemb_new</span>):</span><br><span class="line">    <span class="comment"># Rescale the grid of position embeddings when loading from state_dict. Adapted from</span></span><br><span class="line">    <span class="comment"># https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224</span></span><br><span class="line">    _logger.info(<span class="string">&#x27;Resized position embedding: %s to %s&#x27;</span>, posemb.shape, posemb_new.shape)</span><br><span class="line">    ntok_new = posemb_new.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 除去class token的pos_embed</span></span><br><span class="line">    posemb_tok, posemb_grid = posemb[:, :<span class="number">1</span>], posemb[<span class="number">0</span>, <span class="number">1</span>:]</span><br><span class="line">    ntok_new -= <span class="number">1</span></span><br><span class="line">    gs_old = <span class="built_in">int</span>(math.sqrt(<span class="built_in">len</span>(posemb_grid)))</span><br><span class="line">    gs_new = <span class="built_in">int</span>(math.sqrt(ntok_new))</span><br><span class="line">    _logger.info(<span class="string">&#x27;Position embedding grid-size from %s to %s&#x27;</span>, gs_old, gs_new)</span><br><span class="line">    <span class="comment"># 把pos_embed变换到2-D维度再进行插值</span></span><br><span class="line">    posemb_grid = posemb_grid.reshape(<span class="number">1</span>, gs_old, gs_old, -<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    posemb_grid = F.interpolate(posemb_grid, size=(gs_new, gs_new), mode=<span class="string">&#x27;bilinear&#x27;</span>)</span><br><span class="line">    posemb_grid = posemb_grid.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(<span class="number">1</span>, gs_new * gs_new, -<span class="number">1</span>)</span><br><span class="line">    posemb = torch.cat([posemb_tok, posemb_grid], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> posemb</span><br></pre></td></tr></table></figure><p>但是这种情形一般会造成性能少许损失，可以通过finetune模型来解决。另外最新的论文<a href="https://arxiv.org/abs/2102.10882">CPVT</a>通过implicit Conditional Position encoding来解决这个问题（插入Conv来隐式编码位置信息，zero padding让Conv学习到绝对位置信息）。</p><h3 id="Class-Token"><a href="#Class-Token" class="headerlink" title="Class Token"></a>Class Token</h3><p>除了patch tokens，ViT借鉴BERT还增加了一个特殊的class token。后面会说，transformer的encoder输入是a sequence patch embeddings，输出也是同样长度的a sequence patch features，但图像分类最后需要获取image feature，简单的策略是采用pooling，比如求patch features的平均来获取image feature，但是ViT并没有采用类似的pooling策略，而是直接增加一个特殊的class token，其最后输出的特征加一个linear classifier就可以实现对图像的分类（ViT的pre-training时是接一个MLP head），所以输入ViT的sequence长度是。class token对应的embedding在训练时随机初始化，然后通过训练得到，具体实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机初始化</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier head</span></span><br><span class="line">self.head = nn.Linear(self.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体forward过程</span></span><br><span class="line">B = x.shape[<span class="number">0</span>]</span><br><span class="line">x = self.patch_embed(x)</span><br><span class="line">cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">x = x + self.pos_embed</span><br></pre></td></tr></table></figure><h3 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h3><p>Attention机制称为Scaled dot product attention。Q,K,V都是从一个包含N个向量的sequence(X \in R^{N \times D})通过线性变换得到的(self-attention)，这时会得到N个(key, value)对。</p><p><a href="https://github.com/rwightman/pytorch-image-models">timm</a>中attention是在self-attention基础上改进的multi-head attention，也就是在产生q，k，v的时候，对q，k，v进行了切分，分别分成了num_heads份，对每一份分别进行self-attention的操作，最后再拼接起来，这样在一定程度上进行了参数隔离。</p><p>原始MSA是对QKV分别做h个SA，再拼接，没有分QKV。</p><p>$$Attention(Q,K,V)&#x3D;Softmax(\frac{QK^T}{\sqrt{d_K}}V) $$</p><p>multi-head self-attention(MSA):</p><p>$$MSA(X)&#x3D;Concat(head_1,….,head_h)W^O,\ head_i &#x3D; SA(XW_i^Q,XW_i^K,XW_i^V)$$</p><h3 id="Vit-Blocks"><a href="#Vit-Blocks" class="headerlink" title="Vit Blocks"></a>Vit Blocks</h3><p>Transformer Encoder在Vit中换了个名，叫block,(layer normalization -&gt; multi-head attention -&gt; drop path -&gt; layer normalization -&gt; mlp -&gt; drop path)</p><p>Layer normalization对应的一个概念是我们熟悉的Batch Normalization，这两个根本的不同在于，Layer normalization是对每个样本的所有特征进行归一化，而Batch Normalization是对每个通道的所有样本进行归一化。</p><p>Dropout是最早用于解决网络过拟合的方法，是所有drop类方法的始祖。方法示意图如下<br><img src= "/img/loading.gif" data-lazy-src="/2021/12/08/Vit/fig3.jpg" alt="dropout"><br>在向前传播的时候，让神经元以一定概率停止工作。这样可以使模型泛化能力变强，因为神经元会以一定概率失效，这样的机制会使结果不会过分依赖于个别神经元。训练阶段，以keep_prob概率使神经元失效，而推理的时候，会保留所有神经元的有效性，因此，训练时候加了dropout的神经元推理出来的结果要乘以keep_prob.</p><p>接下来以dropout的思路来理解drop path，drop path没找到示意图，那直接看timm上的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment"># drop_prob是进行droppath的概率</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># work with diff dim tensors, not just 2D ConvNets</span></span><br><span class="line">    <span class="comment"># 在ViT中，shape是(B,1,1),B是batch size</span></span><br><span class="line">    shape = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 按shape,产生0-1之间的随机向量,并加上keep_prob  </span></span><br><span class="line">    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    <span class="comment"># 向下取整，二值化，这样random_tensor里1出现的概率的期望就是keep_prob</span></span><br><span class="line">    random_tensor.floor_()  <span class="comment"># binarize</span></span><br><span class="line">    <span class="comment"># 将一定图层变为0</span></span><br><span class="line">    output = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>由代码可以看出，drop path是在batch那个维度，随机将一些图层直接变成0，以加快运算速度。</p><p>对于ViT模型来说，就类似CNN那样，不断堆积transformer encoder blocks，最后提取class token对应的特征用于图像分类，论文中也给出了模型的公式表达，其中（1）就是提取图像的patch embeddings，然后和class token对应的embedding拼接在一起并加上positional embedding；（2）是MSA，而（3）是MLP，（2）和（3）共同组成了一个transformer encoder block，共有层；（4）是对class token对应的输出做layer norm，然后就可以用来图像分类。<br><img src= "/img/loading.gif" data-lazy-src="/2021/12/08/Vit/fig2.png" alt="模型公式"></p><p>ViT模型的超参数主要包括以下，这些超参数直接影响模型参数以及计算量：</p><ul><li>Layers：block的数量；</li><li>Hidden size D：隐含层特征，D在各个block是一直不变的；</li><li>MLP size：一般设置为4D大小；</li><li>Heads：MSA中的heads数量；</li><li>Patch size：模型输入的patch size，ViT中共有两个设置：14x14和16x16，这个只影响计算量；</li></ul><h2 id="模型代码pytorch版"><a href="#模型代码pytorch版" class="headerlink" title="模型代码pytorch版"></a>模型代码pytorch版</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li><p>Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</p></li><li><p>知乎：<a href="https://zhuanlan.zhihu.com/p/356155277">https://zhuanlan.zhihu.com/p/356155277</a></p></li><li><p>知乎：<a href="https://zhuanlan.zhihu.com/p/427388113">https://zhuanlan.zhihu.com/p/427388113</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Transformer </tag>
            
            <tag> Vit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>孟德尔随机化</title>
      <link href="/2021/12/07/%E5%AD%9F%E5%BE%B7%E5%B0%94%E9%9A%8F%E6%9C%BA%E5%8C%96/"/>
      <url>/2021/12/07/%E5%AD%9F%E5%BE%B7%E5%B0%94%E9%9A%8F%E6%9C%BA%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="孟德尔随机化（Mendelian-randomization"><a href="#孟德尔随机化（Mendelian-randomization" class="headerlink" title="孟德尔随机化（Mendelian randomization)"></a>孟德尔随机化（Mendelian randomization)</h2><h3 id="背景与目的"><a href="#背景与目的" class="headerlink" title="背景与目的"></a>背景与目的</h3><p>目的是明确因果关系，在关联分析中的一个常见问题是，很难确定一个变量是否是真正的因果变量，而非有其他未观测的因素同时影响这个变量与结果，造成这个变量与结果相关联。在循证医学中，或是制定干预策略时，明确因果性是十分必要的。</p><p>这个问题实际上与内生性 endogeneity 相关，包括： 反向因果关系 reverse causation, 忽略的混淆变量造成的偏倚 omitted variable bias due to confounding, 测量误差measurement error, 以及双向因果关系bidirectional causality等等问题。（这里的内生性在统计学上是指在回归分析中，解释变量（x）与误差项相关。）</p><p>一般来说，明确因果关系的金标准时随机对照试验 RCT randomized control trial (RCT)， 即对受试者随机分为对照组和实验组，以研究某个因素的影响。但现实中，要完成随机对照试验的难度非常高，需要大量的人力物力，有时因为伦理问题，对某个因素的研究几乎是不可能的。这时我们就要借助其他方法，而孟德尔随机化就是其中之一。</p><p>孟德尔随机化的核心其实是利用了孟德尔第二定律，也就是自由组合规律(law of independent assortment），当具有两对（或更多对）相对性状的亲本进行杂交，在子一代产生配子时，在等位基因分离的同时，非同源染色体上的基因表现为自由组合，这一过程类似于随机对照试验中的随机分组，所以我个人理解的孟德尔随机化就是 基于孟德尔第二定律的随机对照试验。</p><h3 id="孟德尔随机化的统计学方法–工具变量"><a href="#孟德尔随机化的统计学方法–工具变量" class="headerlink" title="孟德尔随机化的统计学方法–工具变量"></a>孟德尔随机化的统计学方法–工具变量</h3><p>孟德尔随机化在统计学上的本质实际是利用工具变量（Instrumental variables）来研究因果性，这一方法常用在经济学研究中。</p><p>工具变量简单来说就是，一个与X相关，但与被忽略的混淆因素以及Y不相关的变量。在经济学研究中工具变量可以是政策改革，自然灾害等等，而在遗传学中，这个变量就是基因。</p><p>如果一个基因变异Z 是某个暴露因素X的因果变量，并且对结果Y没有直接因果关系，那么这个基因变异Z与结果Y的关联，只能通过X对Y的因果关系而被观察到（X-&gt;Y）。</p><h4 id="两阶段最小二乘法"><a href="#两阶段最小二乘法" class="headerlink" title="两阶段最小二乘法"></a>两阶段最小二乘法</h4><p>通常我们可以用两阶段最小二乘法（2SLS，2 stage least squared method）来估计X对Y的效应：</p><p>考虑一种最简单的单样本的情况，有一个基因变异Z，与Z相关的因素X，以及与Z不相关的结果Y，我们想探究X与Y之间的因果关系。</p><p>第一阶段，X对工具变量进行回归，<br>$$X &#x3D; \mu _1 + \gamma IV + \varepsilon _1$$<br>第二阶段，Y对第一阶段X的预测值进行回归，<br>$$Y &#x3D; \mu _2 + \beta _{2SLS} \hat{X} + \varepsilon <em>2$$<br>合并后可以化为Y直接对工具变量进行回归。<br>$$Y &#x3D; \mu <em>3 + \rho IV + \varepsilon <em>3$$<br>我们所关心的系数$β</em>{2SLS}$实际上也等同于两段协方差的比值。<br>$$\frac{Cov</em>{y,z}}{Cov</em>{x,z}}$$</p><h4 id="两样本MR"><a href="#两样本MR" class="headerlink" title="两样本MR"></a>两样本MR</h4><p>另一种常见的情况则是两样本MR，如果我们有一个与X相关联的工具变量，我们只有在X对Y有因果关系的情况下，才能观测到这个工具变量与Y的关联。</p><p>这意味着βiv,y &#x3D; βiv,x 乘以 βx,y。也就是说，我们可以不用通过X与Y的回归来估计β，而是可以简单地通过 βx,y &#x3D; βiv,y &#x2F; βiv,x 来计算 X对Y的效应量。这就意味着与两阶段最小二乘法相对，我们可以利用两个独立的GWAS 的概括性统计量来计算这个比值。这种方法通常叫做两样本MR.</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/12/07/%E5%AD%9F%E5%BE%B7%E5%B0%94%E9%9A%8F%E6%9C%BA%E5%8C%96/fig1.png" alt="Two sample Mendelian randomisation"></p><h3 id="核心假设"><a href="#核心假设" class="headerlink" title="核心假设"></a>核心假设</h3><p>当然，既然是统计模型那就要满足模型的基本假设，通常情况下MR建立在几点基本假设之上，主要假设：</p><ul><li>遗传变异必须与暴露因素X强相关。（关联性假设），例如：弱工具变量的使用会导致估计出现偏倚。</li><li>遗传变异不能与结果直接相关。(排他性限制)，例如：可能影响因素包括多效性等。</li><li>遗传变异不能与任何可能的混淆因素相关 (独立性假设)，例如：人群分层其他假设：</li><li>不存在选型交配 No genetic assortative mating，例如：人们经常会与自己教育和经济水平相似的人结婚。</li><li>对所有个体，IV对于X的影响方向是相同的。例如：潜在的上位效应与GxE基因与环境的相互作用都可能会影响此假设。</li></ul><p>总结来看，孟德尔随机化以基因型作为工具变量的优势是:</p><ul><li>遗传相关中，因果关系的方向是确定的，遗传多样性导致了不同的表型，反之则不成立</li><li>一般情况下我们所测量的环境暴露因素都或多或少与行为，社会，心理等因素相关，造成偏倚。但遗传变异则不受这些混淆因素影响。</li><li>相对来说，遗传变异与其效应的测量误差较小。</li><li>并不一定要找到因果SNP，一个与因果SNP处于LD的SNP即可满足假设条件。</li><li>目前GWAS的数据相对容易获取。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 生信123 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
            <tag> 孟德尔随机化 </tag>
            
            <tag> GWAS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python_numpy</title>
      <link href="/2021/12/06/python-numpy/"/>
      <url>/2021/12/06/python-numpy/</url>
      
        <content type="html"><![CDATA[<h3 id="行列转换"><a href="#行列转换" class="headerlink" title="行列转换"></a>行列转换</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 非负整数表示的标签列表</span><br><span class="line">labels = [0,1,0,2]</span><br><span class="line"># 行向量转列向量</span><br><span class="line">labels = np.array(labels).reshape(len(labels), -1)</span><br></pre></td></tr></table></figure><h3 id="npz与npy"><a href="#npz与npy" class="headerlink" title="npz与npy"></a>npz与npy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.arrange(<span class="number">10</span>)</span><br><span class="line">np.save(<span class="string">&#x27;arr&#x27;</span>, arr)  <span class="comment"># 数组会以未压缩的原始二进制格式保存在扩展名为.npy的文件中</span></span><br><span class="line">np.load(<span class="string">&#x27;arr.npy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">arr2 = np.arrange(<span class="number">5</span>)</span><br><span class="line">arr3 = np.arrage(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">np.savez(<span class="string">&#x27;arr&#x27;</span>, arr, arr2, arr3_newname = arr3)  <span class="comment"># 可以保存多个arrayw为.npz文件，arr3_newname是arr3的命名</span></span><br><span class="line">data = np.load(<span class="string">&#x27;arr.npz&#x27;</span>)  <span class="comment"># 类似于字典&#123;‘arr’:arr,’arr2’:arr2,’arr3_newname’:arr3&#125;</span></span><br><span class="line">arr = data[<span class="string">&#x27;arr&#x27;</span>]</span><br><span class="line">arr2 = data[<span class="string">&#x27;arr2&#x27;</span>]</span><br><span class="line">arr3 = data[<span class="string">&#x27;arr3_newname&#x27;</span>]</span><br><span class="line"></span><br><span class="line">np.savez_compressed(<span class="string">&#x27;arrz&#x27;</span>, arr, arr2, arr3) <span class="comment"># npz的压缩保存，用法与np.savez()一致。</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch</title>
      <link href="/2021/12/06/pytorch/"/>
      <url>/2021/12/06/pytorch/</url>
      
        <content type="html"><![CDATA[<h3 id="nn-Identity"><a href="#nn-Identity" class="headerlink" title="nn.Identity()"></a>nn.Identity()</h3><p>input 与 output一致，可以使代码逻辑更容易：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch_norm = nn.BatchNorm2d</span><br><span class="line"><span class="keyword">if</span> dont_use_batch_norm:</span><br><span class="line">    batch_norm = Identity</span><br><span class="line"></span><br><span class="line"><span class="comment"># 就可以放心的添加batch_norm了，只需要个参数，就可以选择是否添加该层</span></span><br><span class="line">nn.Sequential(</span><br><span class="line">    ...</span><br><span class="line">    batch_norm(N, momentum=<span class="number">0.05</span>),</span><br><span class="line">    ...</span><br><span class="line">)</span><br><span class="line"><span class="comment"># modifying a pre-defined network to easily remove layers.</span></span><br></pre></td></tr></table></figure><h3 id="torch-chunk"><a href="#torch-chunk" class="headerlink" title="torch.chunk()"></a>torch.chunk()</h3><p>torch.chunk(input, chunks, dim&#x3D;0) → List of Tensors</p><ul><li>input (Tensor) – the tensor to split</li><li>chunks (int) – number of chunks to return</li><li>dim (int) – dimension along which to split the tensor</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">11</span>).chunk(<span class="number">6</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>]),</span><br><span class="line"> tensor([<span class="number">2</span>, <span class="number">3</span>]),</span><br><span class="line"> tensor([<span class="number">4</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">6</span>, <span class="number">7</span>]),</span><br><span class="line"> tensor([<span class="number">8</span>, <span class="number">9</span>]),</span><br><span class="line"> tensor([<span class="number">10</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">12</span>).chunk(<span class="number">6</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>]),</span><br><span class="line"> tensor([<span class="number">2</span>, <span class="number">3</span>]),</span><br><span class="line"> tensor([<span class="number">4</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">6</span>, <span class="number">7</span>]),</span><br><span class="line"> tensor([<span class="number">8</span>, <span class="number">9</span>]),</span><br><span class="line"> tensor([<span class="number">10</span>, <span class="number">11</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">13</span>).chunk(<span class="number">6</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]),</span><br><span class="line"> tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]),</span><br><span class="line"> tensor([ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]),</span><br><span class="line"> tensor([<span class="number">12</span>]))</span><br></pre></td></tr></table></figure><h3 id="torch-matmul"><a href="#torch-matmul" class="headerlink" title="torch.matmul()"></a>torch.matmul()</h3><p>torch乘法方法有三个：</p><ul><li>mm只能进行矩阵乘法,也就是输入的两个tensor维度只能是 $ (n\times m) $和 $(m\times p) $</li><li>bmm是两个三维张量相乘, 两个输入tensor维度是 $(b\times n\times m)$和 $(b\times m\times p) $, 第一维b代表batch size，输出为 $(b\times n \times p) $</li><li>matmul可以进行张量乘法, 输入可以是高维.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = torch.ones(<span class="number">5</span>,<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">c = torch.matmul(a,b)</span><br><span class="line">c.shape <span class="comment"># torch,size[2,5,3,2]</span></span><br></pre></td></tr></table></figure><h3 id="one-hot"><a href="#one-hot" class="headerlink" title="one_hot"></a>one_hot</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">imort torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> function <span class="keyword">as</span> F</span><br><span class="line">a = torch.arange(<span class="number">0</span>, <span class="number">5</span>) % <span class="number">3</span>  <span class="comment"># a =[0,1,2,0,1]</span></span><br><span class="line">a = F.one_hot(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># a: tensor([[1, 0, 0],</span></span><br><span class="line"><span class="comment">#           [0, 1, 0],</span></span><br><span class="line"><span class="comment">#           [0, 0, 1],</span></span><br><span class="line"><span class="comment">#           [1, 0, 0],</span></span><br><span class="line"><span class="comment">#           [0, 1, 0]])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="torch-range-amp-torch-arange"><a href="#torch-range-amp-torch-arange" class="headerlink" title="torch.range &amp; torch.arange"></a>torch.range &amp; torch.arange</h3><p>torch.range(start&#x3D;1, end&#x3D;6) 的结果是会包含end的 ，而且是float32类型.</p><p>而torch.arange(start&#x3D;1, end&#x3D;6)的结果并不包含end，是int64(long)类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.dtype</span><br><span class="line">torch.float32</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z=torch.arange(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.dtype</span><br><span class="line">torch.int64</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建10维数组：</span></span><br><span class="line">a = torch.arange(<span class="number">1</span>,<span class="number">6</span>).repeat(<span class="number">10</span>, <span class="number">5</span>， <span class="number">2</span>) <span class="comment"># 复制得到10 x 5 x （2x10) = 10 x 5 x 20</span></span><br></pre></td></tr></table></figure><h3 id="tensor-view-permute-transpose"><a href="#tensor-view-permute-transpose" class="headerlink" title="tensor.view() permute() transpose()"></a>tensor.view() permute() transpose()</h3><p>permute()能够多维转换tensor维度，transopose()是二维</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">tensor.permute(x,(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>transpose与view不同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">imort torch </span><br><span class="line">x = tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># tensor.size([2, 3])</span></span><br><span class="line">y = x.transpose(<span class="number">1</span>, <span class="number">0</span>)  <span class="comment"># tensor.size([3, 2])</span></span><br><span class="line">z = x.view(<span class="number">3</span>, <span class="number">2</span>)   <span class="comment"># tensor.size([3, 2])</span></span><br><span class="line"><span class="comment"># z = x.view(-1, 2) tensor.size([3, 2])</span></span><br><span class="line">torch.equal(y, z)</span><br><span class="line"><span class="comment">#false</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="comment">#y: </span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment">#z:</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-empty"><a href="#torch-empty" class="headerlink" title="torch.empty()"></a>torch.empty()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.manual_seed(1234) #随机种子无效</span></span><br><span class="line">a = torch.empty((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>), dtype=torch.int32)</span><br><span class="line"></span><br><span class="line">torch.empty_like(a) <span class="comment"># Returns an uninitialized tensor with the same size a</span></span><br><span class="line"></span><br><span class="line">torch.empty(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>) <span class="comment"># 从连续均匀分布[0,1]中采样的数字填充自张量：</span></span><br></pre></td></tr></table></figure><h3 id="torch-rand"><a href="#torch-rand" class="headerlink" title="torch.rand()"></a>torch.rand()</h3>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>common error of Python_coding</title>
      <link href="/2021/12/06/common-error-of-Python-coding/"/>
      <url>/2021/12/06/common-error-of-Python-coding/</url>
      
        <content type="html"><![CDATA[<h2 id="python-error"><a href="#python-error" class="headerlink" title="python error"></a>python error</h2><h4 id="ValueError-too-many-values-to-unpack"><a href="#ValueError-too-many-values-to-unpack" class="headerlink" title="ValueError: too many values to unpack"></a>ValueError: too many values to unpack</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x, y = batch </span><br><span class="line"><span class="comment"># a, b = (1) or a, b=(1,2,3)即赋值不匹配</span></span><br></pre></td></tr></table></figure><h2 id="pytorch-lighting-error"><a href="#pytorch-lighting-error" class="headerlink" title="pytorch lighting error:"></a>pytorch lighting error:</h2><h4 id="TypeError-‘Subset’-object-is-not-callable"><a href="#TypeError-‘Subset’-object-is-not-callable" class="headerlink" title="TypeError: ‘Subset’ object is not callable"></a>TypeError: ‘Subset’ object is not callable</h4><p>优先检查模型维度与输入维度是否一致。<br>困扰许久，模型搭建的LSTM输入维度为三维(Sequence_length x Batch_size x Word_embedding),而我的输入是(Batch_size x Sequence_length)。</p><p>对应pytorch_error: RuntimeError: input must have 3 dimensions, got 2</p><h2 id="pytorch-error"><a href="#pytorch-error" class="headerlink" title="pytorch error"></a>pytorch error</h2><h4 id="fused-dropout-not-implemented-for-‘long’"><a href="#fused-dropout-not-implemented-for-‘long’" class="headerlink" title="fused_dropout not implemented for ‘long’"></a>fused_dropout not implemented for ‘long’</h4><p>dropout 的input feature 必须是float类型, int64(long),int32(int)尝试了都不行</p><h4 id="RuntimeError-one-hot-is-only-applicable-to-index-tensor"><a href="#RuntimeError-one-hot-is-only-applicable-to-index-tensor" class="headerlink" title="RuntimeError: one_hot is only applicable to index tensor."></a>RuntimeError: one_hot is only applicable to index tensor.</h4><p>torch.nn.functional.one_hot(a) a必须是long（int64类型）</p><h2 id="其他错误"><a href="#其他错误" class="headerlink" title="其他错误"></a>其他错误</h2><p>Matplotlib created a temporary config&#x2F;cache directory at &#x2F;tmp&#x2F;matplotlib-4r0tfoj0 because the default path (&#x2F;home&#x2F;caoguangshuo&#x2F;.config&#x2F;matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;MPLCONFIGDIR&#x27;</span>] = os.getcwd() + <span class="string">&quot;Output/configs/&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-ligtning</title>
      <link href="/2021/12/02/pytorch-ligtning/"/>
      <url>/2021/12/02/pytorch-ligtning/</url>
      
        <content type="html"><![CDATA[<p>trainer.fit(model)<br>TypeError: ‘Subset’ object is not callable</p>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python常用函数</title>
      <link href="/2021/12/01/python%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
      <url>/2021/12/01/python%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="torch-hstack"><a href="#torch-hstack" class="headerlink" title="torch.hstack()"></a>torch.hstack()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">torch.hstack(tensors, *, out=<span class="literal">None</span>) → Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.hstack((a,b))</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.hstack((a,b))</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Stack tensors in sequence horizontally (column wise).</p><p>This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.<br>tensor展开</p><h3 id="list-表示"><a href="#list-表示" class="headerlink" title="list[]表示"></a>list[]表示</h3><p>python list</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span> ]</span><br><span class="line">list1[<span class="number">1</span>:<span class="number">5</span>]=[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>] <span class="comment"># list1[1]=2,list[4]=5,不含list1[5]</span></span><br><span class="line">list1[<span class="number">1</span>:] = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>] <span class="comment">#从第二个元素开始截取列表</span></span><br><span class="line">list1[-<span class="number">2</span>] = <span class="number">6</span> <span class="comment"># 读取列表中倒数第二个元素</span></span><br><span class="line"></span><br><span class="line">list2=[]</span><br><span class="line">list2.append(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">list2.append(<span class="string">&quot;b&quot;</span>)</span><br><span class="line">list2= [<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>]</span><br></pre></td></tr></table></figure><p>list.count(obj) #统计元素出现次数</p><p>list.pop([index&#x3D;-1]) # 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值</p><p>list.remove(obj) # 移除列表中某个值的第一个匹配项</p><p>list.reverse() # 反向列表中元素</p><p>list.sort(cmp&#x3D;None, key&#x3D;None, reverse&#x3D;False) # 对原列表进行排序</p><h3 id="torch-bmm"><a href="#torch-bmm" class="headerlink" title="torch.bmm"></a>torch.bmm</h3><p>torch.bmm(input, mat2, *, out&#x3D;None) → Tensor</p><p>Performs a batch matrix-matrix product of matrices stored in input and mat2.input and mat2 must be 3-D tensors each containing the same number of matrices.</p><p>if input is a (b x n x m)tensor, mat2 is a (b x m x p), out will be a (b x n x p)tensor. (b:batch)</p><h3 id="cuda"><a href="#cuda" class="headerlink" title=".cuda()"></a>.cuda()</h3><p>A.cuda()和B.cuda()，使数据在GPU上进行运算,否则就在cpu上计算。</p><p>model的话，建议model.to(device)</p><h3 id="repeat"><a href="#repeat" class="headerlink" title=".repeat()"></a>.repeat()</h3><h4 id="numpy-repeat"><a href="#numpy-repeat" class="headerlink" title="numpy.repeat()"></a>numpy.repeat()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">np.repeat(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.repeat(x, <span class="number">2</span>)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.repeat(x, <span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.repeat(x, [<span class="number">1</span>, <span class="number">2</span>], axis=<span class="number">0</span>)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><h4 id="torch-repeat"><a href="#torch-repeat" class="headerlink" title="torch.repeat()"></a>torch.repeat()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">33</span>, <span class="number">44</span>)</span><br><span class="line">a.repeat(<span class="number">2</span>, <span class="number">1</span>).size() <span class="comment"># (66, 44)</span></span><br><span class="line">a.repeat(batch_size, <span class="number">33</span>, <span class="number">44</span>) <span class="comment"># (batch_size, 33, 44)</span></span><br></pre></td></tr></table></figure><h3 id="torch矩阵维度操作-permute-与transpose-以及-t"><a href="#torch矩阵维度操作-permute-与transpose-以及-t" class="headerlink" title="torch矩阵维度操作 permute()与transpose()以及.t()"></a>torch矩阵维度操作 permute()与transpose()以及.t()</h3><p>x.t() 转置矩阵 x只能为二维矩阵</p><p>torch.transpose(input, dim0&#x3D;0, dim1&#x3D;1, out&#x3D;None) → Tensor 转置矩阵，只能二维, dim0,dim1指定维度</p><p>permute()在高维功能性更强。<br>torch.permute(input, dims) → Tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">x1 = torch.transpose(x, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># [2, 4, 3]</span></span><br><span class="line">x2 = torch.transpose(x, <span class="number">1</span>, <span class="number">0</span>) <span class="comment"># [3, 2, 4]</span></span><br><span class="line">x3 = torch.permute(x, (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)) <span class="comment"># [3, 2, 4]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alpha = self.softmax(torch.bmm(M, self.w.repeat(self.batch_size, <span class="number">1</span>, <span class="number">1</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><p>即$\alpha&#x3D;softmax(w^TM)$</p><h3 id="squeeze"><a href="#squeeze" class="headerlink" title=".squeeze()"></a>.squeeze()</h3><p>numpy.squeeze(a, axis&#x3D;None)<br>a表示输入的数组；<br>axis用于指定需要删除的维度，但是指定的维度必须为单维度，否则将会报错；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = array([[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]])</span><br><span class="line">a.shape  <span class="comment"># (1, 2, 5)</span></span><br><span class="line">np.squeeze(a) <span class="comment"># a.shape: (2, 5)</span></span><br></pre></td></tr></table></figure><p>torch,squeeze() unsqueeze()</p><h3 id="set-seed"><a href="#set-seed" class="headerlink" title="set_seed()"></a>set_seed()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():  <span class="comment"># GPU operation have separate seed</span></span><br><span class="line">        torch.cuda.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure><h3 id="view"><a href="#view" class="headerlink" title=".view()"></a>.view()</h3><p>重构tensor维度</p><p>view(a,b,…) tensor重构为(a x b x …)</p><p>view(a, -1),tensor重构为(a x -1),-1需要计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a=torch.Tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line">a=a.view(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a) </span><br><span class="line">a=a.view(<span class="number">2</span>,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure><h3 id="isinstance"><a href="#isinstance" class="headerlink" title="isinstance()"></a>isinstance()</h3><p>isinstance() 函数来判断一个对象是否是一个已知的类型，类似 type()<br>isinstance() 与 type() 区别：</p><ul><li>type() 不会认为子类是一种父类类型，不考虑继承关系。</li><li>isinstance() 会认为子类是一种父类类型，考虑继承关系。</li></ul><p>如果要判断两个类型是否相同推荐使用 isinstance().</p><p>isinstance(object, classinfo)<br>return True or False</p><h3 id="and-or-amp"><a href="#and-or-amp" class="headerlink" title="and or &amp; |"></a>and or &amp; |</h3><p>如果a，b是数值变量， 则&amp;， |表示位运算， and，or则依据是否非0来决定输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1&amp;2，2在二进制里面是10,1在二进制中是01，那么01与运算10得到是0 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">1</span> &amp; <span class="number">2</span>         <span class="comment"># 输出为 0， </span></span><br><span class="line"><span class="number">1</span> | <span class="number">2</span>          <span class="comment"># 输出为3</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># 判断变量是否为0， 是0则为False，非0判断为True，</span></span><br><span class="line"> <span class="comment"># and中含0，返回0； 均为非0时，返回后一个值， </span></span><br><span class="line"><span class="number">2</span> <span class="keyword">and</span> <span class="number">0</span>   <span class="comment"># 返回0</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">and</span> <span class="number">1</span>   <span class="comment"># 返回1</span></span><br><span class="line"><span class="number">1</span> <span class="keyword">and</span> <span class="number">2</span>   <span class="comment"># 返回2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or中， 至少有一个非0时，返回第一个非0,</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">or</span> <span class="number">0</span>   <span class="comment"># 返回2</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">or</span> <span class="number">1</span>   <span class="comment"># 返回2</span></span><br><span class="line"><span class="number">0</span> <span class="keyword">or</span> <span class="number">1</span>   <span class="comment"># 返回1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="dim-x3D-1"><a href="#dim-x3D-1" class="headerlink" title="dim&#x3D;-1"></a>dim&#x3D;-1</h3><p>关于维度可以用一句话总结，0表示张量的最高维度，1表示张张量的次高维度，2表示张量的次次高维度，以此类推。-1表示张量维度的最低维度，-2表示倒数第二维度，-3表示倒数第三维度。</p><p>共3个维度时，dim&#x3D;-1 与 dim&#x3D;2相同</p><p><a href="dim.png">3维dim</a></p><h3 id="abs"><a href="#abs" class="headerlink" title="abs()"></a>abs()</h3><p>高阶内置函数，绝对值函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f = <span class="built_in">abs</span></span><br><span class="line">a = f(-<span class="number">1</span>) <span class="comment"># a = 1</span></span><br></pre></td></tr></table></figure><h3 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate()"></a>enumerate()</h3><p>enumerate(sequence, start&#x3D;0)</p><ul><li>sequence – 一个序列、迭代器或其他支持迭代对象。</li><li>start – 下标起始位置。</li></ul><h3 id="shell-向python传入参数"><a href="#shell-向python传入参数" class="headerlink" title="shell 向python传入参数"></a>shell 向python传入参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">a = sys.argv[<span class="number">1</span>]</span><br><span class="line">b = sys.argv[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a,b)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py 1 2 # (1, 2)</span><br></pre></td></tr></table></figure><h3 id="读取文件："><a href="#读取文件：" class="headerlink" title="读取文件："></a>读取文件：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> a, <span class="built_in">open</span>(<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> b:</span><br><span class="line">        do_something()</span><br><span class="line"><span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;Operation failed: %s&#x27;</span> % e.strerror</span><br></pre></td></tr></table></figure><h3 id="路径合并"><a href="#路径合并" class="headerlink" title="路径合并"></a>路径合并</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.path.join(<span class="string">&#x27;root&#x27;</span>,<span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;runoob.txt&#x27;</span>)   <span class="comment"># 将目录和文件名合成一个路径</span></span><br></pre></td></tr></table></figure><h3 id="四舍五入"><a href="#四舍五入" class="headerlink" title="四舍五入"></a>四舍五入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">round</span>(x, n) </span><br><span class="line"><span class="comment"># round() 方法返回浮点数x的四舍五入值, 保留n位小数</span></span><br></pre></td></tr></table></figure><h3 id="zip"><a href="#zip" class="headerlink" title="zip()"></a>zip()</h3><p>zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>zipped = <span class="built_in">zip</span>(a,b)     <span class="comment"># 打包为元组的列表</span></span><br><span class="line">[(<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">zip</span>(a,c)              <span class="comment"># 元素个数与最短的列表一致</span></span><br><span class="line">[(<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">zip</span>(*zipped)          <span class="comment"># 与 zip 相反，*zipped 可理解为解压，返回二维矩阵式</span></span><br><span class="line">[(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), (<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM</title>
      <link href="/2021/12/01/LSTM/"/>
      <url>/2021/12/01/LSTM/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> LSTM </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XLnet</title>
      <link href="/2021/11/29/XLnet/"/>
      <url>/2021/11/29/XLnet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>csv_reader &amp; h5py</title>
      <link href="/2021/11/24/csv-reader/"/>
      <url>/2021/11/24/csv-reader/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">train=<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_train.raw&quot;</span></span><br><span class="line">test=<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_test.raw&quot;</span></span><br><span class="line">SNPs_train = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_train_SNPs.hdf5&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">Phenotype_train = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_train_Phenotype.hdf5&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">SNPs_test = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_test_SNPs.hdf5&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">Phenotype_test = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_test_Phenotype.hdf5&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(train,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = csv.reader([fd <span class="keyword">for</span> fd <span class="keyword">in</span> f], delimiter=<span class="string">&#x27; &#x27;</span>)    </span><br><span class="line">    header = <span class="built_in">next</span>(lines)</span><br><span class="line">    <span class="keyword">for</span> i,line <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(lines)):      </span><br><span class="line">        SNPs_train[line[<span class="number">0</span>]]=np.array(line[<span class="number">6</span>:]).astype(<span class="string">&quot;int&quot;</span>)  </span><br><span class="line">        Phenotype_train[line[<span class="number">0</span>]]=np.array(line[<span class="number">5</span>]).astype(<span class="string">&quot;int&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done&quot;</span>)</span><br></pre></td></tr></table></figure><p>why header&#x3D;next(lines)</p><p>The header row is “skipped” as a result of calling next(). That’s how iterators work.</p><p>When you loop over an iterator, its next() method is called each time. Each call advances the iterator. When the for loop starts, the iterator is already at the second row, and it goes from there on.</p><p>Here’s <a href="https://docs.python.org/2/library/stdtypes.html#iterator.next">the documentation</a> on the next() method (<a href="https://docs.python.org/2/reference/expressions.html#generator.next">here’s another piece</a>).</p><p>What’s important is that csv.reader objects are iterators, just like file object returned by open(). You can iterate over them, but they don’t contain all of the lines (or any of the lines) at any given moment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"></span><br><span class="line">SNPs_train_hdf5 = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_train_SNPs.hdf5&quot;</span>,<span class="string">&quot;r&quot;</span>)</span><br><span class="line">Phenotype_train_hdf5 = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_train_Phenotype.hdf5&quot;</span>,<span class="string">&quot;r&quot;</span>)</span><br><span class="line">SNPs_test_hdf5 = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_test_SNPs.hdf5&quot;</span>,<span class="string">&quot;r&quot;</span>)</span><br><span class="line">Phenotype_test_hdf5 = h5py.File(<span class="string">&quot;/media/bgi/caoguangshuo/PRS/GDM/4.PRS/1e-03/GDM_1e-03_test_Phenotype.hdf5&quot;</span>,<span class="string">&quot;r&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># list(SNPs.keys())  # sample id</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hdf52arr</span>(<span class="params">hdf5</span>):</span><br><span class="line">    arr=[]</span><br><span class="line">    <span class="keyword">for</span> i, index <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">list</span>(hdf5.keys())):  </span><br><span class="line">        arr.append(hdf5[index][()]) <span class="comment"># [:] will occur ValueError: Illegal slicing argument for scalar dataspace</span></span><br><span class="line">    arr = np.array(arr)</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line">SNPs_train=hdf52arr(SNPs_train_hdf5)</span><br><span class="line">SNPs_train.shape</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> csvreader </tag>
            
            <tag> h5py </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prs_基本流程</title>
      <link href="/2021/11/23/prs-%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
      <url>/2021/11/23/prs-%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 生信123 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
            <tag> PRS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生信绘图</title>
      <link href="/2021/11/22/%E7%94%9F%E4%BF%A1%E7%BB%98%E5%9B%BE/"/>
      <url>/2021/11/22/%E7%94%9F%E4%BF%A1%E7%BB%98%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<p>这样保存的pdf文字不再是描点，可以方便在AI里修图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.rcParams[<span class="string">&#x27;pdf.fonttype&#x27;</span>] = <span class="number">42</span>  <span class="comment"># 42</span></span><br><span class="line">plt.savefig(<span class="string">&quot;results_plt/raw_data_lab.pdf&quot;</span>, <span class="built_in">format</span>=<span class="string">&#x27;pdf&#x27;</span>, dpi=<span class="number">400</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3D图 两种画法，matplotlib 或 plotly画动态</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 动态3D:</span></span><br><span class="line"><span class="keyword">import</span> plotly.graph_objects <span class="keyword">as</span> go</span><br><span class="line"></span><br><span class="line">color=adata_all.obs[<span class="string">&quot;3d_map_colors&quot;</span>]</span><br><span class="line"></span><br><span class="line">x=adata_all.obs[<span class="string">&quot;array_col&quot;</span>].to_numpy() * <span class="number">1</span></span><br><span class="line">y=adata_all.obs[<span class="string">&quot;array_row&quot;</span>].to_numpy() * <span class="number">1</span></span><br><span class="line">z = (adata_all.obs[<span class="string">&quot;batch&quot;</span>].cat.codes + <span class="number">1</span>) * <span class="number">10</span></span><br><span class="line"><span class="comment"># Helix equation</span></span><br><span class="line"></span><br><span class="line">fig = go.Figure(data=[</span><br><span class="line">    go.Scatter3d(</span><br><span class="line">    x=x, y=y, z=z,</span><br><span class="line">    mode=<span class="string">&#x27;markers&#x27;</span>,</span><br><span class="line">    marker=<span class="built_in">dict</span>(</span><br><span class="line">    size=<span class="number">2.5</span>,</span><br><span class="line">    color=color,                <span class="comment"># set color to an array/list of desired values</span></span><br><span class="line">    opacity=<span class="number">1</span>,</span><br><span class="line">    )</span><br><span class="line">    )])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig.update_layout(</span><br><span class="line">    showlegend=<span class="literal">True</span>, </span><br><span class="line">    width=<span class="number">900</span>, height=<span class="number">600</span>,</span><br><span class="line">    legend=<span class="built_in">dict</span>(</span><br><span class="line">        yanchor=<span class="string">&quot;top&quot;</span>,</span><br><span class="line">        y=<span class="number">0.99</span>,</span><br><span class="line">        xanchor=<span class="string">&quot;left&quot;</span>,</span><br><span class="line">        x=<span class="number">0.01</span>)</span><br><span class="line">    )</span><br><span class="line">fig.update_scenes(xaxis_autorange=<span class="string">&quot;reversed&quot;</span>)</span><br><span class="line"></span><br><span class="line">fig.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># from kaleido.scopes.plotly import PlotlyScope</span></span><br><span class="line"><span class="comment"># scope = PlotlyScope(</span></span><br><span class="line"><span class="comment">#     plotlyjs=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;,</span></span><br><span class="line"><span class="comment">#     # plotlyjs=&quot;/path/to/local/plotly.js&quot;,</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"><span class="comment"># with open(f&#x27;&#123;name&#125;.png&#x27;, &quot;wb&quot;) as f:</span></span><br><span class="line"><span class="comment">#     f.write(scope.transform(fig, format=&quot;png&quot;, width=900, height=600))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Matplotlib</span></span><br><span class="line">labels = [<span class="string">&#x27;#ff7f0e&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;#8c564b&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;#1f77b4&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;#279e68&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;#aa40fc&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;#e377c2&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;#d62728&#x27;</span>]</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"></span><br><span class="line">custom_lines = [Line2D([<span class="number">0</span>], [<span class="number">0</span>], marker=<span class="string">&#x27;o&#x27;</span>, color=labels[<span class="number">0</span>], label=<span class="string">&#x27;cluster_1&#x27;</span>, markersize=<span class="number">1</span>, lw=<span class="number">0</span>),</span><br><span class="line">                Line2D([<span class="number">0</span>], [<span class="number">0</span>], marker=<span class="string">&#x27;o&#x27;</span>, color=labels[<span class="number">1</span>], label=<span class="string">&#x27;cluster_2&#x27;</span>, markersize=<span class="number">1</span>, lw=<span class="number">0</span>),</span><br><span class="line">                Line2D([<span class="number">0</span>], [<span class="number">0</span>], marker=<span class="string">&#x27;o&#x27;</span>, color=labels[<span class="number">2</span>], label=<span class="string">&#x27;cluster_3&#x27;</span>, markersize=<span class="number">1</span>, lw=<span class="number">0</span>),</span><br><span class="line">                Line2D([<span class="number">0</span>], [<span class="number">0</span>], marker=<span class="string">&#x27;o&#x27;</span>, color=labels[<span class="number">3</span>], label=<span class="string">&#x27;cluster_4&#x27;</span>, markersize=<span class="number">1</span>, lw=<span class="number">0</span>),</span><br><span class="line">                Line2D([<span class="number">0</span>], [<span class="number">0</span>], marker=<span class="string">&#x27;o&#x27;</span>, color=labels[<span class="number">4</span>], label=<span class="string">&#x27;cluster_5&#x27;</span>, markersize=<span class="number">1</span>, lw=<span class="number">0</span>),</span><br><span class="line">                Line2D([<span class="number">0</span>], [<span class="number">0</span>], marker=<span class="string">&#x27;o&#x27;</span>, color=labels[<span class="number">5</span>], label=<span class="string">&#x27;cluster_6&#x27;</span>, markersize=<span class="number">1</span>, lw=<span class="number">0</span>),</span><br><span class="line">                Line2D([<span class="number">0</span>], [<span class="number">0</span>], marker=<span class="string">&#x27;o&#x27;</span>, color=labels[<span class="number">6</span>], label=<span class="string">&#x27;cluster_7&#x27;</span>, markersize=<span class="number">1</span>, lw=<span class="number">0</span>),</span><br><span class="line">               ]</span><br><span class="line"></span><br><span class="line">centimeter = <span class="number">1</span> <span class="comment">#1/2.54  # centimeter in inches</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">3</span>*centimeter, <span class="number">3</span>*centimeter), dpi=<span class="number">400</span>)</span><br><span class="line">ax = plt.axes(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax.tick_params(which=<span class="string">&#x27;both&#x27;</span>, width=<span class="number">0.25</span>,length=<span class="number">0.01</span>, labelsize=<span class="number">0</span>, pad=-<span class="number">6</span>)</span><br><span class="line">ax.xaxis.set_tick_params(pad=-<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ax.yaxis.set_major_locator(plt.NullLocator())</span></span><br><span class="line"><span class="comment"># ax.xaxis.set_major_locator(plt.NullLocator())</span></span><br><span class="line"><span class="comment"># ax.zaxis.set_major_locator(plt.NullLocator())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ax.grid(False)</span></span><br><span class="line">ax.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">line_width = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> axis <span class="keyword">in</span> [ax.xaxis, ax.yaxis, ax.zaxis]:</span><br><span class="line">    axis._axinfo[<span class="string">&quot;grid&quot;</span>][<span class="string">&#x27;linewidth&#x27;</span>] = line_width</span><br><span class="line">    axis._axinfo[<span class="string">&quot;grid&quot;</span>][<span class="string">&#x27;linestyle&#x27;</span>] = <span class="string">&quot;:&quot;</span></span><br><span class="line">    axis._axinfo[<span class="string">&#x27;tick&#x27;</span>][<span class="string">&#x27;inward_factor&#x27;</span>] =<span class="number">0</span></span><br><span class="line">    axis._axinfo[<span class="string">&#x27;tick&#x27;</span>][<span class="string">&#x27;outward_factor&#x27;</span>] =<span class="number">0</span></span><br><span class="line">    axis._axinfo[<span class="string">&#x27;tick&#x27;</span>][<span class="string">&#x27;width&#x27;</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> axis <span class="keyword">in</span> [ax.w_xaxis, ax.w_yaxis, ax.w_zaxis]:</span><br><span class="line">    axis.line.set_linewidth(line_width)</span><br><span class="line">    axis.line.set_color(<span class="string">&#x27;grey&#x27;</span>)</span><br><span class="line"></span><br><span class="line">color=adata_all.obs[<span class="string">&quot;3d_map_colors&quot;</span>]</span><br><span class="line"></span><br><span class="line">x=adata_all.obs[<span class="string">&quot;array_col&quot;</span>].to_numpy() * <span class="number">1</span></span><br><span class="line">y=adata_all.obs[<span class="string">&quot;array_row&quot;</span>].to_numpy() * <span class="number">1</span></span><br><span class="line">z = (adata_all.obs[<span class="string">&quot;batch&quot;</span>].cat.codes + <span class="number">1</span>) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="built_in">max</span>(adata_all.obs[<span class="string">&quot;array_col&quot;</span>]))</span><br><span class="line">plt.ylim(<span class="built_in">max</span>(adata_all.obs[<span class="string">&quot;array_row&quot;</span>]),<span class="number">0</span>)</span><br><span class="line">ax.scatter3D(x, y, z, c = color, s=<span class="number">0.75</span>, alpha=<span class="number">1</span>, marker=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax.legend(handles=custom_lines,loc=<span class="string">&#x27;right&#x27;</span>, fontsize=<span class="number">3</span>, bbox_to_anchor=(<span class="number">1.13</span>, <span class="number">0.5</span>), frameon=<span class="literal">False</span>)</span><br><span class="line">ax.view_init(<span class="number">14</span>, -<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.savefig(f&#x27;./result/plot-73-76-3d.pdf&#x27;, bbox_inches=&#x27;tight&#x27;, format=&#x27;pdf&#x27;, dpi=400)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 生信123 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 生物信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux命令</title>
      <link href="/2021/11/22/linux%E5%91%BD%E4%BB%A4/"/>
      <url>/2021/11/22/linux%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="Linux常用命令积累"><a href="#Linux常用命令积累" class="headerlink" title="Linux常用命令积累"></a>Linux常用命令积累</h2><h3 id="bashrc-快捷命令"><a href="#bashrc-快捷命令" class="headerlink" title=".bashrc 快捷命令"></a>.bashrc 快捷命令</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias command=<span class="string">&#x27;conda activate command&#x27;</span> <span class="comment">#注意不要有空格</span></span><br></pre></td></tr></table></figure><h3 id="查看文件夹容量"><a href="#查看文件夹容量" class="headerlink" title="查看文件夹容量"></a>查看文件夹容量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">du -sh <span class="comment"># 查看当前文件夹容量</span></span><br><span class="line">du  -sh * | sort -n <span class="comment"># 统计当前文件夹大小，并按文件大小排序</span></span><br></pre></td></tr></table></figure><h3 id="文件夹操作"><a href="#文件夹操作" class="headerlink" title="文件夹操作"></a>文件夹操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当前文件夹 ./</span><br><span class="line">cd</span><br><span class="line">mkdir</span><br><span class="line">rm -rf /root/<span class="built_in">dir</span> <span class="comment">#删除/root/dir目录以及其下所有文件、文件夹 （-r 递归向下删除 -f强制）</span></span><br><span class="line">rm -f /var/log/httpd/access.log <span class="comment">#这个将会强制删除/var/log/httpd/access.log这个文件</span></span><br></pre></td></tr></table></figure><h3 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#移动文件</span></span><br><span class="line">mv /data/new/* /data/old/</span><br><span class="line">mv /*.txt /out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件</span></span><br><span class="line">CP命令</span><br><span class="line">格式: CP [选项]  源文件或目录   目的文件或目录</span><br><span class="line">选项说明:-b 同名,备分原来的文件</span><br><span class="line">        -f 强制覆盖同名文件</span><br><span class="line">        -r  按递归方式保留原目录结构复制文件</span><br></pre></td></tr></table></figure><h3 id="查看文件"><a href="#查看文件" class="headerlink" title="查看文件"></a>查看文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">1.</span> cat语法：cat [-n]  文件名 （-n ： 显示时，连行号一起输出）</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> less的语法：less 文件名</span><br><span class="line">less -S 文件名 <span class="comment">#行过长时间将超出部分舍弃 对齐输出</span></span><br><span class="line">less -S | wc 统计 :<span class="number">3</span> <span class="number">92</span> <span class="number">598</span> testfile <span class="comment"># testfile文件的行数为3、单词数92、字节数598 </span></span><br><span class="line">less还有一个功能，可以在文件中进行搜索你想找的内容，假设你想在passwd文件中查找有没有weblogic字符串，那么你可以这样来做：</span><br><span class="line">[root@redhat etc]<span class="comment"># less passwd</span></span><br><span class="line">然后输入：</span><br><span class="line">/weblogic</span><br><span class="line">回车</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">3.</span>head和tail</span><br><span class="line">head和tail通常使用在只需要读取文件的前几行或者后几行的情况下使用。head的功能是显示文件的前几行内容</span><br><span class="line">head的语法：head [n number] 文件名 (number 显示行数)</span><br><span class="line"></span><br><span class="line">tail的功能恰好和head相反，只显示最后几行内容</span><br><span class="line">tail的语法:tail [-n number] 文件名</span><br><span class="line"></span><br><span class="line">若只是查看文件内容，more和less是不错的选择。特别对于大文件，打开速度非常快。less命令的好处是能够提供查找高亮；</span><br><span class="line">如果用于从文件中提取内容，首选cat。使用重定向，按照某种固定的规则将文件内容输出到指定文件中。如果文件特别大，也可考虑head和tail，只输出最早或者最近的几行。特别值得一提的是tail -f [文件名]，可以实时监控文件的内容。</span><br></pre></td></tr></table></figure><h3 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h3><p>sshfs <a href="mailto:&#99;&#97;&#x6f;&#x67;&#x75;&#97;&#x6e;&#x67;&#x73;&#104;&#117;&#111;&#x40;&#x31;&#x39;&#50;&#x2e;&#x31;&#x36;&#56;&#x2e;&#49;&#x36;&#46;&#49;&#48;">&#99;&#97;&#x6f;&#x67;&#x75;&#97;&#x6e;&#x67;&#x73;&#104;&#117;&#111;&#x40;&#x31;&#x39;&#50;&#x2e;&#x31;&#x36;&#56;&#x2e;&#49;&#x36;&#46;&#49;&#48;</a>:&#x2F; .&#x2F;UKB</p><p>将<a href="mailto:&#99;&#x61;&#x6f;&#x67;&#117;&#x61;&#110;&#x67;&#115;&#104;&#117;&#x6f;&#64;&#x31;&#x39;&#x32;&#x2e;&#49;&#x36;&#56;&#46;&#49;&#x36;&#x2e;&#x31;&#48;">&#99;&#x61;&#x6f;&#x67;&#117;&#x61;&#110;&#x67;&#115;&#104;&#117;&#x6f;&#64;&#x31;&#x39;&#x32;&#x2e;&#49;&#x36;&#56;&#46;&#49;&#x36;&#x2e;&#x31;&#48;</a>:&#x2F; 路径 挂载到.&#x2F;UKB目录下</p><h3 id="查看CPU"><a href="#查看CPU" class="headerlink" title="查看CPU"></a>查看CPU</h3><p>top</p><p>free</p><h3 id="vim跳转行尾"><a href="#vim跳转行尾" class="headerlink" title="vim跳转行尾"></a>vim跳转行尾</h3><p>1.跳到文本的最后一行：按“G”,即“shift+g”</p><p>2.跳到最后一行的最后一个字符 ： 先重复1的操作即按“G”，之后按“$”键，即“shift+4”。</p><p>3.跳到第一行的第一个字符：先按两次“g”，</p><p>4.跳转到当前行的第一个字符：在当前行按“0”。</p><h3 id="shell命令跳转行尾"><a href="#shell命令跳转行尾" class="headerlink" title="shell命令跳转行尾"></a>shell命令跳转行尾</h3><p>mac ctrl-&gt; control</p><p>ctrl+a: 光标移到行首<br>ctrl+e: 光标移到行尾<br>ctrl+k:清除光标后至行尾的内容。<br>ctrl+l:清屏，相当于clear</p><h3 id="shell输出重定向"><a href="#shell输出重定向" class="headerlink" title="shell输出重定向"></a>shell输出重定向</h3><p>command1 &gt; file1</p><p>上面这个命令执行command1然后将输出的内容存入file1。</p><p>command 2&gt;&gt;fil</p><p>如果希望 stderr 追加到 file 文件末尾</p><h3 id="vim删除所有内容"><a href="#vim删除所有内容" class="headerlink" title="vim删除所有内容"></a>vim删除所有内容</h3><p>ggdG</p><p>gg为跳转到文件首行；dG为删除光标所在行以及其下所有行的内容；</p><p>再细讲，d为删除，G为跳转到文件末尾行；</p><h3 id="shell-数组与循环"><a href="#shell-数组与循环" class="headerlink" title="shell 数组与循环"></a>shell 数组与循环</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array=(1e-5 1e-6 1e-7 1e-8)</span><br><span class="line"></span><br><span class="line">for element in $&#123;array[@]&#125;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">也可以写成<span class="keyword">for</span> element <span class="keyword">in</span> <span class="variable">$&#123;array[*]&#125;</span></span></span><br><span class="line">do</span><br><span class="line">echo $element</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="查看内存占用"><a href="#查看内存占用" class="headerlink" title="查看内存占用"></a>查看内存占用</h2><p>ps aux –sort -rss | head -10</p><p>htop</p><h2 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h2><p>ps aux</p><h2 id="Linux挂载"><a href="#Linux挂载" class="headerlink" title="Linux挂载"></a>Linux挂载</h2><p>在Ubuntu中使用sshfs命令进行挂载远端服务器的磁盘到本地。</p><p>使用方式如下：</p><p>1、挂载</p><p>sshfs 【服务器名称】@服务器地址：【挂载目录】  【本地挂载点】</p><p>sshfs <a href="mailto:&#x75;&#x73;&#x65;&#x72;&#x6e;&#97;&#x6d;&#101;&#x40;&#x31;&#57;&#50;&#46;&#49;&#x36;&#x38;&#46;&#120;&#120;&#x78;&#x2e;&#x78;&#x78;&#120;">&#x75;&#x73;&#x65;&#x72;&#x6e;&#97;&#x6d;&#101;&#x40;&#x31;&#57;&#50;&#46;&#49;&#x36;&#x38;&#46;&#120;&#120;&#x78;&#x2e;&#x78;&#x78;&#120;</a>:&#x2F;home&#x2F;xxx&#x2F;xxx       mountpoint  </p><p>2、取消挂载<br>fusermount  -u  mountpoint</p><p>在挂载服务器的代码到本地时，使用服务器的代码进行fastboot烧机，出现权限不够！</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读--Pitfalls of predicting complex traits from SNPs</title>
      <link href="/2021/11/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Pitfalls-of-predicting-complex-traits-from-SNPs/"/>
      <url>/2021/11/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Pitfalls-of-predicting-complex-traits-from-SNPs/</url>
      
        <content type="html"><![CDATA[<h2 id="Pitfalls-of-predicting-complex-traits-from-SNPs1-–-从SNPs预测复杂性状的陷阱"><a href="#Pitfalls-of-predicting-complex-traits-from-SNPs1-–-从SNPs预测复杂性状的陷阱" class="headerlink" title="Pitfalls of predicting complex traits from SNPs1 – 从SNPs预测复杂性状的陷阱"></a>Pitfalls of predicting complex traits from SNPs<a href="#refer-anchor"><sup>1</sup></a> – 从SNPs预测复杂性状的陷阱</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>The success of genome-wide association studies (GWASs) has led to increasing interest in making predictions of complex trait phenotypes, including disease, from genotype data. Rigorous assessment of the value of predictors is crucial before implementation. Here we discuss some of the limitations and pitfalls of prediction analysis and show how naive implementations can lead to severe bias and misinterpretation of results.</p></blockquote><p>全基因组关联研究(GWASs）的成功使人们越来越关注从基因型数据预测复杂性状表型，包括疾病。在实施之前，对预测值的严格评估至关重要。在这里，我们讨论了预测分析的一些局限性和缺陷，并展示了简单的实现如何导致结果的严重偏差和误解。</p><p>在许多物种中，通过全基因组关联研究（GWASs）发现了单核苷酸多态性（SNP)——性状关联(single-nucleotide polymorphisms (SNPs)–trait associations)。除了性状相关变异体及其生物学功能的发现外，从植物和动物育种、实验生物体和人类群体的基因型数据预测复杂性状表型的兴趣也越来越大。这些预测基于SNP（或其他基因组变体）的选择及其在发现样本中的效应估计，随后在具有已知表型的独立样本中进行验证，并最终应用于具有未知表型的样本（如图1）。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/11/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Pitfalls-of-predicting-complex-traits-from-SNPs/1.png" alt="图1"></p><p>基于SNPs的预测分析，分为三个步骤：发现（构建预测器）、验证（预测器）、应用（预测器）（如图1所示）。</p><p>论文认为验证阶段将是整个过程的主要焦点。在这个阶段，错误的结论可能会导致预测结果不如推测的那样有效，或者在最坏的情况下，根本没有预测准确性。我们将我们的观点组织成预测分析的局限性和常见陷阱。考虑到该特征的性质或可用数据，这些局限性部分是固有的。这些是使用者应该注意的因素，但大多数是无法改变的。这些限制还反映了使用了可改进的次优方法。陷阱（Pitfalls）是分析中的常见错误，可能导致预测结果准确性过高或对导致错误的结果，我们从发生这些错误的文献中给出了一些例子。我们就如何最好地避免，基于SNP的预测器在实际应用中的推导和应用中的陷阱给出了我们的意见。风险预测有许多方面超出了本文的范围。其中包括对可用于发现阶段的统计方法的彻底处理，使用非遗传信息源进行预测或诊断，全面讨论风险预测在人类医学中的临床应用，以及讨论在人类群体中应用的伦理考虑。</p><h2 id="Limitations-of-prediction-analyses-预测分析的局限性"><a href="#Limitations-of-prediction-analyses-预测分析的局限性" class="headerlink" title="Limitations of prediction analyses 预测分析的局限性"></a>Limitations of prediction analyses 预测分析的局限性</h2><h4 id="Limitation-1-prediction-of-phenotypes-from-genetic-markers-从遗传标记预测表型"><a href="#Limitation-1-prediction-of-phenotypes-from-genetic-markers-从遗传标记预测表型" class="headerlink" title="Limitation 1: prediction of phenotypes from genetic markers  从遗传标记预测表型"></a>Limitation 1: prediction of phenotypes from genetic markers  从遗传标记预测表型</h4><p>局限性1：从遗传标记预测表型：复杂性状的变异几乎总是由遗传和环境因素的综合作用造成的。遗传因子重要性的一个有用量化是遗传力（$h^2$）:即遗传因子引起的性状表型变异的比例。</p><p>假设估计的$h^2$是群体参数的真实反映，则$h^2$是基于DNA标记物（如SNP）的线性预测因子（$R^2$）解释的表型方差上限，因此遗传预测因子永远无法完全解释所有表型变异。环境风险因素可以添加到遗传预测因子(genetic predictor)中，以更好地预测表型。实际上，并不是所有的环境因素都被识别出来（一些被归类为“环境”的因素可能只是随机事件）。例如，将SNPs和表型预测因子(phenotypic predictors)（如体重指数和吸烟）相结合，可以改善对年龄相关性黄斑变性的预测，这是一种年龄是主要危险因素的人类眼病。在某些情况下，更准确的表型分析，包括重复测量，可以产生更具遗传性的性状。一般来说，需要对人类表型或疾病预测的应用进行相应调整。</p><p>与完全渗透性孟德尔疾病的确定性基因测试不同，复杂性状的基因预测将是概率性的，并且在临床决策中，该值可能只是增量。遗传风险预测的价值可能在群体层面，而不是个人层面。尽管如此，通过使用遗传预测因子来确定疾病预防干预应重点关注的高风险阶层，可能会产生具有成本效益的公共卫生战略。在农业中，遗传风险预测主要是基于对亲代中加性遗传值（即估计的育种值）的估计来选择种畜；目的是引起后代表型的平均变化。也就是说，遗传预测的影响自然是在一个群体而不是个人的层面上。</p><h3 id="Limitation-2-variance-explainable-by-markers-可由标记基因解释的变异"><a href="#Limitation-2-variance-explainable-by-markers-可由标记基因解释的变异" class="headerlink" title="Limitation 2: variance explainable by markers 可由标记基因解释的变异"></a>Limitation 2: variance explainable by markers 可由标记基因解释的变异</h3><p>用于识别与复杂性状相关的 SNP 的全基因组 SNP 芯片中包含的 SNP 通常不是表型的因果变异——它们更有可能与性状相关，因为它们处于连锁不平衡 (linkage disequilibrium,LD) 具有一种或多种因果变异（casual variants)。</p><blockquote><p>If the variation generated by the causal variants is completely explained by the genotyped SNPs, then the SNPs can potentially explain all of the genetic variation in the trait (that is, $h_m^2$ &#x3D; $h^2$, where $h_m^2$ is defined as the genetic variation captured by the SNPs or markers). Sometimes, $h_m^2$ is referred to as the ‘narrow-sense heritability’; however, in our opinion, the term narrow-sense heritability should be reserved as the definition of the total additive genetic variance: that is, $h^2$.</p></blockquote><p>如果因果变异产生的变异完全由基因分型SNP解释，那么SNP可以潜在地解释性状中的所有遗传变异（即 $h_m^2$ &#x3D; $h^2$，其中 $h_m^2$ 定义为 SNP 捕获的遗传变异或标记）。有时，$h_m^2$ 被称为“狭义遗传力”；然而，在我们看来，术语狭义遗传力应该保留作为总加性遗传方差的定义,即 $h^2$。</p><p>SNP对健康的影响越大，预期因果等位基因的频率越低。 例如，导致人类严重智力障碍的个体突变很少见。因此，在实践中，在发现群体中确定为关联的SNP不太可能解释所有遗传变异（即 $h_m^2$ &lt; $h^2$），因为罕见变异对变异的贡献可能不会被基因分型的 SNP 标记。 例如，对于身高和精神分裂症，身高的$h^2$≈0.7–0.8 和 $h_m^2$≈0.5，精神分裂症的$h_m^2$&#x3D;0.2–0.3</p><blockquote><p>The difference between the variance explained by genome-wide-significant SNPs ($h_{GWS}$) and heritability estimates from family studies ($h_2$) has been called the ‘miss-ing heritability’, and the difference between $h_{GWS}$ and $h_M^2$ has been described as the ‘hidden heritability’.</p></blockquote><p>全基因组显著性SNP（$h_{GWS}$）解释的方差与家族研究的遗传力估计值（$h_2$）之间的差异被称为“缺失遗传力”，$h_{GWS}$与$h_M^2$之间的差异被称为“隐性遗传力”,$h_M^2$与$h_2$之间的差异称为‘still missing heritability’，$h_{GWS} &lt; h_M^2 &lt; h_2$。</p><p>仍然缺失的遗传力（still missing heritability）可能仅仅反映了SNP标记不好的基因组变异。在家畜种群中，当以这种方式定义缺失遗传力时，几乎没有缺失，高达97%的遗传力被普通SNPs捕获，这可能是因为较小的有效种群规模会导致长期LD，因此，即使是罕见的等位基因，也可以通过LD中的SNP与因果变异的线性组合来预测（即排除了still missing hertability）。然而，即使是在奶牛身上，可以合理地假设处于强自然选择下的性状，如生育能力，也有更大的缺失遗传力。此外，当SNPs与一个系谱拟合时，多达一半的遗传变异是由系谱而不是SNPs解释的。最简单的解释是，在家畜和人类中，一些因果变异是罕见的，并且在具有SNP的不良LD中。</p><blockquote><p>With the advances in whole-genome sequencing technologies, causative mutations(致病突变) will be present in the data, and the proportion of variation that can be cap- tured by the sequence data is expected to approach h2. </p></blockquote><p>随着全基因组测序技术的进步，数据中会出现致病突变，序列数据可以覆盖的变异比例有望接近$h_2$。原则上，已知的罕见风险变量（known rare risk variants）（如已识别）可以与常见变量相同的方式包含在预测因子(predictor)中；累积起来，他们的贡献可能很重要。例如，在人群中频率为千分之一、相对患病风险为五分之一的罕见变异将使千分之一的人患病风险增加五倍（因此，对于患病率为1%的疾病，患病风险从1%增加到5%），但是，这种风险的增加也可以通过多个效应大小较小的常见变异的累积效应来实现。通过将罕见变异分为定义的基因类别或结合函数的先验知识，可以将罕见变异的贡献包括在预测因子中。</p><h3 id="Limitation-3-errors-in-the-estimated-effects-of-the-markers-标记物估计效果的误差"><a href="#Limitation-3-errors-in-the-estimated-effects-of-the-markers-标记物估计效果的误差" class="headerlink" title="Limitation 3: errors in the estimated effects of the markers. 标记物估计效果的误差"></a>Limitation 3: errors in the estimated effects of the markers. 标记物估计效果的误差</h3><p>局限性3：标记物估计效果的误差。单核苷酸多态性对一个性状的影响必须通过有限的样本来估计，因此这种影响是通过一定的抽样误差（sampling error）来估计的。如果只有少数几个基因座影响一个性状，就有可能相当准确地估计它们的影响，但大多数复杂性状是由大量基本未知的基因座（loci）控制的。</p><p>因此，发现阶段可能涉及数百万个SNP的全基因组范围。大多数单核苷酸多态性的真实效应很小，因此除非使用大的发现样本，否则估计这些效应的准确度很低。在随机交配群体中同时使用所有SNP的表型和预测因子之间的相关性可以表示为有效群体规模（或独立染色体片段的有效数量，它是有效群体规模的函数）、遗传力和发现样本的规模的函数,如下公式。具体而言，当发现队列的样本量增加时，SNP效应将得到更精确的估计；即使样本量较大，也很难验证罕见变异的估计或预测效应量。</p><p>$$R^2&#x3D;\frac{h_M^2}{1+\frac{M}{N_dh_M^2}(1-R^2)} $$</p><p>即SNP的表型和预测因子之间的相关性，使用所有标记的估计效应形成的数量性状的预测因子解释的表型变异的比例（$R^2$）取决于与该性状相关的独立测量的基因组变异（例如，单核苷酸多态性（SNPs））的数量（$M$），以及它们解释的总变异的比例（$h_M^2$）以及发现样本中的样本量（$N_d$）。不管性状的遗传结构如何，公式都成立，但我们注意到预测因子可能远远不是最优的。$h_M^2$通常小于家族研究估计的遗传力，有时称为SNP遗传力或芯片遗传力。当$R^2$很小时，可以忽略分母中的二次项，否则可以求解$R^2$中的二次项。图中显示，为了获得较高的$R^2$，样本量必须较大。如果标记效应大小的分布明显非正态，具有一些大效应和许多非常小或零效应，并且如果将这种分布的知识用于估计SNP效应，则可以获得更高的$R^2$。</p><p>在本文中，我们使用$R^2$作为统计数据来报告预测因子或R的有效性、表型与预测因子或准确性之间的相关性。相关性的符号对于预测值的解释很重要。在家畜中，遗传预测器已经使用了几十年（基于基因型数据可用之前的系谱数据），准确度（$R_{G，\hat{G}}$）传统上用于评估效用。$R_{G，\hat{G}}$是真实遗传值和估计遗传值之间的相关性（预测值，是所有遗传位点组合值的估计值）。因为<br>$$R_{G，\hat{G}}^2 &#x3D; \frac{R^2}{h^2}$$<br>$R_{G，\hat{G}}$统计量量化了遗传预测因子相对于最佳可能遗传预测因子的功效</p><p>对于疾病特征，Nagelkerke的$R^2$（$R_N^2$）已用于轮廓评分分析。$R_N^2$是二元（0-1）结果数据 (binary (0–1) outcome data)中的$R^2$度量，通常应用于病例对照验证样本(case–control validation samples)，其中病例比例远高于总体。或者，报告受试者-操作曲线（AUC）下的面积。AUC具有独立于验证样本中案例比例的理想特性；AUC的一个定义是，随机选择的病例的预测值高于随机选择的对照组。</p><h3 id="Limitation-4-statistical-methods-in-the-dis-covery-sample-发现样本中的统计方法"><a href="#Limitation-4-statistical-methods-in-the-dis-covery-sample-发现样本中的统计方法" class="headerlink" title="Limitation 4: statistical methods in the dis- covery sample. 发现样本中的统计方法"></a>Limitation 4: statistical methods in the dis- covery sample. 发现样本中的统计方法</h3><p>最小二乘预测法（The least squares prediction）或“轮廓评分法（profile scoring）”通常用于预测遗传风险。虽然它很容易应用，但它不具有期望的统计特性，并且任意的P值阈值用于选择预测因子中的snp。这是因为SNP效应是相关的，并且在轮廓评分法中考虑LD需要在任意阈值上选择SNP。建立单核苷酸多态性效应分布模型以及单核苷酸多态性与多个因果变异之间相关性的方法将更准确。</p><p>在人类应用中，预测因子中有时只包括全基因组的重要SNPs，但使用不太严格的阈值会产生更高的准确性，在动物和植物育种中，通常使用所有可用的SNPs。</p><h2 id="Pitfalls-of-the-analysis"><a href="#Pitfalls-of-the-analysis" class="headerlink" title="Pitfalls of the analysis"></a>Pitfalls of the analysis</h2><h3 id="Pitfall-1-validation-and-discovery-sample-overlap-验证和发现样本重叠"><a href="#Pitfall-1-validation-and-discovery-sample-overlap-验证和发现样本重叠" class="headerlink" title="Pitfall 1: validation and discovery sample overlap 验证和发现样本重叠"></a>Pitfall 1: validation and discovery sample overlap 验证和发现样本重叠</h3><p>如果表型和群体中单个 SNP 之间的相关性 (R) 为零（即，如果 SNP 与性状无关），则平方相关性 ($R^2$) 的预期值估计为大小为 N 的样本为 1 &#x2F; (N–1) 或如果 N 很大，则约为 1 &#x2F; N。因此，随机选择的“候选”（但不是真正相关的）SNP 解释了任何样本中 1 &#x2F; N 的变异。通常，1&#x2F;N 小到不必担心。然而，与感兴趣的表型无关的一组 m 个不相关的 SNP，当装配在一起时，可以解释变异的 m&#x2F;N（由于总和他们的影响）。例如，当在 $N_d$ &#x3D; 1,000 的发现样本中进行回归分析时，一组 100 个独立的 SNP 将平均解释发现样本中 10% ($R^2 &#x3D; 0.10$) 的表型变异没有真正关联的假设。 </p><p>当预测器（predictor）中的 SNP 数量很大而样本量很小时，偶然发现 $R^2$ 可能非常高，并且在应用于独立样本时可能严重高估了由预测变量解释的真实方差。此外，验证样本中的预期$R^2$ 为 $ \sim 1 &#x2F; N_v$，其中 $N_v$ 是验证样本大小，对于从发现样本中选择的一组SNP，但在验证样本中重新估计了SNP的影响大小。因此，为了估计新样本中预测的$R^2$ ，需要在发现样本中估计一个预测方程，并在验证样本（BOX 2）中进行测试，而无需重新估计回归系数。应用不正确的验证程序会导致高估预测的准确性（或过度拟合）。发生过拟合的一个例子是在发现样本中测试预测时：即，使用相同的数据来估计 SNP 对表型的影响并进行预测。我们通过奶牛、黑腹果蝇和人类种群的例子证明了重叠陷阱（图 2a-c）。例如，在对 Dmelanogaster的约 150 个已测序近交系进行的 GWAS 中，作者得出结论，从 &gt;100 万个 SNP 中选出的 6-10 个 SNP 一起解释了系中 51-72% 的变异（取决于性状分析）。然而，对相同数据使用所有遗传标记(all genetic markers )的交叉验证贝叶斯预测分析发现，预测因子只能解释 6% 的表型变异。</p><p>(A less obvious mistake)一个不太明显的错误是选择整个样本中最显着相关的 SNP，并使用这些来估计 SNP 效应并测试它们在发现和验证集中的预测准确性。在这种情况下，当应用于验证样本时由 SNP 解释的方差被夸大了。它会产生偏差和误导性结果，因为 SNP 的初始选择步骤是基于这些 SNP 与整个样本之间以及 SNP 与任何子样本之间的偶然相关性。基于这些 SNP 的预测方程似乎适用于验证样本，但不适用于真正独立的样本。从整个样本中选择初始 SNP 集后的交叉验证分析不会减轻这种偏差。从发现和验证样本中选择 SNP 的陷阱发生在最近的一项研究中，该研究报告了自闭症的遗传预测因素。根据整个数据集中 GWAS 的 P 值，选择在多种生物学途径中与自闭症相关的 SNP。随后使用交叉验证应用模型选择以缩小 SNP 的数量。作者确实跟进了一个独立的验证样本，并且预测准确性降低了。</p><p>这种陷阱的一种变化是，验证样本中的一部分个体也在发现样本中；然后，偏差与发现集中的有效样本比例成正比，在实践中，可能很难确定是否有任何验证个体也在发现集中，特别是如果只有摘要统计数据可用（即SNP效应和等位基因频率的估计和标准误差），尤其是从公共数据库。当验证样本包括在“发现”步骤中时，我们使用牛数据证明SNP预测因子解释的方差膨胀，如图2，随着样本增加，方差下降。<br><img src= "/img/loading.gif" data-lazy-src="/2021/11/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Pitfalls-of-predicting-complex-traits-from-SNPs/2.png" alt="图2"></p><p>解决这个陷阱的方法是使用外部验证。在某些情况下，独立的数据集不可用，因此内部交叉验证是唯一的选择。在交叉验证中，重要的是要避免在验证样本结果的基础上更新预测值的陷阱，从而失去该策略计划实现的发现和验证样本的独立性</p><h3 id="Pitfall-2-the-validation-sample-验证样本"><a href="#Pitfall-2-the-validation-sample-验证样本" class="headerlink" title="Pitfall 2: the validation sample. 验证样本"></a>Pitfall 2: the validation sample. 验证样本</h3><p>如果验证样本与发现总体的关系比与目标总体的关系更密切，则预测精度将被高估。在人类中，来自Framingham心脏研究（FHS；仅限原始和后代队列）的5117名个体的身高多基因预测分析报告，在分析中包括所有个体时，使用十倍交叉验证的预测$R^2$为0.25 。然而，由于FHS包括许多相关个体，作者通过将十倍交叉验证样本限制在数据集中没有已知近亲（即父母-子女、兄弟姐妹或半兄弟姐妹）的个体，在系谱信息的基础上重复分析。在此限制性分析中，预测$R^2$降至0.15。我们警告说，即使已知的近亲被排除在外，神秘的关联性仍然可以提高预测的准确性。为了证明这一点，我们使用FHS共享数据65中的7434名个体进行了多基因身高预测分析（框3）。我们的研究结果表明，除了从谱系推断的密切关系外，隐性关联性可以提高预测准确度，相对于独立验证样本的预测准确度。</p><p>这里描述的陷阱的补救方法是使用传统上不相关的个体（在发现和验证阶段）。亲缘关系可以从SNP数据中估计，因此可以根据观察数据排除近亲。更一般地说，验证总体应代表最终应用预测值的总体。在有效种群规模较小的种群中，如某些家畜品种，所有个体都是相关的。这不会使预测失效，但它确实意味着，当预测方程应用于与发现群体关系不太密切的另一个群体时，无法预期相同的预测精度。有时，验证群体与应用程序（目标）不同因为它的遗传多样性更大。例如，验证（以及可能的发现）群体可能包括一组不同的动物或植物系。预测方程可能适用于该群体，但不适用于多样性较小的应用群体，如作物的商业品系。</p><p>当发现和验证样本相互独立时，当从发现样本中选择m个单核苷酸多态性（SNP）时，验证样本中的简单线性预测因子为:<br>$$ \hat{y}&#x3D;\sum_{i&#x3D;1}^m \hat{b_i}x_i$$</p><p>其中$x_i$&#x3D;0，1或2个SNP参考等位基因，$b_i$ 是发现样本的估计效应大小。</p><p>给定一个多SNP预测因子（ŷ），验证步骤是量化预测因子ŷ对性状y变异的解释程度。ŷ上的y回归只适合单个协变量(covariate)，因此随机预期的R2仅为$1&#x2F;N_v$，其中$N_v$为验证样本量。如果验证样本来自与发现样本相同的人群，则$R^2&gt;1&#x2F;N_V$值是预测因子实际预测能力的证据。因此，验证阶段的样本量不必太大，就可以拒绝无关联H0的零假设：$ρ^2&#x3D;0$，其中$ρ^2$是总体中$R^2$的真实值。R的标准误差（standard error,SE）约为:<br>$$ 1&#x2F;\sqrt{N_v}$$<br>如果ρ非常小，更一般地说:<br>$$ 1-ρ^2&#x2F;\sqrt{N_v}$$<br>就$R^2$而言，其SE约为<br>$$\sqrt{2&#x2F;N_v}$$</p><h3 id="Pitfall-3-population-stratification-similarity-人口分层相似"><a href="#Pitfall-3-population-stratification-similarity-人口分层相似" class="headerlink" title="Pitfall 3: population stratification similarity. 人口分层相似"></a>Pitfall 3: population stratification similarity. 人口分层相似</h3><p>另一种提高预测准确性的方法是，如果发现和验证样本包含相似的人口分层模式，而且最终目标人口没有相似的分层模式。例如，如果发现和验证样本是从分层人群（如欧洲裔美国人）中独立抽样的，则可能发生这种情况。通胀是否应该被视为陷阱的问题取决于分析的最终目标。如果目标是在欧洲裔美国人中进行预测，尽可能充分地利用祖先信息是完全合适的，而这种膨胀（inflation ）不是陷阱（因为发现、验证和目标样本是类似的分层）。然而，如果目标是评估使用结构较少的应用人群可以实现的预测准确性，那么这种膨胀就是一个陷阱。作为一个例子，我们表明，在FHS分析中，人口分层提高了预测精度（详见方框3）。更严重的问题是，在发现和验证病例对照样本中，祖先和疾病状态之间存在混淆，因为这种虚假关联可能导致祖先预测，而不是疾病预测。最近有人提出，上述预测孤独症的人也有这种缺陷。</p><p>解决与人口分层有关的问题的一个切实可行的办法是在发现样本的分析中拟合祖先的主要成分。我们注意到，如果发现和验证样本表现出与使用十倍交叉验证时相同的差异偏差，病例组和对照组之间的差异偏差也会导致虚假预测$R^2$。差异偏差的补救措施是进行严格的质量控制或在完全独立的样本中验证预测值，而不是十倍交叉验证。可以完成的一个质量控制步骤是使用预测因子中的基因型SNP，并量化应用样本与发现和验证样本之间的估计相关性：例如，在主成分分析（PCA）或相关方法中。如果应用程序样本是PCA上的异常值，则目标中的预测精度可能低于验证程序的预期。</p><h3 id="Pitfall-4-expectation-of-equality-of-R-2-and-h-M-2-期望-R-2-和-h-M-2-相等"><a href="#Pitfall-4-expectation-of-equality-of-R-2-and-h-M-2-期望-R-2-和-h-M-2-相等" class="headerlink" title="Pitfall 4: expectation of equality of $R^2$ and $h_M^2$ 期望$R^2$和$h_M^2$相等"></a>Pitfall 4: expectation of equality of $R^2$ and $h_M^2$ 期望$R^2$和$h_M^2$相等</h3><p>有时被称为SNP或芯片遗传力，通过将成对个体之间的表型相似性与其基于SNP的基因型相似性相关联，可以实现对标记h2M解释的方差的无偏估计。在人类群体中，SNP遗传力在迄今为止所研究性状的总遗传力的三分之一到一半之间。仅当单个SNP效应估计无误时，基于同一组SNP的表型预测才能达到$R^2&#x3D;h_M^2$ 。例如，当使用“轮廓评分”方法的多SNP预测因子用于身高65时，样本外预测的R2为0.1–0.15。然而，Yang等人26估计，如果对所有SNP的影响进行无误的估计，那么所有SNP加起来可以解释40-50%的表型变异。</p><p>随着样本量越来越大，SNP效应估计中误差项的大小将减小，两个统计量将收敛到相同的值。然而，对人类群体的模拟表明，随着样本量的增加，性状预测的改进取决于性状的遗传结构，特别是有多少变异的效应大小很小，而对于大多数常见的复杂遗传病来说，即使普通SNP在遗传力中占很大比例，这种改善也将是缓慢而温和的17。因此，要在人群中实现有意义和准确的预测，大数据是关键，需要数十万个样本。这样的数据集开始成为可能。</p><h3 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h3><blockquote><p>遗传标记（英语：genetic marker）是已知在染色体上位置的一种基因或DNA序列，可被用于鉴定生物个体或物种。其可被描述为可观测变异（可因位于基因组基因座中的突变或改变而产生）。遗传标记可能是一段短的DNA序列，例如单个碱基对改变的周围序列（单核苷酸多态性，SNPs）或一段长序列，例如小卫星序列。SNPs是genetic marker的一种。</p></blockquote><blockquote><p>连锁不平衡(英语：linkage disequilibrium)是指分属两个或两个以上基因座位的等位基因同时出现在一条染色体上的几率，高于随机出现的频率。</p></blockquote><blockquote><p>有三种突变，一种基因突变会直接导致表型变化；第二种会伴随着表型变化，但是由linkage disequilibrium (LD)导致的，不是直接诱因；第三种与某种表型变化无关;第一种即casual variants 与表现型变化有因果关系的突变</p></blockquote><blockquote><p>遗传力：遗传力又称遗传率，指遗传方差在总方差（表型方差）中所占的比值，可以作为杂种后代进行选择的一个指标。遗传力分为广义遗传力和狭义遗传力。数量性状受到环境因素的影响很大，那么表型的变异可能有遗传的因素，也有环境的因素，甚至还有环境和遗传相互作用的因素。广义遗传力$H^2$定义为遗传方差$σ_G^2$占表型方差$σ_P^2$的比例。广义遗传力(heritability in the broad sense)是群体中所有遗传因素产生的方差占表型方差的比例，狭义遗传力(heritability in the narrow sense)特指育种值产生的加性遗传方差占表型方差的比例。加性基因是指的等位基因和非等位基因的加性作用所引起的变异值</p></blockquote><blockquote><p>$R^2$：$R^2$的名字是coefficient of determination，另一个名字是Nash–Sutcliffe model efficiency coefficient。给定一系列真值$y_i$和对应的预测值 $\hat{y_i} $，$R^2$的定义为<br>$$R^2&#x3D;1-\frac{\sum_i(\hat{y_i}-y_i)^2}{(y_i-\bar{y})^2}$$<br>$R^2$的含义是，预测值解释了$y_i$ 变量的方差的多大比例，衡量的是预测值对于真值的拟合好坏程度。</p><blockquote><ul><li>R方&#x3D;1：最理想情况，所有的预测值等于真值。</li><li>R方&#x3D;0：一种可能情况是”简单预测所有y值等于y平均值”，即所有$\hat{y_i}$都等于$\bar{y}$(即真实y值的平均数），但也有其他可能。</li><li>R方&lt;0：模型预测能力差，比”简单预测所有y值等于y平均值”的效果还差。这表示可能用了错误模型，或者模型假设不合理。</li><li>R方的最小值没有下限，因为预测可以任意程度的差。因此，R方的范围是 $(-\infty ,1]$。</li><li>注意：R方并不是某个数的平方，因此可以是负值。</li></ul></blockquote></blockquote><blockquote><p>线性模型可以理解为“以最小化R方为目标，寻找y和x之间的最优线性关系</p></blockquote><div id="refer-anchor"></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]<a href="https://www.nature.com/articles/nrg3457">Wray, Naomi R., et al. “Pitfalls of predicting complex traits from SNPs.” Nature Reviews Genetics 14.7 (2013): 507-515.</a></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> 生物信息 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生物信息 </tag>
            
            <tag> PRS </tag>
            
            <tag> Papers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生存模型评价指标</title>
      <link href="/2021/10/28/%E7%94%9F%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
      <url>/2021/10/28/%E7%94%9F%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h3 id="生存分析常见术语："><a href="#生存分析常见术语：" class="headerlink" title="生存分析常见术语："></a>生存分析常见术语：</h3><p>删失（Censoring）：删失是指在对样本进行观测或调查时，某个个体的确切生存时间未知，只知道其生存时间位于某个区间内。删失可分为右删失，左删失和区间删失。</p><p>右删失：（right censoring) 当删失时间C＞生存时间X时，绝大多数删失为右删失。</p><p>左删失：（left censoring)当个体的生存时间X＜删失时间C，称为左删失。</p><p>区间删失：（interval censoring） 个体的生存时间只能确定在某一区间之间，但不知道确切时间  （L，R）L代表左观测时间，R代表右观测时间。</p><p>截尾（Truncation)：截尾数据指只有个体的生存时间处于某区特定区间才能被观测到，在区间之外的个体不能被观测到。</p><p>打结(tied) :当不同个体拥有相同的失效时间，称之为打结。例如，i,j为两个不同的个体（i≠j）当Ti&#x3D;Tj时，发生打结</p><h3 id="生存模型风险预测效果评价指标"><a href="#生存模型风险预测效果评价指标" class="headerlink" title="生存模型风险预测效果评价指标"></a>生存模型风险预测效果评价指标</h3><p>一个生存模型的建立需要经过描述，模型拟合，和预测几个阶段。当我们根据病人的生存数据拟合构建了生存模型后，需要评估模型的预测能力，考察它将研究对象正确的划分为病人和非病人的能力。模型预测能力是评价模型的优劣性的重要指标。</p><ul><li><p>Harrell’s c 指标、Somers’D 指标</p></li><li><p>ROC曲线法、时依ROC曲线法</p></li><li><p>IDI、NRI法</p></li></ul><p>一、Harrell’s c 指标、Somers’D 指标</p><p>Harrell’s c 指标</p><p>Harrell’s c 指标有时也称为C-index，它反映的是模型预测结果与实际情况的一致程度，即为：在一次生存模型分析中随机挑选的两个病人，模型所预测的生存时间更长的那个病人，他的实际生存时间也更长的概率。由于估算出预测的生存时间较复杂，Harrell等人指出，实际运用中，模型的预测生存时间与预测生存概率函数成对应关系，计算Harrell’s c 指标时可以用模型的预测生存概率函数来代替预测生存时间[1]。</p><p>详解：假设一次生存分析中共有n个病人，病人的实际生存时间用Yi来表示（Y1,Y2, …Yn），利用病人的生存数据拟合生存模型后，模型预测的生存时间用Ti来表示（T1,T2, …Tn)，预测的生存概率函数用Xi来表示（X1, X2, …Xn）。接下来，计算Harrell’s c 指标，对模型的预测能力进行评估。原理：将n个病人之间两两随机配对，对于一个配对，共获得四个观测值（Xi,Yi,Xj,Yj），i≠j。</p><p>对子（pair)：在生存分析中，随机挑选两个被试，i、j，这两个被试的实际生存时间Y与建立的生存模型预测的生存概率X组成一个对子（Xi, Yi, Xj, Yj）。在一次共有n个被试的生存分析中，所有被试一共可以组成 nx(n-1) 个对子（若考虑 i, j 和 j, i 的重复情况，也可认为共组成 nx(n-1)&#x2F;2 个对子）。</p><p>根据对子的数据特征，可进一步的区分不同类型的对子</p><p>**可用对（usable pair）：</p><p>**两个实际生存时间不一致的被试（Yi≠Yj）组成的对子称为可用对。（pencina 等人对可用对的范围进行了更严格的划分，两个病人的实际生存时间与预测的生存概率都不一致时（Yi≠Yj、Xi≠Xj），才被选为可用对。[1] [2]）</p><p>一致对（concordant pair）：在后文我们用P表示，如果一个配对中两个被试实际生存时间的趋势与模型预测的生存概率趋势一致（即当Yi&gt;Yj时，Xi&gt;Xj, 或 Yi＜Yj时，Xi＜Xj），这个配对为一致对。</p><p>非一致对 （discordant pair）:在后文我们用Q表示。如果一个配对中两个被试实际生存时间的趋势与模型预测的生存概率趋势不一致（即当Yi&gt;Yj时，Xi＜Xj，或 Yi＜Yj时，Xi＞Xj），这个配对为不一致对。</p><p>不确定对（unsure pair） ：两个被试实际生存时间不同，而模型预测的生存概率相同时，该配对为不确定对。Yi&#x3D;Yj, Xi≠Xj的配对，在后文我们用T表示。（在pencina 等人后续的定义中，unsure pair ，也属于不可用对。）</p><p>公式：C&#x3D;一致对对数数&#x2F;可用对总对数</p><p>在 Harrell 等人的定义中，可用对总对数&#x3D;P+Q+T，实际生存时间（Yi≠Yj）不一致的配对即为可用对[3]，而在pencina等人后续的定义中，可用对总对数&#x3D;P+Q，不确定对（Xi≠Xj, Yi&#x3D;Yj的配对）为不可用对[2]。</p><p>Somers’D 指标</p><p>其概念与Harrell’c 有相似之处，由 1962年 Somers 提出[4]。</p><p>公式：Somers’D(X|Y) &#x3D;（ 一致对对数-不一致对对数）&#x2F;可用对总数</p><p>有人认为，Somers’D 指标与Harrell’s C指标之间存在关系，C&#x3D;(D+1)&#x2F;2，也因此认为，当D&#x3D;(P-Q)&#x2F;(P+Q+T)时，   C&#x3D;(P+0.5T)&#x2F;(P+Q+T)</p>]]></content>
      
      
      <categories>
          
          <category> 生信123 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生信 </tag>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Numeral Calculations</title>
      <link href="/2021/05/11/Numeral-Calculations/"/>
      <url>/2021/05/11/Numeral-Calculations/</url>
      
        <content type="html"><![CDATA[<p>数值计算通常是指通过迭代过程更新解的估计值来解决数学问题的算法，而不是通过解析过程推导出公式来提供正确解的方法。常见的操作包括优化（找到最小化或最大化函数值的参数）和线性方程组的求解。</p><h3 id="上溢-overflow-和下溢-underflow"><a href="#上溢-overflow-和下溢-underflow" class="headerlink" title="上溢(overflow)和下溢(underflow)"></a>上溢(overflow)和下溢(underflow)</h3><p>上溢：当大量级的数被近似为 $\infty$或$-\infty$时发生上溢；下溢：当接近零的数四舍五入为零时发生下溢。</p><p>对上溢和下溢进行数值稳定的例子是softmax函数。<br>$$softmax(x)_i &#x3D; \frac{exp(x)<em>i}{\sum</em>{j&#x3D;1}^n exp(x_j)}$$</p><h3 id="病态条件"><a href="#病态条件" class="headerlink" title="病态条件"></a>病态条件</h3><p>条件数指 函数相对于输入的微小变化而变化的快慢程度。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How to write Enpaper</title>
      <link href="/2021/03/12/How-to-write-Enpaper/"/>
      <url>/2021/03/12/How-to-write-Enpaper/</url>
      
        <content type="html"><![CDATA[<h2 id="Traffic-Flow-Prediction-via-Spatial-Temporal-Graph-Neural"><a href="#Traffic-Flow-Prediction-via-Spatial-Temporal-Graph-Neural" class="headerlink" title="Traffic Flow Prediction via Spatial Temporal Graph Neural"></a>Traffic Flow Prediction via Spatial Temporal Graph Neural</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>首先简要阐述研究问题，和先存在模型所存在的问题，then:</p><p>In this paper, we propose a novel spatial temporal graph neural network for traffic flow prediction, which can comprehensively capture spatial and temporal patterns. In particular, the framework offers a learnable postional attention mechanism to effectively aggregate information from adjacent roads.Meanwhile, it provides a sequential component to model the traffic flow dynamics which can exploit both local and global temporal dependecies. </p><p>阐述实验结果：Experimental results on various real traffic dataset demonstrate the effectiveness of the proposed framework.</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h3><p>The first paragraph briefly describes the research content, which is composed of content, signigicance, history and difficuties.</p><p>The second paragraph briefly introduce all existing methods and models.</p><p>The following paragraph, we will give a deailed introduction to the model which similar to our proposed, and point out the existing problems and challenges.</p><p>(Then introduce our proposed model and list our innovation points separately.)To address the aforementioned challenges, in this paper, we propoese a novel spatial temporal graph neural layer, recurrent complex relations betwwen roads from both spatial and temporal aspects. In summary, the main contributions of this paper are as follows:</p><ul><li>We propose a new Graph Neural Network layer with a position-wise attention mechanism to better aggregate information of traffic flows from adjacent roads;</li><li>We combine a recurrent network and a Transformer layer to capture the local and global temporal dependence;</li><li>We propose a new spatial temporal GNN framework STGNN which is particularly designed for modeling series data with complex topological and temporal dependency; and</li><li>We validate the feasibility and advantages of the proposed framework on real traffic datasets, especially for the short period traffic speed prediction in terms of minutes which is more challenging compared to longer period prediction in terms of days or weeks. Experiments show that our model outperforms state-of-the-art methods significantly</li></ul><p>(The last paragraph , introduction to the paper structure.) The rest of the paper is organized as follows. In Section 2. we introduce… In section 3…In section 5, we review related work. We conclude the work in Section 6 with future work.</p><h3 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2.RELATED WORK"></a>2.RELATED WORK</h3><h3 id="3-PROBLEM-STATEMENT-x2F-PRELIMINARIES"><a href="#3-PROBLEM-STATEMENT-x2F-PRELIMINARIES" class="headerlink" title="3.PROBLEM STATEMENT &#x2F; PRELIMINARIES"></a>3.PROBLEM STATEMENT &#x2F; PRELIMINARIES</h3><p>Abstract the research content into mathematical language and symbolize the process.</p><h3 id="4-MODELS-用自己提出的模型命名-x2F-ThE-PROPOSED-FRAMEWORK"><a href="#4-MODELS-用自己提出的模型命名-x2F-ThE-PROPOSED-FRAMEWORK" class="headerlink" title="4.MODELS(用自己提出的模型命名) &#x2F;(ThE PROPOSED FRAMEWORK)"></a>4.MODELS(用自己提出的模型命名) &#x2F;(ThE PROPOSED FRAMEWORK)</h3><p>先总体简述下模型的组成。然后再小标题分节介绍。<br>The proposed spatial temporal graph neural network framework is shown in Figure1. It mainly consists of three components: 1) the spatial gtaph neural network(S-GNN)layers, which aim to capture the spatial relations between the roads through the traffic network; 2) the GRU layer, which is to capture the temporal relation sequentially (or local temporal dependency); and 3) the transformer layer, which aims to directly capture the long-range temporal dependence in the sequence (or global temporal dependence)<a href="#refer-anchor"><sup>1<sup></sup></sup></a>.</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/03/12/How-to-write-Enpaper/fig1.png" alt="fig1"></p><h3 id="4-1-Modeling-the-Spatial-Dependecy"><a href="#4-1-Modeling-the-Spatial-Dependecy" class="headerlink" title="4.1 Modeling the Spatial Dependecy"></a>4.1 Modeling the Spatial Dependecy</h3><h3 id="4-2-Modeling-the-Temporal-Dependency"><a href="#4-2-Modeling-the-Temporal-Dependency" class="headerlink" title="4.2 Modeling the Temporal Dependency"></a>4.2 Modeling the Temporal Dependency</h3><h3 id="5-EXPERIMENT"><a href="#5-EXPERIMENT" class="headerlink" title="5. EXPERIMENT"></a>5. EXPERIMENT</h3><h3 id="6-RELATED-WORK"><a href="#6-RELATED-WORK" class="headerlink" title="6. RELATED WORK"></a>6. RELATED WORK</h3><h3 id="7-CONCLUSION"><a href="#7-CONCLUSION" class="headerlink" title="7. CONCLUSION"></a>7. CONCLUSION</h3><h3 id="Thinking-different"><a href="#Thinking-different" class="headerlink" title="Thinking different"></a>Thinking different</h3><p>模型通过GNN捕捉spatial relations, 通过GRU捕捉temporal信息，通过Transformer Layers捕捉全局信息。</p><div id="refer-anchor"></div><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>凸优化（一）</title>
      <link href="/2021/01/26/%E5%87%B8%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2021/01/26/%E5%87%B8%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<ol><li>优化&#x2F; 数学规划 （optimization &#x2F; mathematical programming）</li></ol><p>定义：从一个可行解的集合中寻找最优的元素。</p><p>从数学上来看，任何优化问题都能写成如下形式：</p><blockquote><p>$minimize \leq f_0(x)$ <br><br>$subject \quad to: f_i(x) \leq b_i, i&#x3D;1,…,M$<br><br>$x &#x3D; [x_1, x_2,…,x_n]^T  \ \ \ Optimization \ Variable$<br><br>$f_0 :R^n -&gt; R \ \ \ objection function$ <br><br>$f_i : R^n -&gt; R \ \ \ Inequality \ \ constraints$<br><br>$X^* \ \ \ \ optimal \Longleftrightarrow \forall z, z\in {f_i(z) \leq b_i, i &#x3D;1,2,…,m} \ feasible set \quad f_i(z) \geq f_i(x^*)$</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 凸优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 凸优化 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lstm-vae调研</title>
      <link href="/2021/01/23/lstm-vae%E8%B0%83%E7%A0%94/"/>
      <url>/2021/01/23/lstm-vae%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<h2 id="1-VAE（Variational-Auto-Encoder"><a href="#1-VAE（Variational-Auto-Encoder" class="headerlink" title="1.VAE（Variational Auto-Encoder)"></a>1.VAE（Variational Auto-Encoder)</h2><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/23/lstm-vae%E8%B0%83%E7%A0%94/vae1.png" alt="vae1"><br><img src= "/img/loading.gif" data-lazy-src="/2021/01/23/lstm-vae%E8%B0%83%E7%A0%94/vae2.png" alt="vae2"></p><h3 id="Generating-Sentences-From-a-Continuous-Spaces4"><a href="#Generating-Sentences-From-a-Continuous-Spaces4" class="headerlink" title="Generating Sentences From a Continuous Spaces4"></a>Generating Sentences From a Continuous Spaces<a href="#refer-anchor"><sup>4</sup></a></h3><p>这篇文章对后面很多 paper 影响很大而且我也很喜欢，所以重点介绍一下。paper 最早发表在 ICLR 2016 上，motivation 在于作者为了弥补传统的 RNNLM 结构缺少的一些 global feature（其实可以理解为想要 sentence representation）。其实抛开 generative model，之前也有一些比较成功的 non-generative 的方法，比如 sequence autoencoders[1]，skip-thought[2]和 paragraph vector[3]。但随着 VAE 的加入，generative model 也开始在文本上有更多的可能性。</p><p>Loss 的组成还是和 VAE 一样。具体模型上，encoder 和 decoder 都采用单层的 LSTM，decoder 可以看做是特殊的 RNNLM，其 initial state 是这个 hidden code z（latent variable），z 采样自 Gaussian 分布 G，G 的参数由 encoder 后面加的一层 linear layer 得到。这里的 z 就是作者想要的 global latent sentence representation，被赋予了先验 diagonal Gaussians，同时 G 就是学到的后验。</p><p>模型很简单，但实际训练时有一个很严重的问题：KL 会迅速降到 0，后验失效了。原因在于，由于 RNN-based 的 decoder 有着非常强的 modeling power，直接导致即使依赖很少的 history 信息也可以让 reconstruction errors 降得很低，换句话说，decoder 不依赖 encoder 提供的这个 z 了，模型等同于退化成 RNNLM（摊手）。</p><p>这篇 paper 提出的解决方法：KL cost annealing 和 Word dropout。</p><ul><li>KL cost annealing</li></ul><p>作者引入一个权重 w 来控制这个 KL 项，并让 w 从 0 开始随着训练逐渐慢慢增大。作者的意思是一开始让模型学会 encode 更多信息到 z 里，然后随着 w 增大再 smooth encodings。其实从工程&#x2F;代码的角度看，因为 KL 这项更容易降低，模型会优先去优化 KL，于是 KL 很快就降成 0。但如果我们乘以一开始很小的 w，模型就会选择忽视 KL（这项整体很小不用降低了），选择优先去降低 reconstruction errors。当 w 慢慢增大，模型也慢慢开始关注降低 KL 这项了。这个技巧在调参中其实也非常实用。</p><ul><li>Word dropout</li></ul><p>既然问题是 RNN-based 的 decoder 能力太强，那我们就来弱化它好了。具体方法是把 input 的词替换成 UNK（我可能是个假的 decoder），模型被迫只能去多多依赖z。当然保留多少 input 也需要尝试，我们把全都不保留的叫做 inputless decoder，实验表明，inputless VAE 比起 inputless RNN language model 不知道好到哪里去了。</p><p>受到 GAN 的启发，作者还提出了一个 Adversarial evaluation，用一半真一半假的数据作为样本训练出一个分类器，再对比不同模型生成的句子有多少能骗过这个分类器，这个 evaluation 被用在 Imputing missing words 这个任务上，VAE 的表现同样比 RNNLM 出色。</p><p>最后，作者展示模型的确学到了平滑的 sentence representation。选取两个 sentence 的 code z1 和 z2，z1 和 z2 可以看做向量空间的两个点，这两个点连线之间的点对应的句子也都符合语法且 high-level 的信息也保持局部一致。</p><h2 id="2-CNN-Vae"><a href="#2-CNN-Vae" class="headerlink" title="2. CNN-Vae"></a>2. CNN-Vae</h2><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/23/lstm-vae%E8%B0%83%E7%A0%94/image.png" alt="vae-cnn"></p><h2 id="3-LSTM-Vae"><a href="#3-LSTM-Vae" class="headerlink" title="3.LSTM-Vae"></a>3.LSTM-Vae</h2><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/23/lstm-vae%E8%B0%83%E7%A0%94/LSTM1.png" alt="vae3"></p><p>上图原理可以简单理解为当数据输入时，先由VAE的编码器网络对输入数据进行压缩，并做特征提取，将提取到的特征输入LSTM网络进行故障检测或分类，并对特征进行归类预测，将预测得到的结果输入VAE解码器网络，进行重构，并计算重构损失，更新整体网络参数。VAE与LSTM二者结合，进一步提高模型诊断精度。（详细原理阐述可参见论文原文<a href="#refer-anchor"><sup>1</sup></a>)</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/23/lstm-vae%E8%B0%83%E7%A0%94/LSTM2.png" alt="vae4"></p><div id="refer-anchor"></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="https://spaces.ac.cn/archives/5253">苏剑林. (Mar. 18, 2018). 《变分自编码器（一）：原来是这么一回事 》</a> </p><p>[2] ANOMALY DETECTION FOR TIME SERIES USING VAE-LSTM HYBRID MODEL, 2020, ICASSP, CCFB</p><p>[3] A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-Based Variational Autoencoder</p><p>[4] Generating Sentences from a Continuous Space, CoNLL, 2016, CCF-C</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> vae </tag>
            
            <tag> 生成模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Householder flow</title>
      <link href="/2021/01/21/Householder-flow/"/>
      <url>/2021/01/21/Householder-flow/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Isotropic-Distributions1"><a href="#1-Isotropic-Distributions1" class="headerlink" title="1. Isotropic Distributions1"></a>1. Isotropic Distributions<a href="#refer-anchor"><sup>1</sup></a></h2><blockquote><p>An isotropic distribution is a special type of multidimensional distribution which possesses some of the properties of the spherical multivariate normal distribution. These distributions, often called spherically symmetric distributions, are characterized by their invariance under orthogonal transformation. </p></blockquote><p>各向同性分布是一种特殊的多维分布，它具有球形多元正态分布的某些特性。 这些分布通常称为球对称分布，其特征在于它们在正交变换下的不变性。 </p><p>Defination: The random vector $X$ (and its distribution) is isotropic if for all $p × p$ orthogonal matrices $Ŵ, X$ and $ŴX$ have the same distribution.<a href="#refer-anchor"><sup>1</sup></a></p><h2 id="2-Householder-Flow"><a href="#2-Householder-Flow" class="headerlink" title="2. Householder Flow"></a>2. Householder Flow</h2><h3 id="2-1-Normalizing-Flows2"><a href="#2-1-Normalizing-Flows2" class="headerlink" title="2.1 Normalizing Flows2"></a>2.1 Normalizing Flows<a href="#refer-anchor"><sup>2</sup></a></h3><p>Normalizing Flows (NF) are a type of generative model with several attractive properties like exact likelihood, fast sampling and substantial memory savings.They are capable of representing complicated distributions by transforming a simple base distribution $z \sim P_z$ through an invertible neural network $f$ to attain a model distribution $f(z) \sim P_{model}.$ The exact likelihood $p_{model}(x)$ can then be computed through a change of variable formula due to the invertibility of $f$.</p><p>However, the change of variables formula also require the computation of the Jacobian determinant of f. Previous research thus attempt to design invertible neural networks which also allow efficient computation of their Jacobian determinant. </p><p>This makes orthogonal matrices very attractive, since they are easy to invert $U^{−1} &#x3D; U^T$ and have unit Jacobian determinant $|det(∂Ux&#x2F;∂x)| &#x3D; 1.$</p><p>However, the use of orthogonal matrices introduce one complication.One needs to perform gradient descent wrt. weight matrices that are constrained to be orthogonal(梯度下降权重矩阵需要被限制为正交矩阵). Previous work solve this issue using different methods which can roughly be divided into three groups: matrix exponential, Cayley transform and Householder matrices.其中Householderer方法具有最佳的时间复杂度。</p><h3 id="2-2-Improving-posterior-flexibility-using-Normalizing-Flows3"><a href="#2-2-Improving-posterior-flexibility-using-Normalizing-Flows3" class="headerlink" title="2.2 Improving posterior flexibility using Normalizing Flows3"></a>2.2 Improving posterior flexibility using Normalizing Flows<a href="#refer-anchor"><sup>3</sup></a></h3><p>A (finite) normalizing flow is a powerful framework for building flexible posterior distribution by starting with an initial random variable with a simple distribution for generating $z^{(0)}$ and then applying a series of invertible transformations $f^{(t)}$, for $t &#x3D; 1,…,T$. As a result, the last iterate gives a random variable $z^{(T)}$ that has a more flexible distribution. Once we choose transformations $f^{(t)}$ for which the Jacobian-determinant can be computed, we aim at optimizing the following objective:</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/21/Householder-flow/gs1.png" alt="(1)"></p><p>In fact, the normalizing flow can be used to enrich the posterior of the VAE with small or even none modifications in the architecture of the encoder and the decoder.<a href="#refer-anchor"><sup>3</sup></a></p><h3 id="2-3-Householder-flow3"><a href="#2-3-Householder-flow3" class="headerlink" title="2.3 Householder flow3"></a>2.3 Householder flow<a href="#refer-anchor"><sup>3</sup></a></h3><p>The Householder transformation is defined as follows. For a given vector $z^{(t−1)}$ the reflection hyperplane can be defined by a vector (a Householder vector) $v_t \in R^M$ that is orthogonal to the hyperplane, and the reflection of this point about the hyperplane is:</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/21/Householder-flow/gs2.png" alt="(2)"></p><p>Householder transformation：一种线性变换，这一变换将一个向量变化为由一个超平面反射的镜像。householder变换可以讲某些元素置0，同时保持该向量的范数不变。</p><p>The most important property of $H_t$ is that it is an orthogonal matrix and hence the absolute value of the Jacobian determinant is equal 1. This fact significantly simplifies the objective (1) because $ln |det \frac{\partial H_t z^{(t-1)}}{\partial z^{(t-1)}}|&#x3D;0$, for $t &#x3D; 1,…, T$. Starting from a simple posterior with the diagonal covariance matrix for $z^{(0)}$, the series of $T$ linear transformations given by (2) defines a new type of volume-preserving flow that we refer to as the Householder flow (HF). The vectors $v_t , t &#x3D; 1, . . . , T$, are produced by the encoder network along with means and variances using a linear layer with the input $v_{t−1}$, where $v_0 &#x3D; h$ is the last hidden layer of the encoder network. The idea of the Householder flow is schematically presented in Figure 1. Once the encoder returns the first Householder vector, the Householder flow requires $T$ linear operations to produce a sample from a more flexible posterior with an approximate full-covariance matrix.<a href="#refer-anchor"><sup>3</sup></a></p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/21/Householder-flow/pic1.png" alt="(1)"></p><h2 id="3-Neural-variational-Correlated-Topic-Modeling4"><a href="#3-Neural-variational-Correlated-Topic-Modeling4" class="headerlink" title="3.Neural variational Correlated Topic Modeling4"></a>3.Neural variational Correlated Topic Modeling<a href="#refer-anchor"><sup>4</sup></a></h2><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/21/Householder-flow/fig2.png" alt="(2)"><br><img src= "/img/loading.gif" data-lazy-src="/2021/01/21/Householder-flow/fig1.png" alt="(3)"></p><p>论文研究的问题：为了降低计算复杂度，topics are always modeled as isotopic Gaussian distributions. Isotopic Gaussian 的协方差矩阵是对角矩阵，因此主题之间是不相关的。</p><p>使用Normalizing flows来解决这个问题，即最终采用Hoseholder flow，通过Householder flow, 生成全协方差矩阵以及更复杂的后验.</p><h2 id="4-Topic-Guided-Variational-Autoencoders-for-Text-Generation5"><a href="#4-Topic-Guided-Variational-Autoencoders-for-Text-Generation5" class="headerlink" title="4.Topic-Guided Variational Autoencoders for Text Generation5"></a>4.Topic-Guided Variational Autoencoders for Text Generation<a href="#refer-anchor"><sup>5</sup></a></h2><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/21/Householder-flow/fig4.png" alt="(4)"></p><p>论文研究的问题：<br>a. 现存的基于vae的文本生成模型，没有考虑充足的信息，比如主题，语义信息。<br>b.基于vae的文本生成模型存在 “posterior collapse” 缺陷</p><p>posterior collapse：KL＝０，得到无效后验。（LSTM会加剧这种现象）</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/21/Householder-flow/pic3.png" alt="(5)"></p><p>posterior collapse解决措施：弱化decoder<a href="#refer-anchor"><sup>6</sup></a>, 但是，弱化decoder, 会降低 生成连续句子的质量</p><p>论文提出模型：NTM + NSM；NSM中融入Householder flow</p><div id="refer-anchor"></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Isotropic Distributions. Morris L. Eaton,2006, Encyclopedia of Statistical Sciences<br>[2] Faster Orthogonal Parameterization with Householder Matrices.ICML,2020,CCF-A<br>[3] Improving Variational Auto-Encoders using Householder Flow, NIPS,2016，CCF-A<br>[4] Neural variational Correlated Topic Modeling，www, 2019<br>[5] Topic-Guided Variational Autoencoders for Text Generation,ACL,2019<br>[6] Generating Sentences from a Continuous Space, CoNLL, 2016, CCF-C</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Householder flow </tag>
            
            <tag> vae </tag>
            
            <tag> 生成模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN</title>
      <link href="/2021/01/18/GNN/"/>
      <url>/2021/01/18/GNN/</url>
      
        <content type="html"><![CDATA[<h2 id="Graph-neural-networks"><a href="#Graph-neural-networks" class="headerlink" title="Graph neural networks"></a>Graph neural networks</h2><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>曾有学者将本次人工智能深度学习热潮归结为三个方面：</p><ul><li>计算资源的快速发展（GPU）</li><li>大量训练数据的可用性</li><li>深度学习从欧氏空间数据中提取潜在特征的有效性</li></ul><p>尽管传统的深度学习方法（LSTM、CNN、VAE）被应用在提取欧氏空间数据的特征方面取得了巨大的成功，但许多实际应用场景中的数据是从非欧式空间生成的，传统的深度学习方法在处理非欧式空间数据上的表现却仍难以使人满意。例如，在电子商务中，一个基于图（Graph）的学习系统能够利用用户和产品之间的交互来做出非常准确的推荐，但图的复杂性使得现有的深度学习算法在处理时面临着巨大的挑战。这是因为图是不规则的，每个图都有一个大小可变的无序节点，图中的每个节点都有不同数量的相邻节点，导致一些重要的操作（例如卷积）在图像（Image）上很容易计算，但不再适合直接用于图。此外，现有深度学习算法的一个核心假设是数据样本之间彼此独立。然而，对于图来说，情况并非如此，图中的每个数据样本（节点）都会有边与图中其他实数据样本（节点）相关，这些信息可用于捕获实例之间的相互依赖关系。</p><p>图神经网络和图嵌入(graph embedding)或网络嵌入(network embedding)密切相关，图嵌入或网络嵌入是数据挖掘和机器学习界日益关注的另一个课题。图嵌入旨在通过保留图的网络拓扑结构和节点内容信息，将图中顶点表示为低维向量，以便使用简单的机器学习算法（例如，支持向量机分类）进行处理。许多图嵌入算法通常是无监督的算法，它们可以大致可以划分为三个类别，即矩阵分解、随机游走和深度学习方法。同时图嵌入的深度学习方法也属于图神经网络，包括基于图自动编码器的算法（如DNGR和SDNE）和无监督训练的图卷积神经网络（如GraphSage）。下图描述了图嵌入和图神经网络在本文中的区别。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/gcn_ge.png" alt="GCN VS network embedding"></p><h2 id="有哪些图神经网络？"><a href="#有哪些图神经网络？" class="headerlink" title="有哪些图神经网络？"></a>有哪些图神经网络？</h2><p>在本文中，我们将图神经网络划分为五大类别，分别是：图卷积网络（Graph Convolution Networks，GCN）、 图注意力网络（Graph Attention Networks）、图自编码器（ Graph Autoencoders）、图生成网络（ Graph Generative Networks） 和图时空网络（Graph Spatial-temporal Networks）。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89.jpg" alt="符号定义"></p><h3 id="1-图卷积网络（Graph-Convolution-Networks，GCN）"><a href="#1-图卷积网络（Graph-Convolution-Networks，GCN）" class="headerlink" title="1. 图卷积网络（Graph Convolution Networks，GCN）"></a>1. 图卷积网络（Graph Convolution Networks，GCN）</h3><p>图卷积网络将卷积运算从传统数据（例如图像）推广到图数据。其核心思想是学习一个函数映射$f(·)$，通过该映射图中的节点$x_i$可以聚合它自己的特征$x_i$与它的邻居特征$x_j (j \in N(v_i))$来生成节点$v_i$的新表示。图卷积网络是许多复杂图神经网络模型的基础，包括基于自动编码器的模型、生成模型和时空网络等。下图直观地展示了图神经网络学习节点表示的步骤。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/GCN.jpg" alt="GCN"></p><p>GCN方法又可以分为两大类，基于谱（spectral-based）和基于空间（spatial-based）。基于谱的方法从图信号处理的角度引入滤波器来定义图卷积，其中图卷积操作被解释为从图信号中去除噪声。基于空间的方法将图卷积表示为从邻域聚合特征信息，当图卷积网络的算法在节点层次运行时，图池化模块可以与图卷积层交错，将图粗化为高级子结构。如下图所示，这种架构设计可用于提取图的各级表示和执行图分类任务。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/gcn_pooling.jpg" alt="GCN2"></p><h3 id="1-1-Spectral-based-Graph-Convolutional-Networks"><a href="#1-1-Spectral-based-Graph-Convolutional-Networks" class="headerlink" title="1.1 Spectral-based Graph Convolutional Networks"></a>1.1 Spectral-based Graph Convolutional Networks</h3><p>在数字与信号处理中，我们通过引入傅立叶变换将时域信号转换到频域信号进行分析，进而我们完成一些我们在时域上无法完成的操作，基于谱的图卷积网络的核心思想正是来源于此。</p><p>在基于谱的图神经网络中，图被假定为无向图，无向图的一种鲁棒数学表示是正则化图拉普拉斯矩阵，即<br>$$L &#x3D; I_n - D^{-\frac{1}{2}}AD^{-\frac{1}{2}},$$<br>其中，A为图的邻接矩阵，D为对角矩阵且<br>$$D_{ii} &#x3D; \sum_j(A_{ij})$$<br>正则化图拉普拉斯矩阵具有实对称半正定的性质。利用这个性质，正则化拉普拉斯矩阵可以分解为<br>$$L &#x3D; U\Lambda U^T,$$<br>其中$U&#x3D;[u_0,u_1,…,u_{n-1}]\in R^{N \times N}$,$U$是由$L$的特征向量构成的矩阵, $\Lambda$是对角矩阵，对角线上的值为$L$的特征值。正则化拉普拉斯矩阵的特征向量构成了一组正交基。</p><p>在图信号处理过程中，一个图的信号$X \in R^N$是一个由图的各个节点组成的特征向量，$x_i$代表第$i$个节点。<br><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2.png" alt="傅立叶变换"><br>为了更好地理解图的傅里叶变换，从它的定义我们可以看出，它确实将输入的图信号投影到正交空间，而这个正交空间的基（base）则是由正则化图拉普拉斯的特征向量构成。</p><p>转换后得到的信号$\bar x$的元素是新空间中图信号的坐标，因此原来的输入信号可以表示为<br>$$x&#x3D;\Sigma_i\hat{x}_iu_i$$<br>这正是傅里叶反变换的结果。现在我们可以来定义对输入信号$X$的图卷积操作了。<br><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/gcn2.png" alt="gcn2"></p><p>基于谱的图卷积网络都遵循这样的模式，它们之间关键的不同点在于选择的滤波器不同。现有的基于谱的图卷积网络模型有以下这些：Spectral CNN、Chebyshev Spectral CNN (ChebNet)、Adaptive Graph Convolution Network (AGCN)基于谱的图卷积神经网络方法的一个常见缺点是，它们需要将整个图加载到内存中以执行图卷积，这在处理大型图时是不高效的。</p><h3 id="1-2-Spatial-based-Graph-Convolutional-Networks"><a href="#1-2-Spatial-based-Graph-Convolutional-Networks" class="headerlink" title="1.2 Spatial-based Graph Convolutional Networks"></a>1.2 Spatial-based Graph Convolutional Networks</h3><p>基于空间的图卷积神经网络的思想主要源自于传统卷积神经网络对图像的卷积运算，不同的是基于空间的图卷积神经网络是基于节点的空间关系来定义图卷积的。为了将图像与图关联起来，可以将图像视为图的特殊形式，每个像素代表一个节点，如下图a所示，每个像素直接连接到其附近的像素。通过一个3×3的窗口，每个节点的邻域是其周围的8个像素。这八个像素的位置表示一个节点的邻居的顺序。然后，通过对每个通道上的中心节点及其相邻节点的像素值进行加权平均，对该3×3窗口应用一个滤波器。由于相邻节点的特定顺序，可以在不同的位置共享可训练权重。同样，对于一般的图，基于空间的图卷积将中心节点表示和相邻节点表示进行聚合，以获得该节点的新表示，如图b所示。<br><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/gcn3.jpg" alt="gcn3"></p><p>一种共同的实践是将多个图卷积层叠加在一起。根据卷积层叠的不同方法，基于空间的GCN可以进一步分为两类：recurrent-based和composition-based的空间GCN。recurrent-based的方法使用相同的图卷积层来更新隐藏表示，composition-based的方法使用不同的图卷积层来更新隐藏表示。下图说明了这种差异。<br><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/gcn4.png" alt="gcn4"></p><h3 id="1-3-Comparison-Between-Spectral-and-Spatial-Models"><a href="#1-3-Comparison-Between-Spectral-and-Spatial-Models" class="headerlink" title="1.3 Comparison Between Spectral and Spatial Models"></a>1.3 Comparison Between Spectral and Spatial Models</h3><p>作为最早的图卷积网络，基于谱的模型在许多与图相关的分析任务中取得了令人印象深刻的结果。这些模型在图信号处理方面有一定的理论基础。通过设计新的图信号滤波器，我们可以从理论上设计新的图卷积网络。然而，基于谱的模型有着一些难以克服的缺点，下面我们将从效率、通用性和灵活性三个方面来阐述。在效率方面，基于谱的模型的计算成本随着图的大小而急剧增加，因为它们要么需要执行特征向量计算，要么同时处理整个图，这使得它们很难适用于大型图。基于空间的模型有潜力处理大型图，因为它们通过聚集相邻节点直接在图域中执行卷积。计算可以在一批节点中执行，而不是在整个图中执行。当相邻节点数量增加时，可以引入采样技术来提高效率。在一般性方面，基于谱的模型假定一个固定的图，使得它们很难在图中添加新的节点。另一方面，基于空间的模型在每个节点本地执行图卷积，可以轻松地在不同的位置和结构之间共享权重。在灵活性方面，基于谱的模型仅限于在无向图上工作，有向图上的拉普拉斯矩阵没有明确的定义，因此将基于谱的模型应用于有向图的唯一方法是将有向图转换为无向图。基于空间的模型更灵活地处理多源输入，这些输入可以合并到聚合函数中。因此，近年来空间模型越来越受到关注。</p><h2 id="2-图注意力网络（Graph-Attention-Networks）"><a href="#2-图注意力网络（Graph-Attention-Networks）" class="headerlink" title="2.图注意力网络（Graph Attention Networks）"></a>2.图注意力网络（Graph Attention Networks）</h2><p>注意力机制如今已经被广泛地应用到了基于序列的任务中，它的优点是能够放大数据中最重要的部分的影响。这个特性已经被证明对许多任务有用，例如机器翻译和自然语言理解。如今融入注意力机制的模型数量正在持续增加，图神经网络也受益于此，它在聚合过程中使用注意力，整合多个模型的输出，并生成面向重要目标的随机行走。在本节中，我们将讨论注意力机制如何在图结构数据中使用。</p><h2 id="3-Graph-Autoencoders"><a href="#3-Graph-Autoencoders" class="headerlink" title="3.Graph Autoencoders"></a>3.Graph Autoencoders</h2><p>图自动编码器是一类图嵌入方法，其目的是利用神经网络结构将图的顶点表示为低维向量。典型的解决方案是利用多层感知机作为编码器来获取节点嵌入，其中解码器重建节点的邻域统计信息，如positive pointwise mutual information （PPMI）或一阶和二阶近似值。最近，研究人员已经探索了将GCN作为编码器的用途，将GCN与GAN结合起来，或将LSTM与GAN结合起来设计图自动编码器。我们将首先回顾基于GCN的AutoEncoder，然后总结这一类别中的其他变体。</p><p>目前基于GCN的自编码器的方法主要有：Graph Autoencoder (GAE)和Adversarially Regularized Graph Autoencoder (ARGA)</p><p>图自编码器的其它变体有：</p><p>Network Representations with Adversarially Regularized Autoencoders (NetRA)</p><p>Deep Neural Networks for Graph Representations (DNGR)</p><p>Structural Deep Network Embedding (SDNE)</p><p>Deep Recursive Network Embedding (DRNE)</p><p>DNGR和SDNE学习仅给出拓扑结构的节点嵌入，而GAE、ARGA、NetRA、DRNE用于学习当拓扑信息和节点内容特征都存在时的节点嵌入。图自动编码器的一个挑战是邻接矩阵A的稀疏性，这使得解码器的正条目数远远小于负条目数。为了解决这个问题，DNGR重构了一个更密集的矩阵，即PPMI矩阵，SDNE对邻接矩阵的零项进行惩罚，GAE对邻接矩阵中的项进行重加权，NetRA将图线性化为序列。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/GAE.png" alt="gae"></p><h2 id="4-Graph-Generative-Networks"><a href="#4-Graph-Generative-Networks" class="headerlink" title="4.Graph Generative Networks"></a>4.Graph Generative Networks</h2><p>图生成网络的目标是在给定一组观察到的图的情况下生成新的图。图生成网络的许多方法都是特定于领域的。例如，在分子图生成中，一些工作模拟了称为SMILES的分子图的字符串表示。在自然语言处理中，生成语义图或知识图通常以给定的句子为条件。最近，人们提出了几种通用的方法。一些工作将生成过程作为节点和边的交替形成因素，而另一些则采用生成对抗训练。这类方法要么使用GCN作为构建基块，要么使用不同的架构。</p><p>基于GCN的图生成网络主要有</p><p>Molecular Generative Adversarial Networks (MolGAN)：将relational GCN、改进的GAN和强化学习（RL）目标集成在一起，以生成具有所需属性的图。GAN由一个生成器和一个鉴别器组成，它们相互竞争以提高生成器的真实性。在MolGAN中，生成器试图提出一个伪图及其特征矩阵，而鉴别器的目标是区分伪样本和经验数据。此外，还引入了一个与鉴别器并行的奖励网络，以鼓励生成的图根据外部评价器具有某些属性。</p><p>Deep Generative Models of Graphs (DGMG)：利用基于空间的图卷积网络来获得现有图的隐藏表示。生成节点和边的决策过程是以整个图的表示为基础的。简而言之，DGMG递归地在一个图中产生一个节点，直到达到某个停止条件。在添加新节点后的每一步，DGMG都会反复决定是否向添加的节点添加边，直到决策的判定结果变为假。如果决策为真，则评估将新添加节点连接到所有现有节点的概率分布，并从概率分布中抽取一个节点。将新节点及其边添加到现有图形后，DGMG将更新图的表示。</p><p>其它架构的图生成网络主要有</p><p>GraphRNN：通过两个层次的循环神经网络的深度图生成模型。图层次的RNN每次向节点序列添加一个新节点，而边层次RNN生成一个二进制序列，指示新添加的节点与序列中以前生成的节点之间的连接。为了将一个图线性化为一系列节点来训练图层次的RNN，GraphRNN采用了广度优先搜索（BFS）策略。为了建立训练边层次的RNN的二元序列模型，GraphRNN假定序列服从多元伯努利分布或条件伯努利分布。</p><p>NetGAN：Netgan将LSTM与Wasserstein-GAN结合在一起，使用基于随机行走的方法生成图形。GAN框架由两个模块组成，一个生成器和一个鉴别器。生成器尽最大努力在LSTM网络中生成合理的随机行走序列，而鉴别器则试图区分伪造的随机行走序列和真实的随机行走序列。训练完成后，对一组随机行走中节点的共现矩阵进行正则化，我们可以得到一个新的图。</p><h2 id="5-Graph-Spatial-Temporal-Networks"><a href="#5-Graph-Spatial-Temporal-Networks" class="headerlink" title="5.Graph Spatial-Temporal Networks"></a>5.Graph Spatial-Temporal Networks</h2><p>图时空网络同时捕捉时空图的时空相关性。时空图具有全局图结构，每个节点的输入随时间变化。例如，在交通网络中，每个传感器作为一个节点连续记录某条道路的交通速度，其中交通网络的边由传感器对之间的距离决定。图形时空网络的目标可以是预测未来的节点值或标签，或者预测时空图标签。最近的研究仅仅探讨了GCNs的使用，GCNs与RNN或CNN的结合，以及根据图结构定制的循环体系结构。</p><p><img src= "/img/loading.gif" data-lazy-src="/2021/01/18/GNN/time.jpg" alt="gae"></p><p>The key idea of STGNNs is to consider spatial dependency and temporal dependency at the same time.</p><p>目前图时空网络的模型主要有</p><p>Diffusion Convolutional Recurrent Neural Network (DCRNN)</p><p>CNN-GCN</p><p>Spatial Temporal GCN (ST-GCN)</p><p>Structural-RNN</p><h2 id="6-图神经网络的应用"><a href="#6-图神经网络的应用" class="headerlink" title="6.图神经网络的应用"></a>6.图神经网络的应用</h2><p>1、Computer Vision</p><p>图形神经网络的最大应用领域之一是计算机视觉。研究人员在场景图生成、点云分类与分割、动作识别等多个方面探索了利用图结构的方法。</p><p>在场景图生成中，对象之间的语义关系有助于理解视觉场景背后的语义含义。给定一幅图像，场景图生成模型检测和识别对象，并预测对象对之间的语义关系。另一个应用程序通过生成给定场景图的真实图像来反转该过程。自然语言可以被解析为语义图，其中每个词代表一个对象，这是一个有希望的解决方案，以合成给定的文本描述图像。</p><p>在点云分类和分割中，点云是激光雷达扫描记录的一组三维点。此任务的解决方案使激光雷达设备能够看到周围的环境，这通常有利于无人驾驶车辆。为了识别点云所描绘的物体，将点云转换为k-最近邻图或叠加图，并利用图论进化网络来探索拓扑结构。</p><p>在动作识别中，识别视频中包含的人类动作有助于从机器方面更好地理解视频内容。一组解决方案检测视频剪辑中人体关节的位置。由骨骼连接的人体关节自然形成图表。给定人类关节位置的时间序列，应用时空神经网络来学习人类行为模式。</p><p>此外，图形神经网络在计算机视觉中应用的可能方向也在不断增加。这包括人-物交互、少镜头图像分类、语义分割、视觉推理和问答等。</p><p>2、Recommender Systems</p><p>基于图的推荐系统以项目和用户为节点。通过利用项目与项目、用户与用户、用户与项目之间的关系以及内容信息，基于图的推荐系统能够生成高质量的推荐。推荐系统的关键是评价一个项目对用户的重要性。因此，可以将其转换为一个链路预测问题。目标是预测用户和项目之间丢失的链接。为了解决这个问题，有学者提出了一种基于GCN的图形自动编码器。还有学者结合GCN和RNN，来学习用户对项目评分的隐藏步骤。</p><p>3、Traffic</p><p>交通拥堵已成为现代城市的一个热点社会问题。准确预测交通网络中的交通速度、交通量或道路密度，在路线规划和流量控制中至关重要。有学者采用基于图的时空神经网络方法来解决这些问题。他们模型的输入是一个时空图。在这个时空图中，节点由放置在道路上的传感器表示，边由阈值以上成对节点的距离表示，每个节点都包含一个时间序列作为特征。目标是预测一条道路在时间间隔内的平均速度。另一个有趣的应用是出租车需求预测。这有助于智能交通系统有效利用资源，节约能源。</p><p>4、Chemistry</p><p>在化学中，研究人员应用图神经网络研究分子的图结构。在分子图中，原子为图中的节点，化学键为图中的边。节点分类、图形分类和图形生成是分子图的三个主要任务，它们可以用来学习分子指纹、预测分子性质、推断蛋白质结构、合成化合物。</p><p>5、Others</p><p>除了以上四个领域外，图神经网络还已被探索可以应用于其他问题，如程序验证、程序推理、社会影响预测、对抗性攻击预防、电子健康记录建模、脑网络、事件检测和组合优化。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/75307407">图神经网络（Graph Neural Networks，GNN）综述</a></li><li><a href="https://arxiv.org/abs/1901.00596">Wu, Zonghan , et al. “A Comprehensive Survey on Graph Neural Networks.” (2019)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MCMC</title>
      <link href="/2020/12/01/MCMC/"/>
      <url>/2020/12/01/MCMC/</url>
      
        <content type="html"><![CDATA[<h2 id="马尔可夫链蒙特卡罗法"><a href="#马尔可夫链蒙特卡罗法" class="headerlink" title="马尔可夫链蒙特卡罗法"></a>马尔可夫链蒙特卡罗法</h2><p>蒙特卡罗法(Monte Carlo method), 也称为统计模拟方法(statistical simulation method), 是通过概率模型的随机抽样进行近似数值计算的方法。马尔可夫链蒙特卡罗法(Markov Chain Monte Carlo, MCMC),则是以马尔可夫链为概率模型的蒙特卡罗法。Metropolis-Hasting算法是基本的马尔可夫链蒙特卡罗法，吉布斯抽样(Gibbs sampling)是更简单额、使用更广泛的马尔可夫链蒙特卡罗法。</p><h2 id="1-蒙特卡罗法"><a href="#1-蒙特卡罗法" class="headerlink" title="1. 蒙特卡罗法"></a>1. 蒙特卡罗法</h2><h3 id="1-1-随机抽样"><a href="#1-1-随机抽样" class="headerlink" title="1.1 随机抽样"></a>1.1 随机抽样</h3><p>蒙特卡罗法要解决的问题是，假设概率分布的定义已知，通过抽样获得概率分布的随机样本，并通过得到的随机样本对概率分布的特征进行分析，所以 <strong>蒙特卡罗法的核心是随机抽样</strong>。</p><p>蒙特卡罗法有直接抽样法、接受-拒绝抽样法、重要性抽样法等。这里介绍接受-拒绝抽样法(accept-reject method), 假设有随机变量$x$，取值$x \in \chi$,其概率密度函数为$p(x)$。目标得到该概率分布的随机样本，以对这个概率分布进行分析。</p><p>accept-reject method基本思想如下。假设$p(x)$不可以直接抽样。找一个可以直接抽样的分布，称为建议分布(proposal distribution)。假设$q(x)$是建议分布的概率密度函数，并且有$q(x)$的$c$倍一定大于等于$p(x)$, 期中$c &gt; 0$。按照$q(x)$进行抽样，假设得到的结果是$x^{*}$，再按照$\frac{p(x^{*)}}{cq(x)}$的比例随机决定是够接受$x^{*}$。接受拒绝法实际上是按照$p(x)$的涵盖面积(或体积)占$cp(x)$的涵盖面积的比例进行抽样。</p><p>接受拒绝法的具体算法：</p><blockquote><p>输入：抽样的目标概率分布的概率密度函数$p(x)$；<br> 输出：概率分布的随机样本$x_1, x_2,…,x_n$。<br> 参数：样本数$n$;</p></blockquote><ol><li>选择概率密度函数为$q(x)$的概率分布，作为建议分布，使其对任一$x$满足$cp(x) \gg p(x)$, 其中$c &gt; 0$。</li><li>按照建议分布$q(x)$随机抽样得到样本$x^{*}$, 再按照均匀分布在(0, 1)范围内抽样得到$u$;</li><li>如果$u \le \frac{p(x^*)}{cq(x^{*})}$, 则$x^*$作为抽样结果，否则，返回(2);</li><li>直到得到$n$个样本结束。</li></ol><p>该方法优点是简单，但是效率不高</p><h2 id="2-马尔可夫链"><a href="#2-马尔可夫链" class="headerlink" title="2. 马尔可夫链"></a>2. 马尔可夫链</h2><p>随机过程定义。考虑一个随机变量的序列$X&#x3D;{X_0, X_1,…,X_t,…}, X_t$表示时刻$t$的随机变量$t&#x3D;0, 1,2,…$，每个随机变量的取值集合相同，称为状态空间，表示为$S$。随机变量可以是离散的，也可以是连续的，以上随机变量的序列构成随机过程(stochastic process)。</p><p>马尔可夫性：$P(X_t|X_0,X_1,…,X_{t-1}) &#x3D; P(X_t|X{t-1}),\quad t &#x3D; 1,2,…$ 解释：<em>未来只依赖与现在(假设现在已知)，而与过去无关。</em></p><p>具有马尔可夫性的随机序列$X&#x3D;{X_0,X_1,…,X_t,…}$称为马尔可夫链Markov Chain,或马尔可夫过程Process。条件概率分布$P(X_t|X_{t-1})$称为马尔可夫链的转移概率分布。<br>若转移概率分布$P(X_t|X_{t-1})$与$t$无关，即<br>$$P(X_{t+s}|X_{t-1+s}) &#x3D; P(X_t|X_{t-1}), \quad t&#x3D;1,2,…;\ s&#x3D;1,2,…$$<br>则称该马尔可夫链为时间齐次的马尔可夫链(time homogenous Markov Chain)。</p><h3 id="2-1-离散状态马尔可夫链"><a href="#2-1-离散状态马尔可夫链" class="headerlink" title="2.1 离散状态马尔可夫链"></a>2.1 离散状态马尔可夫链</h3><h4 id="2-1-1-概率转移矩阵和状态分布"><a href="#2-1-1-概率转移矩阵和状态分布" class="headerlink" title="2.1.1 概率转移矩阵和状态分布"></a>2.1.1 概率转移矩阵和状态分布</h4><p>离散状态马尔可夫链$X&#x3D;{X_0,X_1,…,X_t,…}$,随机变量$X_t \ (t&#x3D;0,1,2,…)$定义在离散空间$S$, 转移概率分布可由矩阵表示。</p><p>若马尔可夫链在时刻$(t-1)$处于状态$j$，在时刻$t$移动到状态$i$，将转移概率记做<br>$$p_{ij} &#x3D; (X_t &#x3D; i|X_{t-1}&#x3D;j),\quad i&#x3D;1,2,…; j &#x3D; 1,2,…$$<br>满足<br>$$p_{ij} \ge 0, \qquad \sum_i p_{ij} &#x3D; 1$$<br>状态转移矩阵即为：<br>$$P &#x3D; \begin{bmatrix} p_{11}&amp;p_{12}&amp;p_{13}&amp;…\p_{21}&amp;p_{22}&amp;p_{23}&amp;…\p_{31}&amp;p_{32}&amp;p_{33}&amp;…\…&amp;…&amp;…&amp;… \end{bmatrix}$$</p><p>考虑马尔可夫链$X&#x3D;{X_0,X_1,…,X_t,…}$在时刻 $t\ (t&#x3D;0,1,2,…)$的概率分布存在，称为时刻$t$的状态分布，记做<br>$$\pi(t) &#x3D;\begin{bmatrix} \pi_1(t)\ \pi_2(t) \\pi_3(t) \…\end{bmatrix}$$<br>其中$\pi_i(t)$表示时刻$t$状态为$i$的概率$P(X_t &#x3D; i)$,$$\pi_i(t)&#x3D;P(X_t &#x3D; i), \qquad i&#x3D;1,2,…$$,<br>特别地，Markov Chain的初始状态分布可以表示为<br>$$\pi(0) &#x3D;\begin{bmatrix} \pi_1(0)\ \pi_2(0) \\pi_3(0) \…\end{bmatrix}$$<br>$\pi(0)$表示时刻0状态$i$的概率$P(X_0 &#x3D; i)$。通常初始分布$\pi(0)$的向量只有一个分量是1，其余分量都是0，表示Markov Chain从一个具体状态开始，像one-hot编码。</p><p>Markov Chain $X$在时刻$t$的状态分布，可以由在时刻$t-1$的状态分布以及转移概率分布决定:<br>$$\pi(t) &#x3D; P\pi(t-1)$$<br>通过递推可以得到: $\qquad \pi(t) &#x3D; P^t\pi(0)$</p><h4 id="2-1-2-平稳分布"><a href="#2-1-2-平稳分布" class="headerlink" title="2.1.2 平稳分布"></a>2.1.2 平稳分布</h4><p>定义：设有Markov Chain$X&#x3D;{X_0,X_1,…,X_t,…}$,其状态空间为$S$,转移概率矩阵为$P&#x3D;(p_{ij})$,如果存在状态空间$S$上的一个分布：<br>$$\pi &#x3D; \begin{bmatrix}\pi_1 \ \pi_2 \. \. \.\end{bmatrix}$$<br>使得$\qquad \pi &#x3D; P\pi$</p><p><br>则称$\pi$为Markov Chain$\ X&#x3D;{X_0,X_1,…,X_t,…}$的平稳分布。</p><p>意义：如果平稳分布$\pi$存在，且以$\pi$作为初始分布，则未来进行随机状态转移，之后任意一个时刻的状态分布都是该平稳分布。</p><p>Markov Chain可能存在唯一平稳分布，无穷多个平稳分布，或不存在平稳分布。当离散状态MC有无穷个状态时，有可能没有平稳分布。</p><h2 id="2-2-连续状态马尔可夫链"><a href="#2-2-连续状态马尔可夫链" class="headerlink" title="2.2 连续状态马尔可夫链"></a>2.2 连续状态马尔可夫链</h2><blockquote><p>离散状态马尔可夫链$X&#x3D;{X_0,X_1,…,X_t,…}$,随机变量$X_t \ (t&#x3D;0,1,2,…)$定义在离散空间$S$, 转移概率分布可由矩阵表示。</p></blockquote><p>连续状态马尔可夫链$X&#x3D;{X_0,X_1,…,X_t,…}$,随机变量$X_t \ (t&#x3D;0,1,2,…)$定义在离散空间$S$, 转移概率分布由概率转移核或转移核(transition kernel)表示。</p><p>设$S$是连续状态空间，对任意的$x \in S,\  A \subset S$，转移核$P(x,A)$定义为：<br>$$P(x,A)&#x3D;\int_Ap(x,y)dy$$<br>其中$p(x,·)$是概率密度函数，满足$p(x,·) \ge 0,\ P(x,S)&#x3D;\int_S p(x,y)dy&#x3D;1$。转移核$P(x,A)$表示从$x \sim A$的转移概率<br>$$P(X_t &#x3D; A|X_{t-1}&#x3D;x)&#x3D;P(x,A)$$<br>又是也将概率密度函数$p(x,·)$称为转移核。</p><p>若马尔可夫链的状态空间$S$上的概率分布$\pi(x)$满足条件<br>$$\pi(y)&#x3D;\int p(x,y)\pi(x)dx,\qquad \forall y \in S$$<br>则称分布$\pi(x)$为该马尔可夫链的平稳分布。简写为$\pi &#x3D; P\pi$</p><h3 id="2-2-1-马尔可夫链的性质"><a href="#2-2-1-马尔可夫链的性质" class="headerlink" title="2.2.1 马尔可夫链的性质"></a>2.2.1 马尔可夫链的性质</h3><p>(1)不可约(irreducible)</p><blockquote><p>设有Markov Chain$X&#x3D;{X_0,X_1,…,X_t,…}$,其状态空间为$S$，如果存在一个时刻$t(t&gt;0)$满足<br>$$P(X_t&#x3D;i|X_0&#x3D;j) &gt; 0$$<br>即，时刻0从状态$j$出发，时刻$t$到达状态$i$的概率大于0，则称此MC $X$是不可约的(irreducible),否则称MC是可约(reducible)的。直观上一个不可约的MC，从任意状态出发，当经过从充分长时间后，可以到达任意状态。</p></blockquote><p>(2)非周期(aperiodic)</p><blockquote><p>对于任意状态$i \in S$，如果时刻0从状态$i$出发，$t$时刻返回状态的所有时间长${t: P(X_t &#x3D; i | X_0 &#x3D; i) &gt; 0}$的最大公约数是1，则称此马尔可夫链$X$是非周期的(aperiodic)，否则称MC是周期的(periodic)。</p></blockquote><p>(3)正常返(positive recurrent)</p><blockquote><p>对于任意状态$i,j \in S$，定义概率$p_{ij}^t$为时刻0从状态$j$出发，时刻$t$首次转移到状态$i$的概率，即$P_{ij}^t &#x3D; P(X_t&#x3D;i, X_s \ne i,\ s&#x3D;1,2,…,t-1｜X_o&#x3D;j),\ t&#x3D;1,2,…。$若对于所有状态$i,j$都满足$\lim_{p \to \infty}p_{ij}^t &gt; 0$，则称MC $X$是正常返的(positive recurrent)。</p></blockquote><p>直观上，一个正常返的MC，其中任意一个状态，从其他任意一个状态出发，当时间趋于无穷时，首次转移到这个状态的概率不为0。</p><ul><li>定理：不可约(irreducible)、非周期(aperiodic)且正常返(positive recurrent)的MC，有唯一平稳分布存在。</li></ul><p>(4)遍历定理</p><blockquote><p>若MC $X$是不可约的、非周期且正常返的，则该MC有唯一平稳分布$\pi&#x3D;(\pi_1,\pi_2,…)^T$，并且转移概率的极限分布是MC的平稳分布<br>$$lim_{t \to \infty}P(X_t&#x3D;i|X_0&#x3D;j)&#x3D;\pi_i,\qquad i&#x3D;1,2,…;\quad j&#x3D;1,2,…$$<br>若$f(X)$是定义在状态空间上的函数，$E_\pi[|f(X)|]&lt;\infty$,则<br>$$P{\hat{f_t} \to E_\pi[f(X) ]} &#x3D;1,\qquad \hat{f_t} &#x3D; \frac{1}{t}\sum_{s&#x3D;1}^tf(x_s)$$<br>$E_\pi[f(X)]&#x3D;\sum_if(i)\pi_i$是关于平稳分布$\pi&#x3D;(\pi_1,\pi_2,…)^T$的数学期望，公式（1）表示<br>$$\hat{f_t}\to E\pi[f(X)],\qquad t\to \infty$$<br>几乎处处成立或以概率1成立。</p></blockquote><p>遍历定理的直观解释：满足相应条件的MC，当时间趋向于无穷的时，MC的状态分布趋近于平稳分布，随机变量的函数样本均值以概率1收敛于该函数的数学期望。</p><p>(5)可逆马尔可夫链</p><p>MC转移矩阵为$P$，如果有状态分布$\pi&#x3D;(\pi_1,\pi_2,…)^T$，对于任意状态$\ i,j \in S$,对于任意一个时刻$t$满足：<br>$$P(X_t &#x3D; i|X_t-1&#x3D;j)\pi_j &#x3D; P(X_{t-1}&#x3D;j|X_t &#x3D; i)\pi_i, \qquad i,j&#x3D;1,2,…$$<br>或者简写为（细致平衡方程（detailed balance equation））：<br>$$p_{ji}\pi_j &#x3D; p_{ij}\pi_i, \qquad, i,j&#x3D;1,2,…$$<br>则称此MC X为可逆的MC。</p><h2 id="3-马尔科夫链蒙特卡洛法"><a href="#3-马尔科夫链蒙特卡洛法" class="headerlink" title="3. 马尔科夫链蒙特卡洛法"></a>3. 马尔科夫链蒙特卡洛法</h2><p>假设目标是对一个概率分布进行随机抽样，或者是求函数关于该概率分布的数学期望。可以采用传统的蒙特卡洛法，如接受拒绝抽样法等。也可使用马尔科夫链蒙特卡洛法（MCMC），MCMC更适用于随机变量是多元的、密度函数是非标准化的、随机变量各分量是不独立等情况。常用的MCMC方法有Metropolis-Hastings算法、Gipps sampling。</p><p>问题描述：</p><blockquote><p>假设多元随机变量是$x$，满足$x \in \chi$，其概率密度函数为$p(x),\ f(x)$为定义在$x \in \chi$上的函数。</p><p><br>目标：获得概率分布$p(x)$的样本集合，以及求函数$f(x)$的数学期望$E_{p(x)|f(x)|}$。</p></blockquote><p>解决思路：</p><blockquote><p>在随机变量x的状态空间S上定义一个满足遍历定理的MC　X，使其平稳分布就是抽样的目标分布$p(x)$。然后在这个MC上随机游走，每个时刻得到一个样本，由遍历定理得，该样本集合${x_{m+1}, x_{m+2},…x_n}$就是目标概率分布的抽样结果，得到的函数均值（遍历均值）就是要计算的数学期望：<br>$$\hat{E}f&#x3D;\frac{1}{n-m}\sum_{t&#x3D;m+1}^nf(x_i)$$<br>到时刻m为止的时间段称为燃烧期。</p></blockquote><h3 id="3-1-基本步骤"><a href="#3-1-基本步骤" class="headerlink" title="3.1 基本步骤"></a>3.1 基本步骤</h3><p>MCMC三个步骤：</p><ol><li>首先，在随机变量$x$的状态空间$S$上构造一个满足遍历定理的的MC，使其平稳分布为目标分布$p(x)$;</li><li>从状态空间的某一点$x_0$出发，用构造的MC进行随机游走，产生样本序列$x_0,x_1,…,x_t,…$。</li><li>应用MC的遍历定理，确定正整数$m$和$n, \ (m&lt;n)$, 得到样本集合${x_{m+1}, x_{m+2}, …, x_n}$, 求得函数$f(x)$的均值(遍历均值)：<br>$$\hat{E}f&#x3D;\frac{1}{n-m}\sum_{t&#x3D;m+1}^nf(x_i)$$</li></ol><h2 id="4-Metropolis-Hastings算法"><a href="#4-Metropolis-Hastings算法" class="headerlink" title="4. Metropolis-Hastings算法"></a>4. Metropolis-Hastings算法</h2><h3 id="4-1-基本原理"><a href="#4-1-基本原理" class="headerlink" title="4.1 基本原理"></a>4.1 基本原理</h3><h4 id="4-1-1-马尔可夫链"><a href="#4-1-1-马尔可夫链" class="headerlink" title="4.1.1 马尔可夫链"></a>4.1.1 马尔可夫链</h4><p>假设要抽样的而概率分布为$p(x)$。Metropolis-Hastings算法采用转移核为$p(x,x’)$的马尔可夫链：<br>$$p(x,x’) &#x3D; q(x,x’)\alpha(x,x’)$$<br>其中$q(x,x’)和\alpha(x,x’)$分布称为建议分布(proposal distribution)和接受分布(acceptance distribution)。</p><p>建议分布$q(x,x’)$是另一个马尔可夫链的转移核，并且$q(x,x’)$是不可约的，即其概率值不为0，同时是一个容易抽样的分布。接受分布$\alpha(x,x’)$是<br>$$\alpha(x,x’)&#x3D;min{1,\ \frac{p(x’)q(x’,x)}{p(x)q(x,x’)}}$$<br>这时，转移核$p(x,x’)$可以写成<br>$$p(x,x’)&#x3D;\left{<br>\begin{aligned}<br>    q&amp;(x,x’), \qquad &amp;p&amp;(x’)q(x’,x) \ge p(x)q(x,x’) \<br>    q&amp;(x’,x) \frac{p(x’)}{p(x)}, &amp;\qquad p&amp;(x’)q(x’,x) &lt; p(x)q(x,x’)<br>\end{aligned}<br>    \right.$$<br>转移核为$p(x,x’)$的马尔可夫链上的随机游走以下方式进行。</p><p>如果在时刻$(t-1)$处于状态$x$，即$x_{t-1}&#x3D;x$,则先按照建议分布$q(x,x’)$抽样一个候选状态$x’$，然后按照接受分布$\alpha(x,x’)$抽样决定是否接受状态$x’$。以概率$\alpha(x,x’)$接受$x’$,决定时刻$t$转移到状态$x’$，而以概率$1-\alpha(x,x’)$拒绝$x’$,决定时刻$t$仍停留在状态$x$。具体地，从区间（0，1）上的均匀分布中抽取一个随机数$\mu$,决定时刻$t$的状态。<br>$$<br>x_t&#x3D;<br>\left{<br>    \begin{aligned}<br>        x’,&amp; \qquad u \le \alpha(x,x’) \<br>        x,&amp; \qquad u &gt; \alpha(x,x’)<br>    \end{aligned}<br>\right.<br>$$<br>可以证明，转移核为$p(x,x’)$的MC 是可逆MC（满足遍历定理），其平稳分布就是$p(x)$,即要抽样的目标分布。</p><h4 id="4-1-2-建议分布"><a href="#4-1-2-建议分布" class="headerlink" title="4.1.2 建议分布"></a>4.1.2 建议分布</h4><p>建议分布q(x,x’)有多种可能的形式，这里介绍两种常用形式。</p><p>第一种形式，假设建议分布是对称的，即对任意的$x和x’$有<br>$$q(x,x’)&#x3D;q(x’,x)$$<br>这样的建议分布称为Metropolis选择，也是Metropolis-Hastings算法最出采用的建议分布，这时，接受分布$\alpha(x,x’)$简化为<br>$$\alpha(x,x’)&#x3D;min{1,\frac{p(x’)}{p(x)}}$$</p><p>Metropolis选择的一个特例是$q(x,x’)$取条件概率分布$p(x’|x)$，定义为多元正态分布，其均值是$x$，其协方差矩阵是常熟矩阵。</p><p>Metropolis选择的另一个特例是令$q(x,x’)&#x3D;q(|x-x’|)$,这时算法称为随机游走Metropolis算法。</p><p>独立抽样实现简单，但可能收敛速度满，通常选择接近目标分布$p(x)$的分布做为建议分布$q(x)$。</p><h4 id="4-1-3-满条件分布"><a href="#4-1-3-满条件分布" class="headerlink" title="4.1.3 满条件分布"></a>4.1.3 满条件分布</h4><p>MCMC的你目标分布通常是多元联合概率分布$p(x)&#x3D;p(x_1,x_2,…,x_k),其中x&#x3D;(x_1,x_2,…,x_k)^T$为$k$维随机变量。如果条件概率分布$p(x_I|x_{-I})$中所有$k$个变量全部出现，其中$x_I&#x3D;{x_i,i\in I}, x_{-I}&#x3D;{x_i,i \not\in I}, I \subset K &#x3D; {1,2,..,k}$,那么称这种条件概率分布为满条件分布(full conditional distribution)。<br>Sample:<br><img src= "/img/loading.gif" data-lazy-src="/2020/12/01/MCMC/full.png" alt="Sample"></p><p>性质：</p><ol><li>对于任意$x,x’\in \chi$和任意的$I \subset K$，有<br>$$p(x_I|x_{-I})&#x3D;\frac{p(x)}{\int p(x)dx_I} \propto p(x)$$</li><li>对任意的$x,x’\in \chi$和任意的$I \subset K$，有<br>$$\frac{p(x’<em>I|x’</em>{-I})}{p(x_I|x_{-I})}&#x3D;\frac{p(x’)}{p(x)}$$</li></ol><h3 id="4-2-Metropolis-Hasting算法"><a href="#4-2-Metropolis-Hasting算法" class="headerlink" title="4.2 Metropolis-Hasting算法"></a>4.2 Metropolis-Hasting算法</h3><blockquote><p>输入：抽样的目标分布的密度函数$p(x)$，函数$f(x)$；</p><p><br>输出：$p(x)$的随机样本$x_{m+1},x_{m+2},…,x_n$，函数样本均值$f_{mn}$;</p><p><br>参数：收敛步数$m$,迭代步数$n$。</p><p><br>$\quad$ (1)任意选择一个初始值$x_0$</p><p><br>$\quad$ (2)对$i&#x3D;1,2,…,n$循环执行</p><p><br>$\qquad$ (a)设状态$x_{i-1}&#x3D;x$，按照建议分布$q(x,x’)$随机抽取一个候选状态$x’$</p><p><br>$\qquad$ (b)计算接受概率<br>$$\alpha(x,x’)&#x3D;min{1,\frac{p(x’)q(x’,x)}{p(x)q(x,x’)}}$$<br>$\qquad$ (c)从区间(0,1)中按均匀分布随机抽取一个数$u$。</p><p><br>$\qquad\quad$ 若$u \le \alpha(x,x’)$，则状态$x_i&#x3D;x’$；否则状态$x_i&#x3D;x$</p><p><br>$\quad$(3) 得到样本集合${x_{m+1},x_{m+2},…,x_n}$计算<br>$$f_{mn}&#x3D;\frac{1}{n-m}\sum_{t&#x3D;m+1}^nf(x_i)$$</p></blockquote><h3 id="4-3-单分量Metropolis-Hastings算法"><a href="#4-3-单分量Metropolis-Hastings算法" class="headerlink" title="4.3 单分量Metropolis-Hastings算法"></a>4.3 单分量Metropolis-Hastings算法</h3><p>在MH算法中，通常需要对多元变量分布进行抽样，有时对多元变量抽样是困难的。可以对多元变量的每一变量的条件分布依次分别进行抽样，从而实现多元变量的依次抽样，这就是单分量MH(Single-component Metropolis-Hasting)算法。</p><p>假设马尔可夫链的状态由$k$维随机变量表示<br>$$x&#x3D;(x_1,x_2,…,x_k)^T$$<br>其中$x_j$表示随机变量$x$的第$j$个分量，$j&#x3D;1,2,…,k$，而$x^{(i)}$表示马尔可夫链在时刻$i$的状态<br>$$x&#x3D;(x_1^{(i)},x_2^{(i)},…,x_k^{(i)})^T,\qquad i&#x3D;1,2,…,n$$<br>其中$x_j^{(i)}$是随机变量$x^{(i)}$的第$j$个分量，$j&#x3D;1,2,…,k$。</p><p>为了生成容量为n的样本集合${x^{(1)},x^{(2)},…,x^{(n)}}$，单分量Metropolis-Hastings算法由下面的$k$步迭代实现Metropolis-Hastings算法的一次迭代。</p><p>设在第$(i-1)$次迭代结束时分量$x_j$的取值为$x_j^{(i-1)}$，在第$i$次迭代的第$j$步，对分量$x_j$根据Metropolis-Hastings算法更新，得到其新的取值$x_j^{(i)}$。首先，由建议分布$q(x_j^{(i-1)}，x_{-j}^{(i)}$抽样产生分量$x_j$的候选值$x_j^{‘(i)}$，这里$x_j^{(i)}$表示在第$i$次迭代的第$(j-1)$步后的$x^{(i)}$除去$x_j^{(i-1)}$的所有值，即<br>$$x_{-j}^{(i)}&#x3D;(x_1^{(i)},…,x_{(j-1)}^{(i)},x_{(j+1)}^{(i-1)},…,x_k^{(i-1)})^T$$<br>其中分量$1,2,…,j-1$已经更新。然后，按照接受概率<br>$$\alpha(x_j^{(i-1)},x_j^{‘(i)}|x_{-j}^{(i)})&#x3D;min{1,\frac{p(x_j^{‘(i)}|x_{-j}^{(i)})q(x_j^{‘(i)},x_j^{(i-1)}|x_{-j}^{(i)})}{p(x_j^{(i-1)}|x_{-j}^{(i)})q(x_j^{(i-1)},x_j^{‘(i)}|x_{-j}^{(i)})}}$$<br>抽样决定是否接受候选值$x_j^{‘(i)}$。如果$x_j^{‘(i)}$被接受，则令$x_j^{(i)}&#x3D;x_j^{‘(i)}$，否则令$x_j^{(i)}&#x3D;x_j^{(i-1)}$。其余分量在第$j$步不改变。马尔科夫链的转移概率为：<br>$$p(x_j^{(i-1)},x_j^{‘(i)}|x_{-j}^{(i)})&#x3D;\alpha(x_j^{(i-1)},x_j^{‘(i)}|x_{-j}^{(i)})q(x_j^{(i-1)},x_j^{‘(i)}|x_{-j}^{(i)})$$</p><h2 id="5-吉布斯抽样Gibbs-sampling"><a href="#5-吉布斯抽样Gibbs-sampling" class="headerlink" title="5. 吉布斯抽样Gibbs sampling"></a>5. 吉布斯抽样Gibbs sampling</h2><p>Metropolis-Hastings算法的特殊情况，但是更容易实现，因而被广泛采用。</p><h3 id="5-1-基本原理"><a href="#5-1-基本原理" class="headerlink" title="5.1 基本原理"></a>5.1 基本原理</h3><p>Gibbs samppling 用于多元变量联合分布的抽样和估计。其基本做法是，从联合概率分布定义满条件概率分布，依次对fully conditional distribution进行抽样，得到样本的序列。可以证明这样的抽样过程是在一个MC上的随机游走，每一个样本对应着马尔可夫链的状态，平稳分布就是目标的联合分布。整体成为一个MCMC，燃烧期之后的样本就是联合分布的随机样本。</p><p>假设多元变量的联合概率分布为$p(x)&#x3D;p(x_1,x_2,…,x_k)。$Gibbs sampling从一个初始样本$x^{(0)}&#x3D;(x_1^{(0)},x_2^{(0)},…x_k^{(0)})^T$出发，不断进行迭代，每一次迭代得到的联合分布的一个样本$x^{(i)}&#x3D;(x_1^{(i)},x_2^{(i)},…x_k^{(i)})$。最终得到样本序列${x^{(0)},x^{(1)},…,x^{(n)}}$。</p><p>设在第$(i-1)$步得到样本$(x_1^{(i-1)},x_2^{i-1)},…,x_k^{(i-1)})^T$，在第$i$步，首先对第一个变量按照以下fully conditional distribution随机抽样，即<br>$$p(x_1|x_2^{(t-1)},…,x_k^{(t-1)}$$<br>得到$x_1^{(i)}$，最后对第$k$个变量按照以下fully conditional distribution随机抽样<br>$$p(x_k|x_1^{(i)},…,x_{k-1}^{(i)}$$<br>得到$x_k^{(i)}$，于是得到整体样本$x^{(i)}&#x3D;(x_1^{(i)},x_2^{(i)},…,x_k^{(i)})^T$。</p><p>Gibbs sampling是单分量Metropolis-Hastings算法的特殊情况。定义建议分布是当前变量$x_j,\ j&#x3D;1,2,…,k$的fully conditional distribution：<br>$$q(x,x’)&#x3D;p(x’<em>j|x</em>{-j})$$<br>这时，接受概率$\alpha&#x3D;1$</p><p>转移核就是fully conditional distribution:<br>$$p(x,x’)&#x3D;p(x’<em>j|x</em>{-j})$$<br>也就是说按照单变量的满条件概率分布$p(x’<em>j|x</em>{-j})$进行随机抽样，就能实现单分量MH算法。Gibbs sampling对每次抽样的结果都接受，没有拒绝，这是与一般MH算法的不同。这里，假设满条件概率分布$p(x’<em>j|x</em>{-j})$不为0，即MC是不可约的。</p><h3 id="5-2-Gibbs-sampling算法"><a href="#5-2-Gibbs-sampling算法" class="headerlink" title="5.2 Gibbs sampling算法"></a>5.2 Gibbs sampling算法</h3><blockquote><p>输入：目标概率分布的密度函数$p(x)$，函数$f(x)$; </p><p><br>输出：$p(x)$的随机样本$x_{m+1},x_{m+2},…,x_n$，函数样本均值$f_{mn}$;</p><p><br>参数：收敛步数$m$，迭代步数$n$。</p><p><br>(1)初始化。给出初始样本$x^{(0)}&#x3D;(x_1^{(0)},x_2^{(0)},…,x_k^{(0)})^T$。</p><p><br>(2)对$i$循环执行</p><p><br>设第$(i-1)$次迭代结束时的样本为$x^{(i-1)}&#x3D;(x_1^{(i-1)},x_2^{(i-1)},…,x_k^{(i-1)})^T$，则第$i$次迭代进行如下几步操作：</p><p><br>$\qquad$ (1)由满条件分布$p(x_1|x_2^{(i-1)},…,x_k^{(i-1)})抽取x_1^{(i)}$</p><p><br> $\qquad$…</p><p><br> $\qquad$ (j)由满条件分布$p(x_j|x_1^{(i)},…,x_{j-1}^{(i)},x_{j+1}^{(i-1)},…,x_k^{(i-1)})抽取x_j^{(i)}$</p><p><br> $\qquad$…</p><p><br>  $\qquad$ (k)由满条件分布$p(x_k|x_1^{(i)},x_2^{(i)}…,x_k^{(i)})抽取x_k^{(i)}$</p><p><br>则得到第$i$次迭代值$x^{(i)}&#x3D;(x_1^{(i)},x_2^{(i)},…,x_k^{(i)})^T$。</p><p><br>(3)得到样本集合${x_{m+1},x_{m+2},…,x_n}$</p><p><br>(4)计算$f_{mn}&#x3D;\frac{1}{n-m}\sum_{i&#x3D;m+1}^nf(x^{(i)})$</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Probabilistic Graphical Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 马尔可夫链 </tag>
            
            <tag> 蒙特卡罗法 </tag>
            
            <tag> 概率分布 </tag>
            
            <tag> Gibbs sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>共轭分布</title>
      <link href="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/"/>
      <url>/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<p>共轭分布是概率统计中一个常见的名词，要真正了解它和它的用途，我们需要从贝叶斯学派说起。</p><h2 id="贝叶斯学派"><a href="#贝叶斯学派" class="headerlink" title="贝叶斯学派"></a>贝叶斯学派</h2><p>贝叶斯学派试图描述观察者在已有的先验知识状态下，在观察到新事件发生后得到后验知识状态。与之对应的是频率学派，频率学派强调从样本数据中直接得到出现的比例或者频率。频率学派需要大量样本数据作为支持，但是实际应用上，比如在药物等真实场景上是没有那么多数据的，因此在真实环境下贝叶斯理论使用更为广泛。</p><h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86.png" alt="贝叶斯定理"></p><ul><li>似然函数(Likelihood)：关于统计模型中参数$\theta$的函数，表示模型参数中的似然性</li><li>先验分布(Prior)：在未看到的观测数据的时候参数$\theta$的不确定性的概率分布</li><li>后验分布(Posterior)：考虑和给出相关证据或数据后所得到的条件概率分布</li><li>分母( $\int p(D|\theta)p(\theta)d\theta$ )：可以理解为正则化，使得最终概率相加为1，符合基本约束的作用</li></ul><p>在贝叶斯定理中，参数先有一个先验认知（先验分布），然后通过观察新数据，得到后验认知（后验分布）。</p><h2 id="共轭分布"><a href="#共轭分布" class="headerlink" title="共轭分布"></a>共轭分布</h2><p>在贝叶斯统计中，如果后验分布与先验分布属于同类（分布形式相同），则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验(conjugate prior)。</p><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83.png" alt="共轭分布"></p><p>验证共轭分布：因为 **$后验分布 \propto 先验分布$**，因此当我们将似然函数和先验分布式子对应带入，正则化后所得后验分布与先验分布形式相同，那么就说明它们是共轭分布。</p><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/sample.png" alt="sample"></p><h2 id="共轭分布的意义"><a href="#共轭分布的意义" class="headerlink" title="共轭分布的意义"></a>共轭分布的意义</h2><p>从上面的例子推到结果中，我们其实已经能看到共轭分布的意义了。</p><p>因为后验分布和先验分布形式相近，知识参数有所不同，这意味着当我们获得新的观察数据时，我们就能直接通过参数更新，获得新的后验分布，此后验分布将会在下次新数据到来的时候成为新的先验分布。如此一来，我们更新后验分布就不需要大量的计算，十分方便。</p><h2 id="共轭的意义"><a href="#共轭的意义" class="headerlink" title="共轭的意义"></a>共轭的意义</h2><p>我们继续结合上面二项分布和Beta分布的共轭证明，以抛硬币作为例子说明共轭的意义。</p><p>此处先简单介绍一下Beta分布。</p><h3 id="Beta分布"><a href="#Beta分布" class="headerlink" title="Beta分布"></a>Beta分布</h3><p>每个概率模型都具有现实意义，Beta分布是指一组定义在(0, 1)区间的连续概率分布，有两个参数 $\alpha,\beta &gt; 0$,</p><p>$$Beta(\alpha, \beta) &#x3D; \frac{1}{B(\alpha,\beta)}\pi^{\alpha-1}(1-\pi)^(\beta-1)$$</p><p>模型将会在$\pi &#x3D; \frac{\alpha-1}{\alpha + \beta -2}$处取得最大值，模型均值为$E(\pi) &#x3D; \frac{\alpha}{\alpha+\beta}$</p><p>Beta分布常用于表示概率的概率分布，常用于表示成功或失败的概率的概率分布。</p><h3 id="例子：抛硬币"><a href="#例子：抛硬币" class="headerlink" title="例子：抛硬币"></a>例子：抛硬币</h3><p>假设我们有一个硬币，似然函数采用二项分布，先验认为抛一次硬币正面的平均概率是0.5，且$P(正面)&#x3D;\pi$ ，其中$\pi$的取值服从$Beta(50,50)$分布，即$\alpha &#x3D; 50, \beta &#x3D;50, E(\pi)&#x3D;\frac{50}{50+50}&#x3D;0.5$。此处只要使得模型期望等特征与设想相同，取值还可以是其他的数字，比如$\alpha&#x3D;1000,\beta&#x3D;1000$。模型参数的选择将会影响后面观察数据对后验的贡献，下面举例进行说明。假设后面我们扔了十次硬币，结果是三正七反。回想上面我们对$二项分布$和$Beta分布$的共轭证明中得到后验分布将服从的$Beta分布$形式：</p><p>$$P(\pi|x,n,\alpha,\beta) \sim Beta(x+\alpha, n-x+\beta)$$</p><p>此时我们的$n&#x3D;10, x&#x3D;3$，由此我们可以得到后验服从$Beta(3+50, 10-3+50)&#x3D;Beta(53,57)$平均概率约等于0.482。但是如果我们认为先验分布服从$Beta(1000, 1000)$分布，那后验将会是$Beta(3+1000, 10-3+1000)&#x3D;Beta(1003,1007)$，平均概率约等于0.499。在两个模型下，后验分布的期望概率都比之前的0.5要小，十次硬币的数据样本对后面一个模型的后验分布的影响较小。由此可见，先验分布的参数选择有时会影响到样本数据对后验的贡献等，大家应根据自己实际情况进行先验分布模型的具体参数的选择。</p><p>如果没有共轭，在需要计算多批新样本数据下的后验分布时，每次计算都需要整体重新计算。反之如果存在共轭分布，共轭可以使得我们的后验分布，之后直接成为“先验”，不需要重新整体计算，只需要考虑新样本数据。因此共轭的存在将会给我们对后验的更新带来极大的便利。共轭还可以保证后验分布符合某概率模型分布，而常见的概率模型分布如Beta、Gamma、正态分布等会有一些已有的数学性质可以直接使用，比如期望、极值点等。</p><h2 id="指数分布族"><a href="#指数分布族" class="headerlink" title="指数分布族"></a>指数分布族</h2><p>由于指数分布族具有很多很好的性质，其中有一条便是指数分布族都有共轭。<br>狄利克雷分布属于指数分布族。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1]<a href="https://zh.wikipedia.org/wiki/%E5%85%88%E9%A9%97%E8%88%87%E5%BE%8C%E9%A9%97">先验与后验</a><br>[2]<a href="https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0">似然函数</a><br>[3]<a href="https://zh.wikipedia.org/wiki/%CE%92%E5%88%86%E5%B8%83">Beta分布</a><br>[4]<a href="https://www.datalearner.com/blog/1051550130370543">指数分布族</a></p><h2 id="相关阅读"><a href="#相关阅读" class="headerlink" title="相关阅读"></a>相关阅读</h2><p>[1] <a href="/2020/11/30/Wishart-%E5%88%86%E5%B8%83%E5%8F%8A%E9%80%86%E5%88%86%E5%B8%83/">Wishart分布与Wishart逆分布</a></p>]]></content>
      
      
      <categories>
          
          <category> Probabilistic Graphical Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率分布 </tag>
            
            <tag> 共轭分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wishart 分布及逆分布</title>
      <link href="/2020/11/30/Wishart-%E5%88%86%E5%B8%83%E5%8F%8A%E9%80%86%E5%88%86%E5%B8%83/"/>
      <url>/2020/11/30/Wishart-%E5%88%86%E5%B8%83%E5%8F%8A%E9%80%86%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h2 id="Wishart-分布"><a href="#Wishart-分布" class="headerlink" title="Wishart 分布"></a>Wishart 分布</h2><p>Wishart 分布是用来描述多元正态样本的协方差矩阵而引入的矩阵型随机分布，注意它是一个随机矩阵，不是随机变量。所以一般的多元统计书都是一笔带过的。</p><p>从最简单的Wishart分布开始：</p><p>假设有$m$个独立同分布$z_i \sim N_p(0, I_p)$，也就是标准多元正态分布，$V &#x3D; \sum_{i&#x3D;1}^{m}z_iz’_i$, 则称$V$服从自由度为$m的Wishart分布$，记做$V \sim W_p(m)$</p><p>稍微复杂点的：</p><p>假设$m$个独立同分布的 $z_i \sim N_p(0, \Sigma)$, 也就是中心化的多元正态分布，$W &#x3D; \sum_{i&#x3D;1}^mz_iz’_i$, 则$W \sim W_p(m, \Sigma)$, 多了一个用于描述多元正态分布协方差阵的参数$\Sigma$</p><p>另一种定义方式：</p><p>设n个独立同分布的$x_i \sim N_p(\mu, \Sigma)$, 有统计量$\overline{x} &#x3D; \frac{\sum_{i&#x3D;1}^n x_i}{n}$, $S &#x3D; \frac{1}{n-1} \sum_{i&#x3D;1}^n(x_i- \overline{x})(x_i-\overline{x})’$,那么它们的分布是$\overline{x} \sim N_p(\mu, \frac{\sigma}{n})$ 和 $(n-1)S \sim W_p(n-1, \Sigma)$且二者独立。</p><h2 id="Inverse-Wishart-分布"><a href="#Inverse-Wishart-分布" class="headerlink" title="Inverse-Wishart 分布"></a>Inverse-Wishart 分布</h2><p>如果一个正定矩阵$B$的逆矩阵$B^{-1}$服从$Wishart分布W_p(m,\Sigma)$, 那么称$B$服从$Inverse-Wishart分布W_p^{-1}(m, \Sigma)$,</p><p>Inverse-Wishart分布常作为Bayes中多元正态分布的协方差阵的共轭先验分布。</p><p>假设独立同分布的$x_i \sim N_p(0, \Sigma)$， $\Sigma \sim W_p^{-1}(m, \Omega)$，那么后验条件分布 </p><p>$$\Sigma | data \sim W_p^{-1}(m + n, A + \Omega),\quad A &#x3D; \sum_{i&#x3D;1}^nx_ix’_i &#x3D; nS$$</p><p>参考文献<br>[1] Murphy K P. Conjugate Bayesian analysis of the Gaussian distribution[J]. def, 2007, 1(2σ2): 16.</p><h2 id="相关阅读"><a href="#相关阅读" class="headerlink" title="相关阅读"></a>相关阅读</h2><p>[1] <a href="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/">共轭分布</a></p>]]></content>
      
      
      <categories>
          
          <category> Probabilistic Graphical Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率分布 </tag>
            
            <tag> Wishart分布 </tag>
            
            <tag> 逆Wishart分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Probabilistic Graphical Models(1 )</title>
      <link href="/2020/11/26/Probabilistic-Graphical-Models-1/"/>
      <url>/2020/11/26/Probabilistic-Graphical-Models-1/</url>
      
        <content type="html"><![CDATA[<h2 id="What-are-Graphical-Models"><a href="#What-are-Graphical-Models" class="headerlink" title="What are Graphical Models?"></a>What are Graphical Models?</h2><p>A graphical model can be thought of as a probabilistic datasets, a machine that can answer “queries” regarding the values of sets of random variables.</p><ul><li>PGM &#x3D; Multivariate Statistics + Structure</li></ul><p>A more formal description:</p><ul><li>It refers to a family of distributions on a set of random variables that are compatible with all the probabilistic independence propositions(命题) encoded by a graph that connects these variables.</li></ul><p>An (incoplete) genealogy of graphical models<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/26/Probabilistic-Graphical-Models-1/genealogy.png" alt="genealogy"></p><h2 id="Rational-Statistical-Inference"><a href="#Rational-Statistical-Inference" class="headerlink" title="Rational Statistical Inference"></a>Rational Statistical Inference</h2><p>The Bayes Theorem<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/26/Probabilistic-Graphical-Models-1/Bayes.png" alt="Bayes"></p><h3 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h3><p>joint probability(联合概率分布)</p>]]></content>
      
      
      <categories>
          
          <category> Probabilistic Graphical Model </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LDA</title>
      <link href="/2020/11/25/LDA/"/>
      <url>/2020/11/25/LDA/</url>
      
        <content type="html"><![CDATA[<h2 id="0-Prior-Knowledge"><a href="#0-Prior-Knowledge" class="headerlink" title="0. Prior Knowledge"></a>0. Prior Knowledge</h2><ul><li>Multinomial Distribution</li><li>Dirichlet Distribution</li><li>binomial Distribution</li><li>Bernoulli Distribution (0-1 Distribution)</li><li>conjugate prior (共轭分布)</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>LDA(Latent Dirichlet Allocation)是文本集合的生成概率模型。假设每个文本由话题的一个多项式分布表示，每个话题由单词的的一个多项式分布表示，特别假设文本的话题分布的先验分布是Dirichlet Distribution, 话题的单词分布的先验分布是Dirichlet Distribution。先验分布的导入使LDA能够更好地应对话题模型学习中的过拟合现象。</p><p></p><ul><li>文本 &lt;- 话题 &lt;- 单词</li><li>Prior Distribution ： Dirichlet Distribution</li><li>Distirbution: topic、words</li></ul><p>LDA 的文本集合的生成过程如下：</p><ol><li>random(<strong>topic distribution</strong>) for a text</li><li>random(<strong>topic</strong>) for every word in the text by the topic distribution</li><li>random(<strong>word</strong>) in their position by word distribution of topic</li><li>循环至结束</li></ol><p>即：</p><ol start="0"><li>(Prior Distribution):<p><br> Dirichlet Distribution -&gt; topic distribution </p><p><br>Dirichlet Distribution -&gt; word distribution </p></li><li>text :topic distribution </li><li>[topic1, topic2, … ,topicN] (topic ~ topicDistribution(Random)) </li><li>[word1, word2, … ,wordN] (word ~ wordDistribution(topic)</li></ol><h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><h3 id="模型图"><a href="#模型图" class="headerlink" title="模型图"></a>模型图</h3><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/25/LDA/%E6%A8%A1%E5%9E%8B%E5%9B%BE.png" alt="LDA的板块表示"></p><h3 id="生成过程"><a href="#生成过程" class="headerlink" title="生成过程"></a>生成过程</h3><p>给定单词集合$W$，文本集合$D$，话题集合$Z$，Dirichlet Distribution的超参数𝛼和𝛽。</p><ol><li><p>生成话题的单词分布</p><p><br>随机生成K个话题的单词分布:按照$Dir(\beta)$随机生成一个参数向量$\psi_k$, $\psi_k$～$Dir(\beta)$,话题$z_k$的单词分布即为$p(w|z_k)$</p></li><li><p>生成文本的话题分布</p><p><br>随机生成M个文本的话题分布：按照$Dir(\alpha)$随机生成一个参数向量$\theta_m$, $\theta_m$～$Dir(\alpha)$,文本$w_m$的话题分布即为$p(z|w_m)$</p></li><li><p>生成文本的单词序列</p><p></p><p>随机生成M个文本的$N_m$个单词。</p><p>(3.1) 首先按照多项分布随机生成一个话题 $z_{mn}$～$Mult(\theta_m)$ </p><p>(3.2) 然后按照多项分布随机生成一个单词，单词序列对应隐藏的话题序列。 $w_{mn}$～$Mult(\psi_{z_{mn}})$</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Probabilistic Graphical Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生成模型 </tag>
            
            <tag> LDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>array三维拼二维</title>
      <link href="/2020/11/23/array%E4%B8%89%E7%BB%B4%E6%8B%BC%E4%BA%8C%E7%BB%B4/"/>
      <url>/2020/11/23/array%E4%B8%89%E7%BB%B4%E6%8B%BC%E4%BA%8C%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<p>两个二维拼接成三维。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = x[np.newaxis, :] <span class="comment"># 先转成三维，再将两个三维拼接。</span></span><br><span class="line">y=np.vstack((y, x)) <span class="comment"># x,y 都是三维。 vstack是整体拼接</span></span><br></pre></td></tr></table></figure><p>两个矩阵拼接,多于两个是变成[a, array[a,b]]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([a, b])</span><br></pre></td></tr></table></figure><p>用list，不建议用list</p>]]></content>
      
      
      <categories>
          
          <category> 程序语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NPMM</title>
      <link href="/2020/11/19/NPMM/"/>
      <url>/2020/11/19/NPMM/</url>
      
        <content type="html"><![CDATA[<h2 id="A-nonparametric-model-for-online-topic-discovery-with-word-embeddings"><a href="#A-nonparametric-model-for-online-topic-discovery-with-word-embeddings" class="headerlink" title="A nonparametric model for online topic discovery with word embeddings"></a>A nonparametric model for online topic discovery with word embeddings</h2><p>2019 Information science</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>most online clustering models determine the probability of producing a new topic by manually setting some hyper-parameter&#x2F;threshold, which becomes barrier to achieve better topic discovery results. Moreover, topics generated by using existing models often involve a wide coverage of the vocabulary which is not suitable for online social me- dia analysis</p></blockquote><p>在线聚类模型产生一个新的主题都需要手动设置一些超参数，而且，现有主题生成但模型的主题生成经常涉及词汇表的广泛覆盖，而这不适用于在线媒体分析。</p><blockquote><p>Therefore, we propose a nonparametric model (NPMM) which exploits auxiliary word embeddings to infer the topic number and employs a “spike and slab” function to alleviate the sparsity problem of topic-word distributions in online short text analyses. NPMM can automatically decide whether a given document belongs to existing topics, measured by the squared Mahalanobis distance. </p></blockquote><p>因此我们提出了一个无参数模型（NPMM）</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><blockquote><p>Generally speaking, model-based stream clustering approaches can be categorized into two groups: batch-based models and DP-based models.（ Dirichlet process (DP)）</p></blockquote><p>现在，概率图模型被广泛用于短文本流聚类，一般基于短文本流模型聚类的方法可以分为两类：基于批处理的模型和基于DP的模型（batch-based models and DP-based models）。</p><h3 id="1-1-Batch-based-models"><a href="#1-1-Batch-based-models" class="headerlink" title="1.1 Batch-based models"></a>1.1 Batch-based models</h3><p>基于批处理的对于传统在线动态聚类模型，是应用于长文本的，不适用于短文本聚类，这些模型无法处理主题词分布中稀疏性问题；还有一些短文本聚类模型，假设每个文档仅与一种主题相关，而不与多个主题相关联，该模型迭代处理当前批次中的文档，并在新批次到达时将其丢弃。基于批处理的模型的共同限制不仅在于它们缺乏即时处理功能，而且还需要手动设置批处理的大小。此外，上述所有模型在主题推断期间都需要预定义的主题编号。而这对于动态变化主题的聚类是很难的。</p><h3 id="1-2-DP-based-models"><a href="#1-2-DP-based-models" class="headerlink" title="1.2 DP-based models"></a>1.2 DP-based models</h3><blockquote><p>To deal with the problem of the unknown topic number and the requirement of the instant processing in the short text stream clustering, topic models exploiting Dirichlet process (DP) are proposed.</p></blockquote><p>为了解决短文本聚类中未知主题数量和即时处理问题，提出了Dirichlet process。</p><blockquote><p>In essence, existing topic models cannot address the problem of inferring a proper topic number when performing the short text stream clustering. In this paper, we propose a nonparametric model (NPMM) with word embeddings for the online short document clustering. </p></blockquote><p>实质上，现有的主题模型不能address(处理)推断正确主题数量的问题。在本论文中，我们提出了NPMM，带有词嵌入的非参数模型。</p><blockquote><p>Our idea is based on the following observation: semantic relations among words  can be stable for a long period of time. Therefore, given a comprehensive semantic space learned from a large modern external corpus, we can easily infer the topics hidden in the sparse short texts of social media. More specifically, the semantic space of word embeddings can be regarded as the prior knowledge when we derive semantic relations among social media data in the stream.</p></blockquote><p>我们的idea来自于以下的观察：单词之间的语义关系是长期稳定的，因此，给定一个大型的外部现在语料库中学到的全面语义空间，我们可以轻松的推断出社交媒体稀疏短文本中的隐藏主题。具体的说，当我们从社交媒体数据流中去除语义关系时，可以将单词嵌入的语义空间视为先验知识。</p><h3 id="1-3-summarization"><a href="#1-3-summarization" class="headerlink" title="1.3 summarization"></a>1.3 summarization</h3><blockquote><p>batch-based models require manually setting both the batch size and the number of topics, and lack the instant processing capability; DP-based models need manually setting a proper concentration hyper-parameter γ to control the probability of a new latent topic generation.</p></blockquote><p>batch-based model需要手动设置batch size以及主题数量，并且缺乏立即处理的能力；Dirichlet Process-based model需要手动设置超参数来控制新的潜在主题生成的概率。而且由于短文本的长度限制，稀疏性问题存在于文档主题分布以及主题词分布中。</p><blockquote><p>In this paper we propose NPMM to address the above stated challenges, which is characterized as follows:</p></blockquote><ul><li>Online processing. NPMM has one-pass clustering process for each arriving document, which can naturally deal with the streaming data.</li><li>Nonparametric topic discovery. NPMM incorporates the word embeddings in the online topic discovery, to release the parameter settings for generating new topics in the processing.</li><li>Sparsity.</li></ul><p>NPMM 加入词嵌入用于主题发现，NPMM可以自动发现主题通过计算新到达的文档和现有主题之间的距离，并使用卡方分布计算生成新主题的概率。NPMM通过“spike and slab” function减少主题词分布的稀疏性。</p><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><blockquote><p>According to the general survey on the online data clustering in the stream clustering methods can be categorized into the following two groups: model-based stream clustering and threshold-based stream clustering. Moreover, the model-based methods can be further divided into two branches: DP-based models and batch-based models.</p></blockquote><p>根据文献中关于在线数据聚类的一般调查，流聚类方法可分为以下两类：基于模型的流聚类和基于阈值的流聚类。 此外，基于模型的方法可以进一步分为两个分支：DP-based model 和batch-based model。</p><h3 id="2-1-Model-based-stream-clustering"><a href="#2-1-Model-based-stream-clustering" class="headerlink" title="2.1 Model-based stream clustering"></a>2.1 Model-based stream clustering</h3><blockquote><p>Model-based stream clustering methods assume that documents are generated from probabilistic graphical models.</p></blockquote><p>基于模型的流聚类方法产生与概率图模型。它们通过主题词分布和文档主题分布对潜在语义空间建模，然后使用推理方法估计模型参数。</p><p><br>batch-based model: 经典LDA变体模型，当存在一定的不足，即它们设置了固定的主题数量（fixed-number),固定数字设置不适用于数据流聚类，无法处理主题演化问题。因此提出了DP-based model。</p><p><br>DP-based model: 应用广泛，但当前基于DP的模型的局限性在于，潜在主题生成的参数是手动设置的，并且搜索该参数的适当值非常耗时。</p><h3 id="2-2-Threshold-based-stream-clustering"><a href="#2-2-Threshold-based-stream-clustering" class="headerlink" title="2.2 Threshold-based stream clustering"></a>2.2 Threshold-based stream clustering</h3><p>基于阈值的流聚类方法的主要局限性在于，可以手动设置阈值以确定在线文档的主题分配。例如，新到达的文档通过将距离与预定义的阈值进行比较来选择最近的群集或新的群集。但是，在不同的数据集中搜索适当的阈值是一项耗时的操作。</p><h2 id="3-NPMM"><a href="#3-NPMM" class="headerlink" title="3. NPMM"></a>3. NPMM</h2><h3 id="3-1-1-Representative-terms"><a href="#3-1-1-Representative-terms" class="headerlink" title="3.1.1 Representative terms"></a>3.1.1 Representative terms</h3><p>NPMM使用词嵌入在全局语义空间中构造发现主题的多元高斯分布。当出现新文档时，我们可以通过计算该文档与高斯分布之间的距离来获得生成新主题的概率。但是，仅使用现有主题中的所有单词来构建基于单词嵌入的全局高斯分布可能会带来很大的噪音，因为有些单词与其主题之间的联系较弱。</p><blockquote><p>microsoft sony battle gaming supremacy holiday season</p></blockquote><p>其中holiday, season也是主题单词，但是核心是sony battle gaming，Therefore, we select “microsoft sony battle gaming supremacy” as the representative terms of this sentence.<br>我们可以如下获得主题中的representative terms:<br>$$\beta_{z,w} \sim Bernoulli(\lambda_{z,w})$$  </p><p>$$\lambda_{z,w} &#x3D; \frac{p(w|z)}{max_{w’}p(w’｜z)}, \qquad {\forall} w’\in z \tag{1}$$</p><p>$\beta_{z,w}$ 表示单词w是否是主题z中的代表词，$\lambda_{z,w}$与<em>“给定单词w的条件单词概率与$p(w’| z)$中的最大值即主题z的最大单词概率的比值”</em>相关。</p><h3 id="3-1-2-Global-semantic-space"><a href="#3-1-2-Global-semantic-space" class="headerlink" title="3.1.2 Global semantic space"></a>3.1.2 Global semantic space</h3><blockquote><p>The global semantic space G is represented as a multivariate normal distribution in the word embedding space, which<br>is constructed by the representative terms from each topic during the clustering. </p></blockquote><p>全局语义空间G表示为单词嵌入空间中的多元正态分布.</p><blockquote><p>In the scenario of social media streams, the newly coming topic is often outside the convex of those discovered topics, to be free of parameters and make the computation simple, we use just only one multivariate Gaussian to represent those discovered topics. </p></blockquote><p>在社交媒体流的场景中，新出现的主题通常不在那些发现的主题的凸面之外，因为没有参数并且简化了计算，我们仅使用一个多元高斯来表示那些发现的主题。</p><p>When a new document d comes, the squared Mahalanobis distance between each word w in d and G is calculated as follows:<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/%E5%85%AC%E5%BC%8F5.png" alt="global semantic space G"></p><h3 id="3-1-3-The-spike-and-slab-priors"><a href="#3-1-3-The-spike-and-slab-priors" class="headerlink" title="3.1.3 The spike and slab priors"></a>3.1.3 The spike and slab priors</h3><blockquote><p>The spike and slab priors introduced to topic modeling are able to address the sparsity of topic-word distribution by using auxiliary Bernoulli variables to present the “on” and “off” of the priors, which indicate whether or not a term is selected by a topic as the representative term (or called the focused word). The selection process is given in Eq. (1). As in the real-world scenarios of short texts, a topic covers a narrow range of terms instead of a wide coverage of the vocabulary. It is hard for the sampling algorithm to distinguish relevant words from the irrelevant ones due to the small difference in word frequencies of short texts.</p></blockquote><p>引入主题建模的spike and slab priors能够通过使用辅助Bernoulli变量来表示先验的“开”和“关”来解决主题词分布的稀疏性。</p><h3 id="3-1-4-The-cluster-feature-CF-vector"><a href="#3-1-4-The-cluster-feature-CF-vector" class="headerlink" title="3.1.4 The cluster feature (CF) vector"></a>3.1.4 The cluster feature (CF) vector</h3><p>The cluster feature vector is used to represent a cluster, along with an addible property(可添加属性) and a deletable property(可删除属性).</p><p>CF vector for a cluster z is defined as a tuple <strong>(${n_z^w}, m_z, n_z$)</strong>, $n_z^w$是单词w在cluster类z中出现的次数(number of occurrences of word w), $m_z$是cluster z中documents的数量, $n_z$是the number of words in cluster z.我们用$D$表示记录文档(recorded documents)的集合，$V$表示记录文档的词汇的集合(we denote $V$ as the set of vocabulary of the recorded documents).</p><ul><li>(a). Addible proberty</li></ul><blockquote><p>When a new document d comes, it will be firstly assigned to a topic z in the one-pass(遍历) clustering process, the detail is given in the later section. Then, the CF vector and the recorder sets($i.e.$, $D$ and $V$) are updated in the following way:</p></blockquote><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/%E5%85%AC%E5%BC%8F3.png" alt="addible property"></p><p>当有一个新文档d时，将在一次遍历聚类过程中首先将其分配给主题z，详细信息在later section, 然后更新CF vector 和 $D$、$V$。</p><ul><li>(b). Deletable property.<blockquote><p>Accordingly, when a document d is deleted from z in the update clustering process, the CF vector and the recorded sets are updated as follows:</p></blockquote></li></ul><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/%E5%85%AC%E5%BC%8F4.png" alt="deletable property"></p><p>After defining the above auxiliary components, the major tasks of NPMM given a data stream in a certain time range can be defined as(即当给定一个确定时间段数据流时，NPMM的主要任务如下)：</p><ol><li>Sample a set of representative terms from the existing clusters(or called topics);从已有聚类（或主题）中采样有代表性的terms(术语)。</li><li>Construct a semantic space G with the representative terms using a multivariate normal distributions.使用多元正态分布构造具有representative terms的语义空间G。</li><li>Calculate the probability of a newly arriving document d belong to the semantic G. 计算新到达文档d属于语义空间G的概率。</li><li>Learned the topic-word distribution $\psi$ and document-topic distribution $\theta$, respectively(各自地). 分别学习主题词分布$\psi$和文档主题分布$\theta$。</li></ol><p>All the notations used in this paper are summarized in table 1.</p><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/%E7%AC%A6%E5%8F%B7%E8%A1%A8%E7%A4%BA.png" alt="符号表示 table 1"></p><h3 id="3-2-Generative-process"><a href="#3-2-Generative-process" class="headerlink" title="3.2 Generative process"></a>3.2 Generative process</h3><p>在实践中，each document only focuses on one topic in short texts很有效，所以本文模型也遵循该假设。The graphical model of the proposed NPMM is given in Fig.1. </p><blockquote><p>In the generative process, we use the inverse Wishart distribution<a href="#refer-anchor"><sup>1</sup></a> as the conjugate prior  (共轭先验)<a href="#refer-anchor"><sup>2</sup></a> for the covariance(协方差) matrix of a multivariate normal of distrubution.In Bayesian statistics, it is widely used for estimating a multivariate normal distribution with unknown mean and covariance matrix.</p></blockquote><p>在生成过程中，我们使用Wishart逆分布<a href="#refer-archor"><sup>1<sup></sup></sup></a>用作多元正态分布协方差矩阵的共轭先验。在贝叶斯统计中，它被广泛用于估计均值和协方差矩阵未知的多元正态分布。</p><blockquote><p>Since a document is just related to one topic, there are two possible statuses for each newly arriving document in the clustering, either relevant or irrelevant to the existing global semantic space G.This status variable is denoted by $r$. In addition, $λ_{z,w}$ is as defined in $Eq. (1)$ and other notations are summarized in Table 1.</p></blockquote><p>The generative process is presented as follows:<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/model.png" alt="model"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/NPMM.png" alt="Fig. 1"></p><ol><li>$Draw \   \Sigma_0 \sim W^{-1}(\Psi, v)$.</li><li>$Draw \  \mu_0 \sim N(\mu, \frac{1}{\tau}\Sigma_0)$.</li><li>$Draw \  a \  topic \  distribution \ \theta \sim Dirichlet(x)$</li><li>$For \  each \  term \ z \in {1,2,…,|z|}$:<br>$\quad(a) \  For\ each \ term \ w \in {1,2,…,|V|}$:<br>$\qquad i.\ Draw \ a \ focused \ term \ \beta_{z,w} \sim Bernoulli(\lambda_{z,w})$<br>$\quad(b)\ Draw\ a\ word\ distribution\ \psi_z \sim\ Dirichlet(\delta\beta_z +\epsilon1),\beta_z &#x3D;{\beta_z,w}_{w&#x3D;1}^{|V|}.$</li><li>$For \ each \ document \ d \in {1,2,…}$:<br>$\quad (a)\ Draw \ \eta_d \sim \chi_{dim}^2(\mu_0, \Sigma_0)$.<br>$\quad (b)\  Draw \ relevance \ r\  base \  on\  the\  representative \ term \ indicator \ \pi \ and\  p(\eta_d)\ (refer \ to\ Eqs.(3)\ and\ (4)).$<br>$\quad (c)\ If\ the\ document\ is\ relevant\ to\ the\ global\ semantic\ space\ G,\ i.e.,\ r\ &#x3D;\ 1$:<br>$\qquad i.\ Draw\ a\ topic\ z\ \sim \ Multinomial(\theta).$<br>$\qquad ii.\ Emit\ a\ word\ w \sim Multinomial(\psi_z).$<br>$\quad (d)\ If\ the\ document\ is\ irrelevant\ to\ the\ global\ semantic\ space\ G,\ i.e.,\ r\ &#x3D;\ 0$:<br>$\qquad i.\ Create\ a\ new\ topic\ z.$</li></ol><p>Here is a example:</p><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/example.png" alt="example"></p><h3 id="3-3-The-spark-and-slab-priors"><a href="#3-3-The-spark-and-slab-priors" class="headerlink" title="3.3 The spark and slab priors"></a>3.3 The spark and slab priors</h3><blockquote><p>When document d is sampled as relevant to the global semantic space G, we draw a topic assignment for it and apply the spike and slab priors on its word generation. The key idea of employing the priors is to restrict the size of the word simplex over Dirichlet distribution to reduce the sparsity in the topic-word distribution by incorporating a Bernoulli variable.</p></blockquote><p>当文档d被采样为语义空间G相关时，我们为其绘制一个主题分配（topic assignment），并在单词生成过程中应用<strong>spark and slab priors</strong>。采用先验的关键思想是通过合并Bernoulli变量来限制单词在Dirichlet分布上的大小，以此来较少主题单词分布的稀疏性（the sparsity in the topic-word distribution)。</p><p>The spark prior $\delta$ is much larger than the slab prior $\epsilon$, that is <strong>$\delta \gg \epsilon$</strong>. The representative term indicator $\beta_{z,w}$ serves as a switch of “on” and “off” to determine which term is a representative term,即 <strong>$\beta_{z,w}$用作指示器，确定哪个term是代表性的</strong>。</p><p>In the previous case, when $r &#x3D; 1$, a topic $z$ is sampled from $\theta$, and a word is emitted from $\psi_z \sim Dirichlet(\delta\beta_{z}+\epsilon1)$. This does not violate(违反) the definition of a multinomial distribution(多项式分布) as $\sum_{w&#x3D;1}^{|V|}\psi_{z,w}&#x3D;1$ where $|V|$ is the size of the vocabulary of current recorded documents. 当r&#x3D;1时，从$\theta$采样$z$, 从$\psi_z \sim Dirichlet(\delta\beta_{z}+\epsilon1)$采样出单词。$|V|$是当前记录文档的词汇量。</p><blockquote><p>The spike-and-slab prior setting w&#x3D;1 enables the model to generate more coherent topics. Besides, when r &#x3D; 0 that d is sampled as irrelevant to G, we create a new topic for d and also update the global space G as well.</p></blockquote><p>spike-and-slab prior预先设置w &#x3D; 1使模型能够生成更连贯的主题。此外，当r &#x3D; 0时，采样到的d与G不相关，我们为d创建了一个新主题，并且还更新了全局空间G。</p><h3 id="3-4-The-NPMM-algorithms"><a href="#3-4-The-NPMM-algorithms" class="headerlink" title="3.4 The NPMM algorithms"></a>3.4 The NPMM algorithms</h3><blockquote><p>The details of the clustering process for the proposed NPMM model are shown in Algorithms 1 and 2 . The meanings of the notations are as given in Table 1. As mentioned in the introduction section, NPMM has one-pass clustering process and update clustering process. The one-pass scheme (lines 5–8 in Algorithm 1) grants NPMM the instant processing capability to handle the massive amount of short text streams. And the update clustering scheme (or called batch scheme) (lines 9–25 in Algorithm 1) enables NPMM to achieve a better performance through multiple iterations. In addition, Algorithm 2 is a common operation for sampling the latent variables (details in the following section) in NPMM.</p></blockquote><p>算法1和2中显示了所提出的NPMM模型的聚类过程的详细信息。 表1给出了这些符号的含义。如引言部分所述，NPMM具有一遍聚类过程和更新聚类过程。 one-pass方案（算法1中的第5-8行）授予NPMM即时处理能力，以处理大量的短文本流。 更新聚类方案（或称为批处理方案）（算法1中的9–25行）使NPMM可以通过多次迭代获得更好的性能。 另外，算法2是用于在NPMM中对潜在变量（下一节中的详细信息）进行采样的通用操作。</p><h2 id="4-Inference"><a href="#4-Inference" class="headerlink" title="4. Inference"></a>4. Inference</h2><p>对参数推理使用Gibbs sampling<a href="#refer-anchor"><sup>3</sup></a>。模型中有5个latent variables，包括topic assignments $z$, relevance indicator $r$, the posterior normal distribution parameters $\mu_0$ and $\Sigma_0$, and the focused terms $\beta_z$ in each topic.</p><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/%E5%85%AC%E5%BC%8F7.png" alt="公式7"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/A1.png" alt="Algorithm1"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/S1.png" alt="Sampling1"> <img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/s1_2.png" alt="Sampling2"></p><h3 id="4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition"><a href="#4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition" class="headerlink" title="4.1. Reduction of the sampling complexity with cholesky decomposition"></a>4.1. Reduction of the sampling complexity with cholesky decomposition</h3><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/4.png" alt="4"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/4_1.png" alt="4.1"></p><h3 id="4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step"><a href="#4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step" class="headerlink" title="4.2. Improved reduction of the sampling complexity with metropolis hastings step"></a>4.2. Improved reduction of the sampling complexity with metropolis hastings step</h3><blockquote><p>The Metropolis Hastings (MH) algorithm is a Markov chain Monte Carlo method for obtaining a certain number of random samples from a stale probability distribution. The precondition of using MH is that the objective distribution changes relatively slowly and its changing distribution is similar to the stale one. From the observation of the parameters ($\mu_0$ and $\Sigma_0$) in the global space G, they do not change drastically during the clustering. One possible explanation is that in the real world, the number and the content of topics are relatively stable in a certain time window (e.g., people always like to talk about the latest news on social media platforms). We can exploit this observation and employ MH step in our sampling process (i.e., obtaining a few samples with the stale distribution of the global space G).</p><p><br>Therefore, combining Cholesky decomposition with the MH step, the sampling complexity of calculation can be brought down from $O(IDn^2)$ to $O(I\frac{D}{H}n^2)$, where H represents the number of MH steps used in the clustering and $\frac{D}{H} \ll D$. The experiment results (i.e., sampling naively, with Cholesky decomposition (CH), with CH+MH step) of the running time will be demonstrated in the following experiment section.</p></blockquote><p>Metropolis Hastings（MH）<a href="#refer-anchor"><sup>4</sup></a>算法是马尔可夫链蒙特卡罗方法，用于从陈旧的概率分布中获取一定数量的随机样本。使用MH的前提是目标分布变化相对较慢，并且其变化分布与陈旧状态相似。从全局空间G中的参数（μ0和􏰉0）观察，它们在聚类期间不会发生剧烈变化。一种可能的解释是，在现实世界中，主题的数量和内容在一定的时间范围内相对稳定（例如，人们总是喜欢在社交媒体平台上谈论最新新闻）。我们可以利用这一观察结果并在我们的采样过程中采用MH步骤（即获取一些具有全球空间G的陈旧分布的样本）。<br>因此，将Cholesky分解与MH步结合起来，可以将计算的采样复杂度从O$O(IDn^2)$ 降低到$O(I\frac{D}{H}n^2)$，其中H代表在聚类中使用的MH步数和$\frac{D}{H} \ll D$。运行时间的实验结果（即，通过Cholesky分解（CH），CH + MH步进行天真采样）将在以下实验部分中演示。</p><h2 id="5-Experiment"><a href="#5-Experiment" class="headerlink" title="5. Experiment"></a>5. Experiment</h2><h3 id="5-1-Datasets"><a href="#5-1-Datasets" class="headerlink" title="5.1 Datasets"></a>5.1 Datasets</h3><p>Two real-world datasets from Google News and Twitter, and two variants of them are used in the experimental study.<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/19/NPMM/dataset.png" alt="dataset"></p><blockquote><p>Google News <a href="https://news.google.com/news/">https://news.google.com/news/</a><br>TweetSet <a href="https://trec.nist.gov/data/microblog.html">https://trec.nist.gov/data/microblog.html</a></p></blockquote><h2 id="6-Conclustion-and-future-work"><a href="#6-Conclustion-and-future-work" class="headerlink" title="6. Conclustion and future work"></a>6. Conclustion and future work</h2><blockquote><p>In this paper, we have proposed a nonparametric topic model (NPMM) with auxiliary word embeddings for online topic discovery. NPMM can discover a new topic by computing the probabilities of a document belonging to the existing topics and a new one. NPMM can achieve the state-of-the-art performance of online clustering with one-pass process, and can have even better performance with multiple iterations. Moreover, after obtaining the representative terms from each topic, NPMM can exploit the spike and slab priors function to amplify the contrast of word generation probabilities between the relevant words and the irrelevant ones, which alleviates the sparsity problem of the topic-word distribution in the short text clustering. In addition, in order to speed up the sampling process, we have proposed two improved sampling methods (i.e., NPMM with CH and NPMM with CH+MH step) to compare with the naive sampling one. Our extensive experimental study has shown that NPMM with CH+MH100 step can achieve similar performance in less time compared with the naive sampling method on real-life datasets.</p></blockquote><p>In the future work, we intend to exploit NPMM to improve the performance of other text mining applications, such as event detection, search result diversification, and text classification.</p><div id="refer-anchor"></div><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[0] A, Junyang Chen , Z. G. A , and W. L. B . “A nonparametric model for online topic discovery with word embeddings.” Information sciences 504(2019):32-47.<br><a href="/2020/11/30/Wishart-%E5%88%86%E5%B8%83%E5%8F%8A%E9%80%86%E5%88%86%E5%B8%83/">[1] Wishart分布与Wishart逆分布</a><br><a href="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/">[2] 共轭分布</a><br>[3] H. Amoualian, M. Clausel, E. Gaussier, M.-R. Amini, Streaming-LDA: A copula-based approach to modeling topic dependencies in document streams, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2016, pp. 695–704. (Gipps Sampling)<br>[4] S. Chib, E. Greenberg, Understanding the metropolis-hastings algorithm, Am. Stat. 49 (4) (1995) 327–335. (MH马尔可夫链蒙特卡罗方法)</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dynamic Clustering </tag>
            
            <tag> 生成模型 </tag>
            
            <tag> Clustering Topic model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dynamic Clustering of Streaming Short</title>
      <link href="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/"/>
      <url>/2020/11/16/Dynamic-Clustering-of-Streaming-Short/</url>
      
        <content type="html"><![CDATA[<h2 id="Dynamic-Clustering-of-Streaming-Short-Documents"><a href="#Dynamic-Clustering-of-Streaming-Short-Documents" class="headerlink" title="Dynamic Clustering of Streaming Short Documents"></a>Dynamic Clustering of Streaming Short Documents</h2><p>KDD 2016</p><h2 id="Abstract："><a href="#Abstract：" class="headerlink" title="Abstract："></a>Abstract：</h2><blockquote><p>In this paper, we consider the problem of dynamically clustering a streaming corpus of short document.The short length of documents makes the inference of the latent topic distribution challenging, while the temporal dynamics of streams allow topic distributions to change over time.To tackle these two challenges we propose a new dynamic clustering topic model - DCT - that enables tracking the time-varying distributions of topics over documents and words over topics. DCT models temporal dynamics by a short-term or long-term dependency model over sequential data, and overcomes the difficulty of handling short text by assigning a single topic to each short document and using the distributions inferred at a certain point in time as priors for the next inference, allowing the aggregation of information. At the same time, taking a Bayesian approach allows evidence obtained from new streaming documents to change the topic distribution. Our experimental results demonstrate that the proposed clustering algorithm outperforms state-of-the-art dynamic and non-dynamic clustering topic models in terms of perplexity and when integrated in a cluster- based query likelihood model it also outperforms state-of-the-art models in terms of retrieval quality.</p></blockquote><p>短文本流的动态聚类，有两个难点，一是文本流（streaming）的主题随着时间是变化，二是短文的长度较短，使得主题的潜在分布推断变难。论文提出了一个新的模型DCT。 DCT通过对顺序数据的短期或长期依赖性模型对时间动态进行建模，并为每个简短文档分配一个主题，并使用在某个时间点推断的分布作为下一个推断的先验，从而可以进行信息汇总。采用贝叶斯方法可以使从新的流式传输文档中获得的证据改变主题分布。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>两个挑战：1.主题分布随时间消失 2.短文本单词有限，不易推出其主题分布。通过引入collapse Gibbs sampling应对挑战</p><p></p><blockquote><p>One of the key challenges in clustering streaming data is the dy- namic nature of topics (or clusters): topic distributions change with time, with previously salient topics “fading-off” and vice versa.</p></blockquote><p>流数据聚类的挑战之一：topic distributions随着时间改变，previously salient(显著) topic 会“fading-off”逐渐消失。</p><blockquote><p>However, most of the previous work makes the assumption that the content of documents is rich enough to infer a per-document multinomial distribution of topics. The second key challenge in clustering streaming data is that this assumption does not hold for short text, as the number of words in each document is limited, which prohibits the accurate inference of a topic distribution over the document. </p></blockquote><p>流数据聚类的另一个挑战:先前工作都是假设文档具有足够的内容，一次推断出主题的多项式分布。而短文本流单词是有限的，不易推出其主题分布。</p><blockquote><p>Our method tackles the two challenges by introducing a collapsed Gibbs sampling algorithm that (a) assigns a single topic to all the words of a short document, and (b) uses the inferred topic distribution of past documents as a prior of the topic distribution of the current documents, while at the same time allowing new evidence (newly streamed documents) to change the posterior distribution of topics. Based on the exact definition of the prior the proposed model enables both short-term and long-term dependencies between the previously and currently inferred distributions.</p></blockquote><p>通过引入collapsed Gipps samping algorthm解决这两个挑战: a）对短文本的每个单词都分配一个主题；b）使用推断的过去文档的主题分布作为当前文档主题分布的先验，同时允许新evidence（新流式文档）更改主题的后验分布。基于对先验的精确定义，所提出的模型可以实现先前和当前推断的分布之间的短期和长期依赖性。</p><p>本文工作：</p><ul><li>Dynamic Dirichlet Multinomial Mixture Model that captures short and long term temporal dependencies, tracking dynamic topic distributions over short document streams.捕获短期和长期的时间依赖性，跟踪短文档流上的动态主题分布。</li><li>collapsed Gibbs sampling algorithm for DDMMM to infer the changes in topic and document probability distributions.为动态Dirichlet多项式混合模型提出了一种折叠的Gibbs采样算法，以推断主题和文档概率分布的变化</li><li>通过聚类提高检索性能</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>topic modeling and clustering (主题建模和聚类)：LDA与Kmeans等。<br>在短文本聚类中的表现良好的模型 Dirichlet multinomial mixture clustering model。但这些模型都是假设文本是长文本的。</p><h2 id="3-Task-Description"><a href="#3-Task-Description" class="headerlink" title="3. Task Description"></a>3. Task Description</h2><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/Table1.png" alt="符号说明"><br>The task we address in this work is the clustering of short text streaming documents, with clusters changing dynamically, as new documents stream in.要解决的任务是对短文本流文档进行聚类，其中聚类随着新文档的流入而动态变化。</p><p>动态聚类本质是在条件f下完成的函数：<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F1.png" alt="动态聚类本质"><br>其中d≤t代表文档流，其中d’t是在时间t到达的最新短文档集，而c≤t是直到时间t的文档簇的结果集，而c’z是 ct中的第z个群集，Z中的群集总数。 d’t包含一组简短的文本文档，每个文档d由出现在d中的一系列单词表示，这些单词来自词汇V &#x3D; {v1，v2，。 。 。 ，vV}。 我们假设d的长度不超过预定义的小长度（例如，对于Twitter，为140个字符）。 为了简洁起见，在本文的其余部分中，我们分别用dt和ct表示d≤t和c≤t。</p><h2 id="4-Dynamic-Clustering-Model"><a href="#4-Dynamic-Clustering-Model" class="headerlink" title="4. Dynamic Clustering Model"></a>4. Dynamic Clustering Model</h2><p>介绍DCT(Dynamic Clstering Topic)模型，aiming at the effective clustering of short document streams.</p><h3 id="4-1-Preliminaries"><a href="#4-1-Preliminaries" class="headerlink" title="4.1 Preliminaries"></a>4.1 Preliminaries</h3><blockquote><p>The goal of the dynamic clustering topic model is to infer the dynamically changing topic distribution and document distribution over topics at any given time t. That is, we want to infer the temporal word probability for a topic, P (v|t, z), and the temporal topic probability over a document, P(z|t,d).<br>…<br>our proposed Gibbs sampling as it will be described in the following sections  assigns a single topic to all the words in each short document.</p></blockquote><p>动态主题聚类模型的目的是在任何给定时间t推断主题上的动态变化主题分布和文档分布。即 单词分布P(v|t, z)与主题分布P(z|t,d)。由于短文本单词是有限的，采用Gipps sampling为每个简短文档中的所有单词分配一个主题。<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F%E8%AF%B4%E6%98%8E1.png" alt="公式"><br>当涉及数据流设置时，在（1）和（2）中所做的假设是不现实的，其中时间t的分布取决于过去的分布。 在以下小节中，我们通过对短期依赖关系（第4.2节）和长期依赖关系（第4.3节）建模来推断Θt和Φt。</p><h3 id="4-2-Short-term-Dependency-DCT"><a href="#4-2-Short-term-Dependency-DCT" class="headerlink" title="4.2 Short-term Dependency DCT"></a>4.2 Short-term Dependency DCT</h3><h4 id="Modeling-short-term-dependency"><a href="#Modeling-short-term-dependency" class="headerlink" title="Modeling short-term dependency"></a>Modeling short-term dependency</h4><blockquote><p>we propose a short-term-dependency DCT model. According to this model, the topic distribution at time t remains the same as the one at time t − 1 if no new documents are observed, while it is updated on the basis of new evidence when a new set of documents is observed at time t.</p></blockquote><p>根据此模型，如果未观察到新文档，则时间t的主题分布与时间t-1的主题分布保持相同，而在时间t观察到一组新文档时，则根据新证据对主题分布进行更新。将公式1中的k分解为，k&#x3D;theta(t-1)*αt,其中精度值α(t,z)表示主题持久性，即与时间t-1相比，主题t在时间t的显着性。分布是多项式分布的共轭形式，因此可以通过Gipps sampling进行推断。<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F2.png" alt="公式"></p><p>以类似的方式，为了模拟特定于主题z的单词的名词分布的动态变化，我们假设Dirichlet先验，其中当前分布Φt的均值从先前分布Φt-1的均值演化为 精度为βt，<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F3.png" alt="公式"></p><p>Dynamic Dirichlet Multinomial Mixture Model是由t-1时刻的主题分布和单词分布决定的生成主题模型。可以初始化这二者0时刻的分布，对t时刻文档的生成过程使用Gipps samplin对参数进行估计。<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F4.png" alt="公式"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F5.png" alt="公式"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E6%A8%A1%E5%9E%8B%E5%9B%BE1.png" alt="Dynamic Dirichlet Multinomial Mixture Model"></p><h4 id="Inference-for-the-Dynamic-Dirichlet-Multinomial"><a href="#Inference-for-the-Dynamic-Dirichlet-Multinomial" class="headerlink" title="Inference for the Dynamic Dirichlet Multinomial"></a>Inference for the Dynamic Dirichlet Multinomial</h4><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E7%AE%97%E6%B3%951.png" alt="Inference for the Dynamic Dirichlet Multinomial"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F5.5.png" alt="公式"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F6.png" alt="公式"></p><h3 id="4-3-Long-term-Dependency-DCT"><a href="#4-3-Long-term-Dependency-DCT" class="headerlink" title="4.3 Long-term Dependency DCT"></a>4.3 Long-term Dependency DCT</h3><p>分布Θt和Φt取决于先前的时间步长分布。 研究表明，主题分布或用户在搜索信息时对主题的兴趣可能取决于较长的时间步长历史。 我们根据分布先验对这种长期（L步）依赖性DCT模型进行建模。<br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F6.6.png" alt="公式"><br><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/%E5%85%AC%E5%BC%8F7.png" alt="公式"></p><h2 id="4-Clustering"><a href="#4-Clustering" class="headerlink" title="4. Clustering"></a>4. Clustering</h2><p><img src= "/img/loading.gif" data-lazy-src="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/C.png" alt="Clustering"></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dynamic Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++(2)--vector</title>
      <link href="/2020/01/05/c-2-vector/"/>
      <url>/2020/01/05/c-2-vector/</url>
      
        <content type="html"><![CDATA[<h4 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h4><p>使用:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;vector&gt;;</span><br></pre></td></tr></table></figure><h4 id="定义"><a href="#定义" class="headerlink" title="定义:"></a>定义:</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; ivec;//ivec保存int类型的对象</span><br><span class="line">vector&lt;Sale_item&gt; Sale_vec;//Sale_vec保存Sale_item类型对象</span><br><span class="line">vector&lt;vector&lt;string&gt;&gt;file;</span><br></pre></td></tr></table></figure><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;T&gt; v1;</span><br><span class="line">vector&lt;T&gt; v2(v1);</span><br><span class="line">vector&lt;T&gt; v2=v1;</span><br><span class="line">vector&lt;T&gt; v3(n,val);//v3包含了n个重复的元素，每个元素的值都是val</span><br><span class="line">vector&lt;T&gt; v4(n);//v4包含n个重复地执行了值初始化的对象</span><br><span class="line">vector&lt;T&gt; v5&#123;a,b,c,...&#125;;</span><br><span class="line">vector&lt;T&gt; v5 = &#123;a,b,c,...&#125;;</span><br></pre></td></tr></table></figure><h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">v.empty();</span><br><span class="line">v.size();</span><br><span class="line">v.push_back(t);//v尾端加入t</span><br><span class="line">v[n];//返回v中第n个位置上元素的引用0到n-1,必须访问确知已存在的元素,若v[0]不存在元素，访问v[0]，则出错。（error:buffer overflow)</span><br><span class="line">v1 = v2;</span><br><span class="line">v1 = &#123;a,b,c,...&#125;;</span><br></pre></td></tr></table></figure><h4 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h4><p>迭代器用来间接访问容器类型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//b表示v的第一个元素，e表示v的尾元素的下一位置，v为空时，begin和end返回同一个迭代器</span><br><span class="line">auto b = v.begin(), e = v.end();</span><br></pre></td></tr></table></figure><p>迭代器类型：iterator、const_interator</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt;::iterator it;  //it能读写vector&lt;int&gt;的元素</span><br><span class="line">string::iterator it2;  //it2能读写string类型元素，对应begin、end</span><br><span class="line">vector&lt;int&gt;::const_iterator it3;//只能读不能写</span><br><span class="line">vector&lt;int&gt;::const_iterator it4;//对应cbegin、cend</span><br></pre></td></tr></table></figure><p>*it.empty();<br>(*it).empty()等于it-&gt;empty();</p><p><br>迭代器访问：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for(auto it = text.cbegin(); it != text.cend(); it++)&#123;</span><br><span class="line">    cout &lt;&lt; *it &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>迭代器实现二分搜索：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">auto beg = text.begin(),end = text.end();</span><br><span class="line">auto mid = text.begin() + (end-beg)/2;</span><br><span class="line">while(mid != end &amp;&amp; *mid != target)&#123;</span><br><span class="line">    if(target&lt; *mid)</span><br><span class="line">        end = mid;</span><br><span class="line">    else</span><br><span class="line">        beg = mid+1;</span><br><span class="line">    mid = beg + (end - beg)/2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 程序语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> vector </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>泰戈尔诗集</title>
      <link href="/2020/01/01/%E4%B9%A6%E9%A6%99-%E6%B3%B0%E6%88%88%E5%B0%94%E8%AF%97%E9%9B%86/"/>
      <url>/2020/01/01/%E4%B9%A6%E9%A6%99-%E6%B3%B0%E6%88%88%E5%B0%94%E8%AF%97%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h4 id="小小的长笛"><a href="#小小的长笛" class="headerlink" title="小小的长笛"></a>小小的长笛</h4><ul><li>1.你一再倒空我的心杯，又一再斟满崭新的生命。<p></p></li><li>2.这管小小的长笛，你带着它翻山越谷，吹出永远新鲜的旋律。</li></ul><h4 id="纯洁"><a href="#纯洁" class="headerlink" title="纯洁"></a>纯洁</h4><ul><li>1.我要永远在思想中屏除谎言，因我知道，你就是真理，点燃我心中的理性之火。<p></p></li><li>2.今日，夏天带着叹息与低语，来到我的窗前；蜜蜂如吟游使人般，在花果园林中娴熟地吟唱。<p><br>此刻,当是静静坐在你身旁，和你面对面，在这宁静与无限的闲暇里吟唱除生命的献歌。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 泰戈尔诗集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理想国</title>
      <link href="/2020/01/01/%E4%B9%A6%E9%A6%99-%E7%90%86%E6%83%B3%E5%9B%BD/"/>
      <url>/2020/01/01/%E4%B9%A6%E9%A6%99-%E7%90%86%E6%83%B3%E5%9B%BD/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>C++(1)</title>
      <link href="/2020/01/01/C-1/"/>
      <url>/2020/01/01/C-1/</url>
      
        <content type="html"><![CDATA[<h4 id="const-constexpr限定符"><a href="#const-constexpr限定符" class="headerlink" title="const constexpr限定符"></a>const constexpr限定符</h4><p>const常量：无法修改，定义时必须初始化</p><p><br>constexpr 常量表达式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">constexpr int m = 20;</span><br><span class="line">constexpr int s = m+1;</span><br></pre></td></tr></table></figure><h4 id="auto类型"><a href="#auto类型" class="headerlink" title="auto类型"></a>auto类型</h4><p>auto类型定义的变量必须有初始值，由编译器判断类型</p><h4 id="decltype类型"><a href="#decltype类型" class="headerlink" title="decltype类型"></a>decltype类型</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decltype(f()) sum = x;//sum的类型是函数f的返回类型</span><br></pre></td></tr></table></figure><h4 id="自定义数据结构"><a href="#自定义数据结构" class="headerlink" title="自定义数据结构"></a>自定义数据结构</h4> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">struct 类名&#123;</span><br><span class="line">    类体</span><br><span class="line">&#125;别名1，别名2;</span><br></pre></td></tr></table></figure><h4 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h4><p>输入cin &gt;&gt; data;输出cout &lt;&lt; data;</p><h4 id="编写头文件-h"><a href="#编写头文件-h" class="headerlink" title="编写头文件.h"></a>编写头文件.h</h4> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#ifndef SALES_DATA_H </span><br><span class="line">#define SALES_DATA_H</span><br><span class="line">#include \&lt;string&gt; </span><br><span class="line">struct Sales_data&#123; </span><br><span class="line">    std::string bookNo; </span><br><span class="line">    unsigned units_sold=0; </span><br><span class="line">    double revenue =0.0; </span><br><span class="line">&#125;; </span><br><span class="line">#endif </span><br></pre></td></tr></table></figure><h4 id="标准库类型string"><a href="#标准库类型string" class="headerlink" title="标准库类型string"></a>标准库类型string</h4><p>使用:<br> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#include \&lt;string&gt;</span><br><span class="line">string s5=&quot;hiya&quot;;</span><br><span class="line">string s6(&quot;hiya&quot;);</span><br><span class="line">string s7(10,&#x27;c&#x27;);//s7的内容是cccccccccc</span><br></pre></td></tr></table></figure><br>操作:<br> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">getline(is,s);//从is中读取第一行赋给s，返回is;</span><br><span class="line">s.empty();</span><br><span class="line">s.size();</span><br><span class="line">s[n];//返回s中第n个字符的引用，位置n从0计数；</span><br><span class="line">s1+s2;//返回s1+s2连接的结果</span><br><span class="line">s1=s2;//s1=s2;</span><br></pre></td></tr></table></figure><br>读写string对象：</p><p><br>读取时，string对象会自动忽略开头的空白，从第一个真正字符读起，直至遇见下一处空白为止。<br> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cin &gt;&gt; s1; //输入  hello world; 实际上s1=&quot;hello&quot;;</span><br></pre></td></tr></table></figure><br>如果希望保留读取空格，使用getline();触发getline()返回的是换行符。</p><p><br>for循环<br> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">格式：</span><br><span class="line">for(declaratin: expression)&#123;</span><br><span class="line">    statement;</span><br><span class="line">&#125;</span><br><span class="line">string str(&quot; some string&quot;);</span><br><span class="line">for(auto c : str) //打印str</span><br><span class="line">   cout &lt;&lt; c;</span><br></pre></td></tr></table></figure>   </p>]]></content>
      
      
      <categories>
          
          <category> 程序语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> string </tag>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch与lucene</title>
      <link href="/2019/04/05/Elasticsearch%E4%B8%8Elucene/"/>
      <url>/2019/04/05/Elasticsearch%E4%B8%8Elucene/</url>
      
        <content type="html"><![CDATA[<p>搜索引擎选择： Elasticsearch与Solr</p><p>搜索引擎选型调研文档</p><p>Elasticsearch简介*</p><p>Elasticsearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速度去处理大规模数据。</p><p>它可以用于全文搜索，结构化搜索以及分析，当然你也可以将这三者进行组合。</p><p>Elasticsearch是一个建立在全文搜索引擎 Apache Lucene™ 基础上的搜索引擎，可以说Lucene是当今最先进，最高效的全功能开源搜索引擎框架。</p><p>但是Lucene只是一个框架，要充分利用它的功能，需要使用JAVA，并且在程序中集成Lucene。需要很多的学习了解，才能明白它是如何运行的，Lucene确实非常复杂。</p><p>Elasticsearch使用Lucene作为内部引擎，但是在使用它做全文搜索时，只需要使用统一开发好的API即可，而不需要了解其背后复杂的Lucene的运行原理。</p><p>当然Elasticsearch并不仅仅是Lucene这么简单，它不但包括了全文搜索功能，还可以进行以下工作:</p><p>分布式实时文件存储，并将每一个字段都编入索引，使其可以被搜索。</p><p>实时分析的分布式搜索引擎。</p><p>可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。</p><p>这么多的功能被集成到一台服务器上，你可以轻松地通过客户端或者任何你喜欢的程序语言与ES的RESTful API进行交流。</p><p>Elasticsearch的上手是非常简单的。它附带了很多非常合理的默认值，这让初学者很好地避免一上手就要面对复杂的理论，</p><p>它安装好了就可以使用了，用很小的学习成本就可以变得很有生产力。</p><p>随着越学越深入，还可以利用Elasticsearch更多高级的功能，整个引擎可以很灵活地进行配置。可以根据自身需求来定制属于自己的Elasticsearch。</p><p>使用案例：</p><p>维基百科使用Elasticsearch来进行全文搜做并高亮显示关键词，以及提供search-as-you-type、did-you-mean等搜索建议功能。</p><p>英国卫报使用Elasticsearch来处理访客日志，以便能将公众对不同文章的反应实时地反馈给各位编辑。</p><p>StackOverflow将全文搜索与地理位置和相关信息进行结合，以提供more-like-this相关问题的展现。</p><p>GitHub使用Elasticsearch来检索超过1300亿行代码。</p><p>每天，Goldman Sachs使用它来处理5TB数据的索引，还有很多投行使用它来分析股票市场的变动。</p><p>但是Elasticsearch并不只是面向大型企业的，它还帮助了很多类似DataDog以及Klout的创业公司进行了功能的扩展。</p><p>Elasticsearch的优缺点**:</p><p>优点</p><p>Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。<br>Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。<br>处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级设置。<br>Elasticsearch 采用 Gateway 的概念，使得完备份更加简单。<br>各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。<br>缺点</p><p>只有一名开发者（当前Elasticsearch GitHub组织已经不只如此，已经有了相当活跃的维护者）<br>还不够自动（不适合当前新的Index Warmup API）<br>Solr简介*</p><p>Solr（读作“solar”）是Apache Lucene项目的开源企业搜索平台。其主要功能包括全文检索、命中标示、分面搜索、动态聚类、数据库集成，以及富文本（如Word、PDF）的处理。Solr是高度可扩展的，并提供了分布式搜索和索引复制。Solr是最流行的企业级搜索引擎，Solr4 还增加了NoSQL支持。</p><p>Solr是用Java编写、运行在Servlet容器（如 Apache Tomcat 或Jetty）的一个独立的全文搜索服务器。 Solr采用了 Lucene Java 搜索库为核心的全文索引和搜索，并具有类似REST的HTTP&#x2F;XML和JSON的API。Solr强大的外部配置功能使得无需进行Java编码，便可对其进行调整以适应多种类型的应用程序。Solr有一个插件架构，以支持更多的高级定制。</p><p>因为2010年 Apache Lucene 和 Apache Solr 项目合并，两个项目是由同一个Apache软件基金会开发团队制作实现的。提到技术或产品时，Lucene&#x2F;Solr或Solr&#x2F;Lucene是一样的。</p><p>Solr的优缺点</p><p>优点</p><p>Solr有一个更大、更成熟的用户、开发和贡献者社区。<br>支持添加多种格式的索引，如：HTML、PDF、微软 Office 系列软件格式以及 JSON、XML、CSV 等纯文本格式。<br>Solr比较成熟、稳定。<br>不考虑建索引的同时进行搜索，速度更快。<br>缺点</p><p>建立索引时，搜索效率下降，实时索引搜索效率不高。<br>Elasticsearch与Solr的比较*</p><p>当单纯的对已有数据进行搜索时，Solr更快。</p><p>Search Fesh Index While Idle</p><p>当实时建立索引时, Solr会产生io阻塞，查询性能较差, Elasticsearch具有明显的优势。</p><p>search_fresh_index_while_indexing</p><p>随着数据量的增加，Solr的搜索效率会变得更低，而Elasticsearch却没有明显的变化。</p><p>search_fresh_index_while_indexing</p><p>综上所述，Solr的架构不适合实时搜索的应用。</p><p>实际生产环境测试*</p><p>下图为将搜索引擎从Solr转到Elasticsearch以后的平均查询速度有了50倍的提升。</p><p>average_execution_time</p><p>Elasticsearch 与 Solr 的比较总结</p><p>二者安装都很简单；<br>Solr 利用 Zookeeper 进行分布式管理，而 Elasticsearch 自身带有分布式协调管理功能;<br>Solr 支持更多格式的数据，而 Elasticsearch 仅支持json文件格式；<br>Solr 官方提供的功能更多，而 Elasticsearch 本身更注重于核心功能，高级功能多有第三方插件提供；<br>Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。<br>Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用。</p><p>其他基于Lucene的开源搜索引擎解决方案*</p><p>直接使用 Lucene<br>说明：Lucene 是一个 JAVA 搜索类库，它本身并不是一个完整的解决方案，需要额外的开发工作。</p><p>优点：成熟的解决方案，有很多的成功案例。apache 顶级项目，正在持续快速的进步。庞大而活跃的开发社区，大量的开发人员。它只是一个类库，有足够的定制和优化空间：经过简单定制，就可以满足绝大部分常见的需求；经过优化，可以支持 10亿+ 量级的搜索。</p><p>缺点：需要额外的开发工作。所有的扩展，分布式，可靠性等都需要自己实现；非实时，从建索引到可以搜索中间有一个时间延迟，而当前的“近实时”(Lucene Near Real Time search)搜索方案的可扩展性有待进一步完善</p><p>Katta<br>说明：基于 Lucene 的，支持分布式，可扩展，具有容错功能，准实时的搜索方案。</p><p>优点：开箱即用，可以与 Hadoop 配合实现分布式。具备扩展和容错机制。</p><p>缺点：只是搜索方案，建索引部分还是需要自己实现。在搜索功能上，只实现了最基本的需求。成功案例较少，项目的成熟度稍微差一些。因为需要支持分布式，对于一些复杂的查询需求，定制的难度会比较大。</p><p>Hadoop contrib&#x2F;index<br>说明：Map&#x2F;Reduce 模式的，分布式建索引方案，可以跟 Katta 配合使用。</p><p>优点：分布式建索引，具备可扩展性。</p><p>缺点：只是建索引方案，不包括搜索实现。工作在批处理模式，对实时搜索的支持不佳。</p><p>LinkedIn 的开源方案<br>说明：基于 Lucene 的一系列解决方案，包括 准实时搜索 zoie ，facet 搜索实现 bobo ，机器学习算法 decomposer ，摘要存储库 krati ，数据库模式包装 sensei 等等</p><p>优点：经过验证的解决方案，支持分布式，可扩展，丰富的功能实现</p><p>缺点：与 linkedin 公司的联系太紧密，可定制性比较差</p><p>Lucandra<br>说明：基于 Lucene，索引存在 cassandra 数据库中</p><p>优点：参考 cassandra 的优点</p><p>缺点：参考 cassandra 的缺点。另外，这只是一个 demo，没有经过大量验证</p><p>HBasene<br>说明：基于 Lucene，索引存在 HBase 数据库中</p><p>优点：参考 HBase 的优点</p><p>缺点：参考 HBase 的缺点。另外，在实现中，lucene terms 是存成行，但每个 term 对应的 posting lists 是以列的方式存储的。随着单个 term 的 posting lists 的增大，查询时的速度受到的影响会非常大</p><p>转载：<a href="https://blog.csdn.net/jameshadoop/article/details/44905643">https://blog.csdn.net/jameshadoop/article/details/44905643</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>写在考研成绩前</title>
      <link href="/2019/02/15/%E5%86%99%E5%9C%A8%E8%80%83%E7%A0%94%E6%88%90%E7%BB%A9%E5%89%8D/"/>
      <url>/2019/02/15/%E5%86%99%E5%9C%A8%E8%80%83%E7%A0%94%E6%88%90%E7%BB%A9%E5%89%8D/</url>
      
        <content type="html"><![CDATA[<p>我究竟想要的是什么？<br>最好的东西？不服输？抑或是人生的巅峰？财富&amp;名声？<br>我真正想要的是探索自己不曾到达的地方。<br>站在学术的最前沿，探索一片未知的领域，对知识的渴求，对技术的渴求，是我真正想要的。<br>自己，要时刻保持这种信念，怀有一颗探索未来的好奇心。</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>final &amp; Runtime.getRuntime &amp; String...string</title>
      <link href="/2019/01/28/final-Runtime-getRuntime-String-string/"/>
      <url>/2019/01/28/final-Runtime-getRuntime-String-string/</url>
      
        <content type="html"><![CDATA[<h3 id="1-final"><a href="#1-final" class="headerlink" title="1. final"></a>1. final</h3><p>final类不能被继承，没有子类，final类中的方法默认是final的<br><br>final方法不能被子类的方法复盖，但可以被继承<br><br>final成员变量表示常量，只能被赋值一次，赋值后不能再被改变<br><br>final不能用于修饰构造方法<br><br>private不能被子类方法覆盖，private类型的方法默认是final类型的<br></p><p>final修饰的变量有三种：静态变量、实例变量和局部变量，分别表示三种类型的常量。<br><br>注意：final变量定义的时候，可以先声明，而不给初值，这中变量也称为final空白，无论什么情况，编译器都确保空白final在使用之前必须被初始化。</p><h3 id="2-static"><a href="#2-static" class="headerlink" title="2. static"></a>2. static</h3><p>static表示“全局”或者“静态”的意思，用来修饰成员变量和成员方法，也可以形成静态static代码块，但是Java语言中没有全局变量的概念。</p><p>被static修饰的成员变量和成员方法独立于该类的任何对象。也就是说，它不依赖类特定的实例，被类的所有实例共享。只要这个类被加载，Java虚拟机就能根据类名在运行时数据区的方法区内定找到他们。因此，static对象可以在它的任何对象创建之前访问，无需引用任何对象。</p><p>用public修饰的static成员变量和成员方法本质是全局变量和全局方法，当声明它类的对象市，不生成static变量的副本，而是类的所有实例共享同一个static变量。</p><ul><li>类成员变量<ul><li>静态变量（类变量）: static修饰</li><li>实例变量      : 无static修饰</li></ul></li><li>局部变量</li></ul><h3 id="3-static和final一起使用"><a href="#3-static和final一起使用" class="headerlink" title="3. static和final一起使用"></a>3. static和final一起使用</h3><p>static final用来修饰成员变量和成员方法，可以理解为“全局变量”</p><p>对于变量，表示一旦给值就不可修改，并且通过类名可以访问。<br>对于方法，表示不可覆盖，并且可以通过类名直接访问。</p><p>注意：<br>对于被static和final修饰过的实例常量，实例本身不能再改变了，但对于一些容器类型（比如，ArrayList、HashMap）的实例变量，不可以改变容器变量本身，但可以修改容器中存放的对象。</p><h3 id="4-Runtime-getRuntime-exec"><a href="#4-Runtime-getRuntime-exec" class="headerlink" title="4.Runtime.getRuntime().exec()"></a>4.Runtime.getRuntime().exec()</h3><p>有时候我们可能需要调用系统外部的某个程序，此时就可以用Runtime.getRuntime().exec()来调用，他会生成一个新的进程去运行调用的程序。</p><p>此方法返回一个java.lang.Process对象，该对象可以得到之前开启的进程的运行结果，还可以操作进程的输入输出流。</p><p>Process对象有以下几个方法：</p><ul><li><p>1、destroy()　　　　　　杀死这个子进程</p></li><li><p>2、exitValue()　　　 　得到进程运行结束后的返回状态</p></li><li><p>3、waitFor()　　　　 　得到进程运行结束后的返回状态，如果进程未运行完毕则等待知道执行完毕</p></li><li><p>4、getInputStream()　　得到进程的标准输出信息流</p></li><li><p>5、getErrorStream()　　得到进程的错误输出信息流</p></li><li><p>6、getOutputStream()　得到进程的输入流</p></li></ul><h3 id="5-String…-args-和-String-args-的区别"><a href="#5-String…-args-和-String-args-的区别" class="headerlink" title="5.String… args 和 String[] args 的区别"></a>5.String… args 和 String[] args 的区别</h3> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  public static void callMe1(String[] args) &#123;  </span><br><span class="line">System.out.println(args.getClass() == String[].class);  </span><br><span class="line">for(String s : args) &#123;  </span><br><span class="line">System.out.println(s);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">public static void callMe2(String... args) &#123;  </span><br><span class="line">System.out.println(args.getClass() == String[].class);  </span><br><span class="line">for(String s : args) &#123;  </span><br><span class="line">System.out.println(s);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>他们可以这样调用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">callMe1(new String[] &#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;);  </span><br><span class="line">callMe2(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);  </span><br><span class="line">callMe2(new String[] &#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;);</span><br></pre></td></tr></table></figure><p>不可以这样调用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calllMe1(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);</span><br></pre></td></tr></table></figure><p>String …args 是一个不定长参数。<br>String args[] 是一个数组作为参数。<br><br>注：mac 的反引号就在数字1的左边，但是你需要按住option(alt)键并且在英文状态下即可。</p>]]></content>
      
      
      <categories>
          
          <category> 程序语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日记1 19.1.22</title>
      <link href="/2019/01/22/%E6%97%A5%E8%AE%B01-19-1-22/"/>
      <url>/2019/01/22/%E6%97%A5%E8%AE%B01-19-1-22/</url>
      
        <content type="html"><![CDATA[<h3 id="路"><a href="#路" class="headerlink" title="路"></a>路</h3><p>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;清晨，鸟儿展开歌喉，太阳缓缓升起，就这样，新的一天开始了。<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;而我，此时此刻还在被窝里呼呼大睡，浑然不知时间正在流失。每天就这样漫无目的的过日子。与其探索我最终要的是什么，不如寻求我现在要的是什么，脚踏实地的在自己的人生路上走一走。不荒废这最美好的光阴，不辜负自己儿时的梦想。<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;照着计划列表，一条一条的去做，去做最好的自己，累点苦点也要坚持，因为自己要做一个优秀的人。今天不是有所改善么？做了英语口语笔记。既然踏出了第一步，就勇敢的坚持一天，两天，万事开头难，坚持使之成为习惯，终会遇到最好的自己。<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;In the morning, the birds began to sing and the sun rose slowly, then the new day began.<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;I still slept at that moment, and didn’t realize the time was running off. Everyday is like this, purposelessly. Instead of exploring what I want ultimately, why not seek what I want now, and walk down the road of my life.Don’t waste this most beautiful time, don’t live up to the childhood dreams.<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;<br>According to the list of plans, do one by one ,and be the best I can be.</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>java输入</title>
      <link href="/2019/01/16/java%E8%BE%93%E5%85%A5/"/>
      <url>/2019/01/16/java%E8%BE%93%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="java输入几种方式"><a href="#java输入几种方式" class="headerlink" title="java输入几种方式"></a>java输入几种方式</h2><h3 id="1-int-m-x3D-int-System-in-read"><a href="#1-int-m-x3D-int-System-in-read" class="headerlink" title="1.int m &#x3D; (int)System.in.read();"></a>1.int m &#x3D; (int)System.in.read();</h3><p>System.in.read()返回一个整型字节数据，该数据表示的是字节因此是Unicode的第一个字节或是字符的ASCII码值。该方法是从一个流中一个一个的读取数据，因此是一个迭代的过程。我们可以看出in是一个静态的流，因此在该程序中只有一个流，重复调用System.in.read()实际上是在遍历该流中的每一个字节数据。最常见的流是键盘输入流。我们可以在键盘中输入一个字符串（其中按下回车键代表了两个字符\r\n,\r的ASCII码值是10，\n是13）。我们可以重复调用System.in.read()来读取从键盘输入的字符串中的字符所代表的字节（ASCII值）。<br><br>想输入字符或数字要用输入流如<em>DataInputStream</em>等来协助转换</p><blockquote><p>DataInputStream in &#x3D; new DataInputStream(System.in);</p></blockquote><p>或BufferReader</p><blockquote><p>BufferedReader reader &#x3D; new BufferedReader(new InputStreamReader(System.in));</p></blockquote><p>其中InputStreamReader是用于象读取字符流一样读取二进制输入流的具体类，在这里将System.in放到InputStreamReader对象之中，然后通过使用BufferedReader对象将其缓冲，提高输入操作的效率。</p><h3 id="2-Scnner类"><a href="#2-Scnner类" class="headerlink" title="2.Scnner类"></a>2.Scnner类</h3><blockquote><p>import java.until.Scanner; <br><br>…<br><br>Scanner sc &#x3D; new Scanner(System.in); <br><br>String name &#x3D; sc.nextLine(); &#x2F;&#x2F;输入字符串<br><br>int age &#x3D; sc.nextInt(); &#x2F;&#x2F;输入int类型<br><br> float salary &#x3D; sc.nextFloat(); &#x2F;&#x2F;输入float类型<br></p></blockquote><p> next()和nextLine()的区别：next()方法是不接收空格的，在接收到有效数据前，所有的空格或者tab键等输入被忽略，若有有效数据，则遇到这些键退出。nextLine()可以接收空格或者tab键，其输入应该以enter键结束。</p>]]></content>
      
      
      <categories>
          
          <category> 程序语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Factorization Machines with libFM</title>
      <link href="/2019/01/15/%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA/"/>
      <url>/2019/01/15/%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="Factorization-Machines-with-libFM（因子分解机"><a href="#Factorization-Machines-with-libFM（因子分解机" class="headerlink" title="Factorization Machines with libFM（因子分解机)"></a>Factorization Machines with libFM（因子分解机)</h2><p>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;因子分解机在许多重要预测问题上提供了高精度，例如：推荐系统（recommender system）。然而将分解方法应用于新的预测问题是一项非常重要的任务，需要大量的专业知识。通常，开发一个新的模型，需要一个学习算法，并且必须实施该算法。<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;因子分解机（FM）是一种通用方法，因为它们可以仅通过特征工程来模拟大多数分解模型。 这样，分解机器将特征工程的一般性与分解模型的优越性结合起来，用于估计大型分类变量之间的相互作用。 LIBFM是分解机器的软件实现，具有随机梯度偏差（SGD）和交替最小二乘（ALS）优化，以及使用马尔可夫链蒙托卡罗（MCMC）的贝叶斯推断。 本文总结了最新的分解算法在建模和学习方面的研究，为ALS和MCMC算法提供了扩展，并描述了软件工具LIBFM。<br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;尽管分解模型在许多应用中具有很高的预测质量，但使用它们并不容易。对于无法用分类变量描述的每个问题，必须导出新的专用模型，并且必须开发和实现学习算法。这非常耗时，容易出错，并且仅适用于因子分解模型中的专家。<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;另一方面，在实践中，机器学习的典型方法是使用特征向量（预处理步骤，即特征工程）描述数据，并应用标准工具，例如，LIBSVM [Chang and Lin 2011]用于支持向量机器，Weka等工具箱[Hall et al。 2009]，或简单的线性回归工具。即使对于没有深入了解底层机器学习模型和推理机制的用户，这种方法也很容易适用。<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在本文中，介绍了分解算法（FM）[Rendle 2010]。 FM将分解模型的高预测精度与特征工程的灵活性相结合。 FM的输入数据用实值特征描述，与其他机器学习方法（如线性回归，支持向量机等）完全相同。但是，FM的内部模型使用变量之间的分解交互，因此，它与其他分解模型共享稀疏设置中的高预测质量，如推荐系统。已经证明FM只能通过特征工[Rendle 2010]来模仿大多数因子分解模型。本文总结了最近对FM的研究，包括基于随机梯度下降的学习算法，交替最小二乘法和使用MCMC的贝叶斯方法。 FM和所有呈现的算法可在公开的软件工具LIBFM中获得。使用LIBFM，应用分解模型就像应用标准工具一样简单，例如SVM或线性回归。<br><br>&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;本文的结构如下：（1）介绍了LIBFM中可用的FM模型及其学习算法; （2）给出了输入数据的几个例子，并给出了与专业化因子分解模型的关系; （3）简要介绍了LIBFM软件; （4）进行实验。</p><h3 id="FACTORIZATION-MACHINE-MODEL"><a href="#FACTORIZATION-MACHINE-MODEL" class="headerlink" title="FACTORIZATION MACHINE MODEL"></a>FACTORIZATION MACHINE MODEL</h3><p>让我们假设预测问题的数据由设计矩阵X∈Rn×p描述，其中X的第i行xi∈Rp描述具有p个实值变量的一个情况，并且其中yi是第i个的预测目标。 案例（例如，见图1)。 或者，可以将此设置描述为元组（x，y）的集合S，其中（再次）x∈Rp是特征向量，y是其对应的目标。 这种具有数据矩阵和特征向量的表示在许多机器学习方法中很常见，例如，在线性回归或支持向量机（SVM）中。<br>因子分解机（FM）使用分解交互参数对x中d与p之间输入变量之间的所有嵌套交互进行建模。d&#x3D;2时 因子分解机可以被定于为：<img src= "/img/loading.gif" data-lazy-src="/images/%E5%85%AC%E5%BC%8F.jpg" alt="&quot;图片1&quot;"><br>其中k是分解的维数，模型参数􏰀&#x3D; {w0，w1，…，wp，v1,1，… vp，k}是w0∈R, w∈Rp, V∈Rp×k.<br><br><em>案例图一：<img src= "/img/loading.gif" data-lazy-src="/images/%E5%88%86%E8%A7%A3%E6%9C%BA.png" alt="分解机"><br>每行表示具有其对应目标yi的特征向量xi。 为了便于解释，这些功能分为活动用户（蓝色），活动项目（红色），同一用户评级的其他电影（橙色），每月上映日期（绿色）和上次电影评分 （棕色）。</em><br></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018总结</title>
      <link href="/2019/01/03/2018%E6%80%BB%E7%BB%93/"/>
      <url>/2019/01/03/2018%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="2018年终总结"><a href="#2018年终总结" class="headerlink" title="2018年终总结"></a>2018年终总结</h2><blockquote><p>“一个男人要走多少路，才能被称作男人。”<br><br>&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;—--鲍勃.迪伦<br> </p></blockquote><p>&amp;emsp;&amp;emsp;这份总结很早就应该做的，2018年发生了好多事情，对自己来说也是至关重要的一年。2018年自己成熟了不少。<br><br>&amp;emsp;&amp;emsp;”光阴似箭，日月如梭“，儿时日记的标准开头，而用在此时此景却再恰当不过了。大一仿佛还在昨天，而今天却已面临毕业。回想这一年，主题是简简单单的两个字“考研”。“好高骛远，眼高手低”是最大的收获。自己从始至终都明白自己想要什么，但最大的问题是拖延症，总觉得时间够用，三天打鱼两天晒网。这又牵扯到另一个问题，自制力，曾经引以为傲的强大的自制力与自信心已经被现实的诱惑消磨殆尽。2018又有许多不堪回首的往事，经历的东西越多，也越加成熟。<br><br>&amp;emsp;&amp;emsp;心念的力量，往往可以跨越现实的阻隔，结合所有的有利条件构成一道神奇莫测的磁场，当你真心想要某件东西时，全宇宙都会联合起来帮你的忙。<br><br>&amp;emsp;&amp;emsp;2018已经远去，新的一年也已开始，带着未来的憧憬，坚定的向前出发，不以物喜，不以己悲，对自己有一个清晰的规划，并将行动贯彻到底，要时刻明白自己想要什么。灰暗的日子里，灰暗的是人而不是日子，要乐观自信。<br><br>&amp;emsp;&amp;emsp;新的一年，新的开始。2019，我来了，一个全新的我！这是对自己的一个承诺。</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7z命令</title>
      <link href="/2018/07/06/7z/"/>
      <url>/2018/07/06/7z/</url>
      
        <content type="html"><![CDATA[<p>###brew直接安装解压工具</p><p><em>$ brew search 7z</em></p><p><em>$ brew install p7zip</em></p><p>然后使用7z命令即可<br><em><strong>7z e filename.7z</strong></em></p><p>####用法<br>7z.exe在CMD窗口的使用说明如下：</p><p>7-Zip (A) 4.57 Copyright (c) 1999-2007 Igor Pavlov 2007-12-06</p><p>Usage: 7za <command> [<switches>…] <archive_name> [<file_names>…]<br>[&lt;@listfiles…&gt;]</file_names></archive_name></switches></p><Commands>a: Add files to archiveb: Benchmarkd: Delete files from archivee: Extract files from archive (without using directory names)l: List contents of archivet: Test integrity of archiveu: Update files to archivex: eXtract files with full paths<Switches>-ai[r[-|0]]{@listfile|!wildcard}: Include archives-ax[r[-|0]]{@listfile|!wildcard}: eXclude archives-bd: Disable percentage indicator-i[r[-|0]]{@listfile|!wildcard}: Include filenames-m{Parameters}: set compression Method-o{Directory}: set Output directory-p{Password}: set Password-r[-|0]: Recurse subdirectories-scs{UTF-8 | WIN | DOS}: set charset for list files-sfx[{name}]: Create SFX archive-si[{name}]: read data from stdin-slt: show technical information for l (List) command-so: write data to stdout-ssc[-]: set sensitive case mode-ssw: compress shared files-t{Type}: Set type of archive-v{Size}[b|k|m|g]: Create volumes-u[-][p#][q#][r#][x#][y#][z#][!newArchiveName]: Update options-w[{path}]: assign Work directory. Empty path means a temporary directory-x[r[-|0]]]{@listfile|!wildcard}: eXclude filenames-y: assume Yes on all queries<p>中文使用说明如下：</p><p>语法<br>7z &lt;命令行&gt; [&lt;选项&gt;…] &lt;基本档案名称&gt; [&lt;参数变量&gt;…]<br>7z <command> [<switch>…] <base_archive_name> [<arguments>…]* g8 T1 {- l2 e) P% T’ V; A<br>&lt;参数变量&gt; ::&#x3D; &lt;选项&gt; | &lt;通配符&gt; | &lt;文件名&gt; | &lt;文件列表&gt;<br>&lt;选项&gt;::&#x3D; &lt;选项标记&gt;&lt;选项字符&gt;[&lt;选项&gt;]<br>&lt;选项标记&gt; ::&#x3D; ‘&#x2F;‘ | ‘-‘<br>&lt;文件列表&gt; ::&#x3D; @{文件名}( @4 _; F   r6 j. t+ a5 @</arguments></base_archive_name></switch></p><p><arguments> ::&#x3D; <switch> | <wildcard> | <filename> | <list_file><br><switch>::&#x3D; <switch_symbol><switch_characters>[<option>]<br><switch_symbol> ::&#x3D; ‘&#x2F;‘ | ‘-‘<br><list_file> ::&#x3D; @{filename}<br>在方括号内的表达式(“[” 和 “]”之间的字符)是可选的。’ ?2 k0 k&#x2F; m* f. Q$ c4 h<br>在书名号内的表达式(“&lt;” 和 “&gt;”之间的字符)是必须替换的表达式(而且要去掉括号)。<br>表达式<br>expression1 | expression2 | … | expressionN* R- s: |- P&#x2F; n&amp; m9 C<br>命令行 及 选项 使用大写或小写字母都可以。<br>首个命令行必须是无选项的参数变量。<br>选项及其它文件名的输入顺序可以打乱。<br>带有空格的通配符或文件名必须加上引号：<br>“Dir\Program files*“+ r+ u2 k   b’ Z&amp; U. n<br>Dir&quot;Program files”*. M   R   V3 S&#x2F; s<br>通配符是一个键盘字符，例如星号(<em>)或问号(?)，当执行添加文件、释放文件、选定文件、删除文件等操作时，您可以使用它来代表一个或多个字符。当您不知道真正字符或者不想键入完整名称时，常常使用通配符代替一个或多个字符。<br>7-Zip 支持和 Windows 相类似的通配符：<br>“</em>”可以使用星号代替零个或多个字符。<br>“?”可以用问号代替名称中的单个字符。<br>7-Zip 使用的并不是系统处理通配符的默认方法，因而 7-Zip 不支持其它通配符规则，在系统中 <em>.</em> 相当于所有文件。而 7-Zip 会将其视为任何扩展名的全部文件。所以，要处理所有文件您必须使用 * 通配符。<br>示例：<br><em>.txt 这样会查找(添加、选定……)所有扩展名是“.txt”的文件 &#x2F; z9 x0 }’ d, B, s# z<br>?a</em> 这样会查找(添加、选定……)所有第二个字母为“a”的文件 &#x2F; f+ ^8 N7 S% b. b’ Q<br><em>1</em> 这样会查找(添加、选定……)所有包含“1”的文件 3 W# Q” f   C* h   E6 n<br><em>.</em>.* 这样会查找(添加、选定……)所有包含“.”的双扩展名文件<br>如果在命令行中没有文件名，系统将会使用默认通配符“<em>”。</em> K. J# R7 u) w7 n3 A6 ?<br>档案文件中通配符及文件名的使用限制：<br>通配符及文件名不能包括系统盘符或网址。每个通配符及文件名路径将被视为从盘符到当前目录的完整路径&#x2F;从压缩档案的根目录算起的完整路径。换句话说，路径的开始部分(在首个斜线(“\”)之前的字符)必须是某个名称或通配符。 1 [% e: w&#x2F; V&#x2F; g, l” t0 N<br>通配符及文件名不能以斜线(“\”)结尾。<br>通配符只可以在完整路径的最后一部分中出现。 ‘ M) Q’ ~9 v2 O; m2 I9 _$ G2 D, U” b<br>示例：<br>Dir1*.cpp 正确<br>c:\Dir1*.cpp 错误：路径中不能包括盘符<br>Dir1\Dir2\g?.txt 正确 ; i7 Q+ P) e2 F8 g<br>Dir1\D?r2\file1.txt 错误：只有在以路径的最后一部分才能使用通配符<br>文件列表: l   Z$ }; P4 O<br>您可以使用文件列表来对要操作的文件进行批量操作。在文件中的文件名必须用空格或另起一行隔开。(如使用空格格开，每一个文件必须加引号)。<br>7-Zip 命令行支持多个文件列表同时操作。2 Y+ k2 G9 |2 g: p. x2 M’ Z<br>举个例子，这里有一个文件列表“listfile.txt”包含下列内容：” I! Z) }. C7 k&#x2F; H4 g; u’ @+ Q<br>“My programs*.cpp”5 v; @* F) U5 ]$ k3 H6 J! b* N<br>Src*.cpp<br>那么我们可以输入命令：<br>7z a -tzip archive.zip @listfile.txt<br>将“My programs”及“Src”目录中所有扩展名为“cpp”的文件添加到压缩档案“archive.zip”中。</list_file></switch_symbol></option></switch_characters></switch_symbol></switch></list_file></filename></wildcard></switch></arguments></p><p>命令行<br>命令行的命令不分大小写。<br>更多有关命令行的详细内容请参阅 语法。<br>命令要点参考<br>命令 作用说明<br>a 添加 . M% Z1 A&#x2F; G8 y: S9 d* c<br>d 删除 : j9 _   _2 ~$ W, W2 m: U<br>e 释放 - Y7 {! I   _( K$ y<br>l 列表 4 w&amp; j5 K   X4 }+ Y# A) d* k: S<br>t 测试 . h: w&amp; C4 ~- P* o+ @<br>u 更新<br>x 完整路径释放<br>&amp; <code>- O( ]   M. T&#39; k   x0 </code><br>命令行选项&#x2F; }1 o6 h9 v9 B2 V9 |9 q’ R. y<br>语法   V* Y, &#96;+ p( K5 b<br>&lt;选项&gt;::&#x3D; &lt;选项_符号&gt;&lt;选项_字符&gt;[&lt;选项&gt;]<br>&lt;选项_符号&gt; ::&#x3D; ‘&#x2F;‘ | ‘-‘<br>&amp;ltswitch&gt;::&#x3D; <switch_symbol><switch_characters>[<option>]<br><switch_symbol> ::&#x3D; ‘&#x2F;‘ | ‘-‘<br>在命令行中，一个完整的选项由指定的选项、连字符(-)或斜线(&#x2F;)组成，而且选项的符号不能使用缩写。, q$ P! D- o” H   b” w&#x2F; Y3 _# D<br>选项名称不区分大小写。而一部分选项会包括参数变量，它们是需要区分大小写的。<br>选项可以使用在命令行中的任何位置。##</switch_symbol></option></switch_characters></switch_symbol></p><p>（参考greegree的文章）先给出一个压缩文件的例子：</p><p>7z a -t7z archive.7z *.exe *.dll -m0&#x3D;BCJ -m1&#x3D;LZMA:d&#x3D;21 -ms -mmt</p><pre><code>添加 *.exe 及 *.dll 文件到固实压缩档案 archive.7z。使用 LZMA 压缩算法、2 MB 字典大小及 BCJ 转换器。压缩将开启多线程优化(如果可用)。</code></pre><p>   -ms 默认设置固实模式。在创建固实压缩档案模式中，它把压缩档案中的所有文件都当成一个连续数据流来看待。通常情况下，固实压缩可增加压缩比，特别是在添加大量小文件的时候<br>-mmt 默认开启多线程模式。<br>以上两条倒是可以理解，但是“-m0&#x3D;BCJ -m1&#x3D;LZMA:d&#x3D;21 ”又该如何解释？<br>按照给出的中文的文档说明：<br>-m0&#x3D;BCJ – 第一个备选的压缩算法为BCJ<br>-m1&#x3D;LZMA:d&#x3D;21 – 第二个备选的压缩算法为LZMA，指定字典大小为默认的21(2MB的1次方)2MB。<br>如果：想要使用最大化压缩，可以使用下面的参数选项：</p><pre><code>7z a -t7z DriverTest_1.7z &quot;I:\t\t1\*&quot; -mx=9 -ms=200m -mf -mhc -mhcf -m0=LZMA:a=2:d=25:mf=bt4b:fb=64 -mmt -r</code></pre><p>-t7z – 压缩文件的格式为7z<br>-mx&#x3D;9 -ms&#x3D;200m -mf -mhc -mhcf -m0&#x3D;LZMA:a&#x3D;2:d&#x3D;25:mf&#x3D;bt4b:fb&#x3D;64 -mmt<br>– 指定压缩算法选项<br>-mx&#x3D;9 – 设置压缩等级为极限压缩（默认为：LZMA 最大算法、32 MB 字典大小、BT4b Match finder、单词大小为 64、BCJ2 过滤器）<br>-ms&#x3D;200m – 开启固实模式，设置固实数据流大小为200MB。<br>-mf – 开启可执行文件压缩过滤器。<br>-mhc – 开启档案文件头压缩。<br>-mhcf – 开启档案文件头完全压缩。我所使用的7z版本为4.42&gt;2.30。<br>-m0&#x3D;LZMA:a&#x3D;2:d&#x3D;25:mf&#x3D;bt4b:fb&#x3D;64<br>– 第一个备选压缩算法为LZMA，压缩等级为最大压缩，LZMA算法使用的字典大小为25(2MB的5次方)32MB，算法的匹配器为bt4b(所需要内存为d×9.5 + 34 MB)，压缩算法的紧凑字节为最大模式的64字节。<br>-mmt – 开启多线程模式。<br>-r – 递归到所有的子目录。</p><p>7z命令解压文件的例子：解压File.7z文件到目录Mydir。</p><p>7z x “d:\File.7z” -y -aos -o”d:\Mydir”</p><p>参数说明：</p><p>x:完整路径下解压文件</p><p>-y:所有确认选项都默认为是（即不出现确认提示）</p><p>-aos:跳过已存在的文件</p><p>-o:设置输出目录</p><p>关于内存的使用：</p><p>一般来说，WindowsXP至少使用80~160MB的内存，为了保证系统的运行正常，还要留够32MB的剩余物理内存。<br>所以如果是512MB的内存，那么7z压缩所使用的内存为（512-32-160）320MB，使用bt4b(d<em>9.5+34MB)，所以d&#x3D;32MB。<br>如果是256MB的内存，那么7z压缩所使用的内存为（256-32-120）104MB，使用bt4b(d</em>9.5+34MB)，所以d&#x3D;8MB。<br>由此可见，在WindowsXP的图形界面下要想得到更大的压缩比，或者扩大物理内存的容量，或者可以采用在DOS下运行32位程序的方法；否则无论你的虚拟内存或者系统的磁盘缓存设置得再大，只能看见硬盘灯狂闪得交换页面文件。<br>通常情况下，较大的字典文件能提供较高的压缩比。但是在压缩和解压缩的时候会比较慢而且需要较多的物理内存：压缩时所使用的物理内存约为字典文件的10倍，解压缩时所使用的物理内存约等于字典文件大小。</p><p>语法格式：（详细情况见7-zip帮助文件，看得头晕可以跳过，用到再学）<br>7z <command> [<switch>…] <base_archive_name> [<arguments>…] </arguments></base_archive_name></switch></p><p>7z.exe的每个命令都有不同的参数<switch>,请看帮助文件<br><base_archive_name>为压缩包名称<br><arguments>为文件名称，支持通配符或文件列表<br>a: 添加文件的压缩包，或者创建新的压缩包。 </arguments></base_archive_name></switch></p><p>d: 从压缩包中删除文件。 </p><p>e: 从压缩包中提取。 </p><p>t: 测试压缩包的是否出错。 </p><p>u: 更新压缩包中的文件。<br>其中，7z是至命令行压缩解压程序7z.exe，<command>是7z.exe包含的命令，列举如下： </p><p>a： Adds files to archive. 添加至压缩包<br>a命令可用参数：<br>-i (Include)<br>-m (Method)<br>-p (Set Password)<br>-r (Recurse)<br>-sfx (create SFX)<br>-si (use StdIn)<br>-so (use StdOut)<br>-ssw (Compress shared files)<br>-t (Type of archive)<br>-u (Update)<br>-v (Volumes)<br>-w (Working Dir)<br>-x (Exclude) </p><p>b： Benchmark </p><p>d： Deletes files from archive. 从压缩包中删除文件<br>d命令可用参数：<br>-i (Include)<br>-m (Method)<br>-p (Set Password)<br>-r (Recurse)<br>-u (Update)<br>-w (Working Dir)<br>-x (Exclude) </p><p>e： Extract解压文件至当前目录或指定目录<br>e命令可用参数：<br>-ai (Include archives)<br>-an (Disable parsing of archive_name)<br>-ao (Overwrite mode)<br>-ax (Exclude archives)<br>-i (Include)<br>-o (Set Output Directory)<br>-p (Set Password)<br>-r (Recurse)<br>-so (use StdOut)<br>-x (Exclude)<br>-y (Assume Yes on all queries) </p><p>l： Lists contents of archive.<br>t： Test<br>u： Update </p><p>x： eXtract with full paths用文件的完整路径解压至当前目录或指定目录<br>x命令可用参数：<br>-ai (Include archives)<br>-an (Disable parsing of archive_name)<br>-ao (Overwrite mode)<br>-ax (Exclude archives)<br>-i (Include)<br>-o (Set Output Directory)<br>-p (Set Password)<br>-r (Recurse)<br>-so (use StdOut)<br>-x (Exclude)<br>-y (Assume Yes on all queries)<br>-m (Set compression Method) switch<br>Specifies the compression method.</p><p>用7-ZIP实现批处理 命令行压缩和解压功能 </p><p>编辑一个.bat文件；每行这样写，就可以连续压制多个目录了<br>7z a -mx9 -md64m -mfb&#x3D;273 -slp -ssw -v1024m -mmt&#x3D;2 -r<br>{路径及7z档名} {路径及要压缩的文件名及路径 可以空格填多个}</p><p>例子：建议以成对双引号来包压缩档名和路径名<br>7z a -mx9 -md64m -mfb&#x3D;273 -slp -ssw -v1024m “game” “d:\game*.*”<br>把d:\game\ 以ultra模式 64m字典fb273 每分卷1024m模式压缩</p><p>-mxN N&#x3D;0~9 ；压缩模式选择<br>Level Method Dictionary FastBytes MatchFinder Filter Description<br>0 Copy No compression.<br>1 LZMA 64 KB 32 HC4 BCJ Fastest compressing<br>3 LZMA 1 MB 32 HC4 BCJ Fast compressing<br>5 LZMA 16 MB 32 BT4 BCJ Normal compressing<br>7 LZMA 32 MB 64 BT4 BCJ Maximum compressing<br>9 LZMA 64 MB 64 BT4 BCJ2 Ultra compressing -mdNm 填字典大小 比如填26 和填64m一样的；看说明更大内存也可以填，比如128m字典<br>最大1024m</p><p>The maximum value for dictionary size is 1 GB &#x3D; 2^30 bytes. Default values for LZMA are 24 (16 MB) in normal mode, 25 (32 MB) in maximum mode (-mx&#x3D;7) and 26 (64 MB) in ultra mode (-mx&#x3D;9). </p><p>-mfb&#x3D;N 填fastbytes大小，此数字增大会稍微加大压缩但减慢速度</p><p>-slp (Set Large Pages mode)；会加快压缩，但开始会卡下，This feature works only on Windows 2003 &#x2F; XP x64</p><p>-ssw 也压缩共享文件</p><p>-v (Create Volumes) switch<br>Specifies volume sizes.<br>Syntax<br>-v{Size}[b | k | m | g]</p><p>-mmt&#x3D;N 多核选项，比如双核填2</p><p>命令行压缩解压一 7z</p><ol><li>简介<br>7z，全称7-Zip， 是一款开源软件。是目前公认的压缩比例最大的压缩解压软件。<br>主页：<a href="http://www.7-zip.org/">http://www.7-zip.org/</a><br>中文主页：<a href="http://7z.sparanoid.com/">http://7z.sparanoid.com/</a><br>命令行版本下载：<a href="http://7z.sparanoid.com/download.html">http://7z.sparanoid.com/download.html</a><br>主要特征：</li></ol><h1 id="全新的LZMA算法加大了7z格式的压缩比"><a href="#全新的LZMA算法加大了7z格式的压缩比" class="headerlink" title="全新的LZMA算法加大了7z格式的压缩比"></a>全新的LZMA算法加大了7z格式的压缩比</h1><h1 id="支持格式："><a href="#支持格式：" class="headerlink" title="支持格式："></a>支持格式：</h1><ul><li>压缩 &#x2F; 解压缩：7z, XZ, BZIP2, GZIP, TAR, ZIP</li><li>仅解压缩：ARJ, CAB, CHM, CPIO, DEB, DMG, FAT, HFS, ISO, LZH, LZMA, MBR, MSI, NSIS, NTFS, RAR, RPM, UDF, VHD, WIM, XAR, Z</li></ul><p>2）退出代码<br>0 ： 正常，没有错误；<br>1 ： 警告，没有致命的错误，例如某些文件正在被使用，没有被压缩；<br>2 ： 致命错误；<br>7 ： 命令行错误；<br>8 ： 没有足够的内存；<br>255 ： 用户停止了操作；</p><p>3）使用语法<br>7z &lt;命令行&gt; [&lt;选项&gt;…] &lt;基本档案名称&gt; [&lt;参数变量&gt;…]</p><p>在方括号内的表达式(“[” 和 “]”之间的字符)是可选的。<br>在书名号内的表达式(“&lt;” 和 “&gt;”之间的字符)是必须替换的表达式(而且要去掉括号)。</p><p>7-Zip 支持和 Windows 相类似的通配符：<br>“<em>”可以使用星号代替零个或多个字符。<br>“?”可以用问号代替名称中的单个字符。<br>如果只用</em>，7-Zip 会将其视为任何扩展名的全部文件。</p><p>4）命令及实例</p><p>a 添加文件到压缩档案。<br>7z a archive1.zip subdir\ ：增加subdir文件夹下的所有的文件和子文件夹到archive1.zip中，archived1.zip中的文件名包含subdir\前缀。<br>7z a archive2.zip .\subdir* ：增加subdir文件夹下的所有的文件和子文件夹到archive1.zip中，archived2.zip中的文件名不包含subdir\前缀。<br>cd &#x2F;D c:\dir1\<br>7z a c:\archive3.zip dir2\dir3\ ：archiive3.zip中的文件名将包含dir2\dir3\前缀，但是不包含c:\dir1前缀。<br>7z a Files.7z *.txt -r ： 增加当前文件夹及其子文件夹下的所有的txt文件到Files.7z中。</p><p>b 测试 CPU 运行速度及检查内存错误。</p><p>d 从压缩档案删除文件。<br>7z d archive.zip *.bak -r ：从archive.zip中删除所有的bak文件。</p><p>e 从压缩档案中释放文件到当前目录中。或者到指定的输出文件夹。输出文件夹设置可以通过 -o (设置输出文件夹) 选项来更改。此命令会将所有被释放的文件放置到一个文件夹。如果您想使用完整路径释放文件，您必须使用 x (完整路径释放) 命令。<br>7z e archive.zip ：从压缩档案 archive.zip 中释放所有文件到当前文件夹。<br>7z e archive.zip -oc:\soft *.cpp ：从压缩档案 archive.zip 中释放 *.cpp 文件到 c:\soft 文件夹。</p><p>l 列出压缩档案内容。<br>7z l archive.zip ：列出压缩档案 archive.zip 的内容。</p><p>t 测试压缩档案文件的完整性。<br>7z t archive.zip *.doc ：在压缩档案 archive.zip 中测试 *.doc 文件的完整性。</p><p>u 在压缩档案文件中使用较新的文件替换掉较旧的文件。<br>7z u archive.zip *.doc ：在压缩档案 archive.zip 中更新 *.doc 文件。</p><p>x 在当前目录中，使用完整路径从压缩档案中释放文件.或者到指定的输出文件夹。<br>7z x archive.zip ：从压缩档案 archive.zip 中释放所有文件到当前文件夹。<br>7z x archive.zip -oc:\soft *.cpp ：从压缩档案 archive.zip 中释放 *.cpp 文件到 c:\soft 文件夹。</p><p>5）更多的选项</p><p>– 在命令行中使“–”后的选项开关“-”都失效。这样就允许在命令行中使用文件名以“-”开头的文件。<br>7z t – -ArchiveName.7z ：测试 -ArchiveName.7z 压缩档案.</p><p>-i指定压缩时附加文件或一类文件。此选项可附件添加多个类型。<br>i[<recurse_type>]<file_ref> 其中<recurse_type>为可以为r[- | 0]（具体的-r选项见后面-r），<file_ref>可以为@{listfile} | !{wildcard}。<br>7z a -tzip src.zip *.txt -ir!DIR1*.cpp ：从当前目录中添加 *.txt 文件，和 DIR1 目录及其子目录中的 *.cpp 文件到 src.zip 压缩档案。</file_ref></recurse_type></file_ref></recurse_type></p><p>-x 指定某一文件或某一类文件从操作中排除。此选项可同时排除多个类型。<br>x[<recurse_type>]<file_ref> 其中<recurse_type>为可以为r[- | 0]（具体的-r选项见后面-r），<file_ref>可以为@{listfile} | !{wildcard}。<br>7z a -tzip archive.zip <em>.txt -x!temp.</em> ：添加除 temp.* 文件之外的所有 *.txt 文件到压缩档案 archive.zip。</file_ref></recurse_type></file_ref></recurse_type></p><p>-o 指定释放文件的输出文件夹。此选项只能和释放命令配合使用。<br>7z x archive.zip -oc:\Doc ：从 archive.zip 压缩档案释放所有文件到 c:\Doc 文件夹。</p><p>-r 递归子目录选项。<br>-r 开启递归子目录。对于 e (释放)、l (列表)、t (测试)、x (完整路径释放) 这些在压缩档案中操作的命令， 会默认使用此选项。<br>-r- 关闭递归子目录。对于 a (添加)、d (删除)、u (更新) 等所有需扫描磁盘文件的命令，会默认使用此选项。<br>-r0 开启递归子目录。但只应用于通配符。<br>7z l archive.zip -r- *.doc ：列出在 archive.zip 压缩档案中根目录下的 *.doc 文件。<br>7z a -tzip archive.zip -r src*.cpp src*.h ：将 src 目录及其子目录中的 *.cpp 及 *.h 文件添加到 archive.zip 压缩档案。</p><p>-t 指定压缩档案格式。指定压缩档案格式。它们可以是：zip、7z、rar、cab、gzip、bzip2、tar 或其它格式。而 默认值是 7z 格式。<br>7z a -tzip archive.zip *.txt ：使用 zip 格式从当前目录中添加所有 *.txt 文件到压缩档案 archive.zip。</p><p>-y 使 7-Zip 执行命令时的大多数提示失效。您可以使用此选项来阻止在 e (释放) 和 x (完整路径释放) 命令中文件覆盖时的提示。<br>7z x src.zip -y ：从 src.zip 释放所有文件。所有的覆盖提示将被阻止且所有相同文件名的文件将被覆盖。</p><p>-v指定分卷大小。<br>{Size}[b | k | m | g]<br>指定分卷大小，可以使用字节、KB(1 KB＝1024 字节)，MB(1 MB &#x3D; 1024 KB)或 GB(1 GB &#x3D; 1024 MB)。如果您只指定了 {Size}，7-zip 将把它视为字。<br>7z a a.7z *.txt -v10k -v15k -v2m ： 创建 a.7z 分卷压缩档案。第一个分卷为 10 KB，第二个为 15 KB，剩下全部为 2 MB。</p><p>-p 指定密码。<br>7z x archive.zip -psecret ：将设有密码“secret”的压缩档案 archive.zip 中所有文件释放。</p><p>-ao 指定在释放期间如何覆盖硬盘上现有的同名文件。<br>语法：-ao[a | s | u ]<br>-aoa 直接覆盖现有文件，而没有任何提示。<br>-aos 跳过现有文件，其不会被覆盖。<br>-aou 如果相同文件名的文件以存在，将自动重命名被释放的文件。举个例子，文件 file.txt 将被自动重命名为 file_1.txt。<br>-aot 如果相同文件名的文件以存在，将自动重命名现有的文件。举个例子，文件 file.txt 将被自动重命名为 file_1.txt。<br>7z x test.zip -aoa ：从压缩档案 test.zip 中释放所有文件并却不做提示直接覆盖现有文件。</p><p>-an 不解析命令行中的 archive_name 区域。此选项必须和 -i (附加文件) 开关一起使用。比如您为压缩档案使用列表文件，您就需要指定 -ai 选项，所以您需要禁止解析命令行中的 archive_name 区域。<br>实例见后面的-ai和-ax中。</p><p>-ai 指定附加文件，包括压缩档案文件名及通配符。此选项可同时附加多个类型。<br>ai[<recurse_type>]<file_ref> 其中<recurse_type>为可以为r[- | 0]（具体的-r选项见后面-r），<file_ref>可以为@{listfile} | !{wildcard}。<br>7z t -an -air!*.7z ： 在当前目录及子目录下测试 *.7z 压缩档案。</file_ref></recurse_type></file_ref></recurse_type></p><p>-ax 指定必须从操作中排除的压缩档案。此选项可同时排除多个类型。<br>ax[<recurse_type>]<file_ref> 其中<recurse_type>为可以为r[- | 0]（具体的-r选项见后面-r），<file_ref>可以为@{listfile} | !{wildcard}。<br>7z t -an -ai!<em>.7z -ax!a</em>.7z ：测试除 a*.7z 之外的 *.7z 压缩档案。</file_ref></recurse_type></file_ref></recurse_type></p><p>更多的不常用的选项，可以查看帮助。例如：-m设置压缩算法；-scs 设置要压缩的文件的列表文件的字符集；-seml通过电子邮件发送压缩档；-sfx创建自释放档；-si从标准输入读入数据，-so从输出到标准输 出；-slp设置大内存模式；-slt显示技术信息；-ssc设置区分大小写；-ssw压缩正在写入的文件；-u更新选项。</p></Switches></Commands>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 解压 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数组和Vector</title>
      <link href="/2018/01/23/%E6%95%B0%E7%BB%84%E5%92%8CVector/"/>
      <url>/2018/01/23/%E6%95%B0%E7%BB%84%E5%92%8CVector/</url>
      
        <content type="html"><![CDATA[<h4 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h4><p>c++数组没有直接获取长度的方法,利用sizeof(a)&#x2F;sizeof(a[0])</p><h4 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h4><p>头文件#include <vector><br>vector<int>a<br>vector<int>a(10)<br>vector<int>a(10,1)<br>vector<int>b(a)<br>vector<int>b(a.begin(), a.begin()+3)</int></int></int></int></int></vector></p><h5 id="基本操作："><a href="#基本操作：" class="headerlink" title="基本操作："></a>基本操作：</h5><p>a.size()<br>a.empty()<br>a.clear()<br>a.push_back()<br>a.pop_back()<br>a &#x3D; b ;            &#x2F;&#x2F;将b向量复制到a向量中<br>a.insert(a.begin(), 1000);            &#x2F;&#x2F;将1000插入到向量a的起始位置前<br>a.insert(a.begin(), 3, 1000) ;        &#x2F;&#x2F;将1000分别插入到向量元素位置的0-2处(共3个元素)<br>b.insert(b.begin(), a.begin(), a.end()) ;        &#x2F;&#x2F;将a.begin(), a.end()之间的全部元素插入到b.begin()前<br>b.erase(b.begin()) ;                     &#x2F;&#x2F;将起始位置的元素删除<br>b.erase(b.begin(), b.begin()+3) ;        &#x2F;&#x2F;将(b.begin(), b.begin()+3)之间的元素删除</p><h5 id="Vector的遍历："><a href="#Vector的遍历：" class="headerlink" title="Vector的遍历："></a>Vector的遍历：</h5><p>for(vector<int>::iterator it &#x3D; a.begin(); it !&#x3D; a.end(); ++it){<br>    cout &lt;&lt; *it;<br>}</int></p>]]></content>
      
      
      <categories>
          
          <category> 程序语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ffmpeg命令</title>
      <link href="/2018/01/23/ffmpeg%E5%91%BD%E4%BB%A4/"/>
      <url>/2018/01/23/ffmpeg%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>（经常用到ffmpeg 做一些视频数据的处理转换等，用来做测试，今天总结了一下，参考了网上部分朋友的经验，一起在这里汇总了一下，有需要的朋友可以收藏测试一下，有问题欢迎在下面回帖交流，谢谢;by ternence.hsu)</p><p>1、ffmpeg使用语法<br>命令格式：<br>    ffmpeg -i [输入文件名] [参数选项] -f [格式] [输出文件]<br>    ffmpeg [[options][&#96;-i’ input_file]]… {[options] output_file}…<br>    1、参数选项：<br>    (1) -an: 去掉音频<br>    (2) -acodec: 音频选项， 一般后面加copy表示拷贝<br>    (3) -vcodec:视频选项，一般后面加copy表示拷贝<br>    2、格式：<br>    (1) h264: 表示输出的是h264的视频裸流<br>    (2) mp4: 表示输出的是mp4的视频<br>    (3)mpegts: 表示ts视频流<br>    如果没有输入文件，那么视音频捕捉（只在Linux下有效，因为Linux下把音视频设备当作文件句柄来处理）就会起作用。作为通用的规则，选项一般用于下一个特定的文件。如果你给 –b 64选项，改选会设置下一个视频速率。对于原始输入文件，格式选项可能是需要的。缺省情况下，ffmpeg试图尽可能的无损转换，采用与输入同样的音频视频参数来输出。（by ternence.hsu）</p><p>2、视频转换<br>H264视频转ts视频流</p><p>ffmpeg -i test.h264 -vcodec copy -f mpegts test.ts</p><p>H264视频转mp4</p><p>ffmpeg -i test.h264 -vcodec copy -f mp4 test.mp4</p><p>ts视频转mp4</p><p>ffmpeg -i test.ts -acodec copy -vcodec copy -f mp4 test.mp4<br>mp4视频转flv<br>ffmpeg -i test.mp4 -acodec copy -vcodec copy -f flv test.flv </p><p>转换文件为3GP格式 </p><p>ffmpeg -y -i test.mpeg -bitexact -vcodec h263 -b 128 -r 15 -s 176x144 -acodec aac -ac 2 -ar 22500 -ab 24 -f 3gp test.3gp</p><p>转换文件为3GP格式 v2</p><p>ffmpeg -y -i test.wmv -ac 1 -acodec libamr_nb -ar 8000 -ab 12200 -s 176x144 -b 128 -r 15 test.3gp</p><p>使用 ffmpeg 编码得到高质量的视频</p><p>ffmpeg.exe -i “D:\Video\Fearless\Fearless.avi” -target film-dvd -s 720x352 -padtop 64 -padbottom 64 -maxrate 7350000 -b 3700000 -sc_threshold 1000000000 -trellis -cgop -g 12 -bf 2 -qblur 0.3 -qcomp 0.7 -me full -dc 10 -mbd 2 -aspect 16:9 -pass 2 -passlogfile “D:\Video\ffmpegencode” -an -f mpeg2video “D:\Fearless.m2v”</p><p>转换指定格式文件到FLV格式</p><p>ffmpeg.exe -i test.mp3 -ab 56 -ar 22050 -b 500 -r 15 -s 320x240 f:\test.flv<br>ffmpeg.exe -i test.wmv -ab 56 -ar 22050 -b 500 -r 15 -s 320x240 f:\test.flv</p><p>转码解密的VOB</p><p>ffmpeg -i snatch_1.vob -f avi -vcodec mpeg4 -b 800 -g 300 -bf 2 -acodec mp3 -ab 128 snatch.avi</p><p>（上面的命令行将vob的文件转化成avi文件，mpeg4的视频和mp3的音频。注意命令中使用了B帧，所以mpeg4流是divx5兼容的。GOP大小是300意味着29.97帧频下每10秒就有INTRA帧。该映射在音频语言的DVD转码时候尤其有用，同时编码到几种格式并且在输入流和输出流之间建立映射）</p><p>转换文件为3GP格式</p><p>ffmpeg -i test.avi -y -b 20 -s sqcif -r 10 -acodec amr_wb -ab 23.85 -ac 1 -ar 16000 test.3gp</p><p>（如果要转换为3GP格式，则ffmpeg在编译时必须加上–enable-amr_nb –enable-amr_wb，详细内容可参考：转换视频为3GPP格式）</p><p>转换文件为MP4格式（支持iPhone&#x2F;iTouch）</p><p>ffmpeg  -y  -i input.wmv  -f mp4 -async 1-s 480x320  -acodec libfaac -vcodec libxvid  -qscale 7 -dts_delta_threshold 1 output.mp4<br>ffmpeg  -y  -i source_video.avi input -acodec libfaac -ab 128000 -vcodec mpeg4 -b 1200000 -mbd 2 -flags +4mv+trell -aic 2 -cmp 2 -subcmp 2 -s 320x180 -title X final_video.mp4</p><p>将一段音频与一段视频混合</p><p>ffmpeg -i son.wav -i video_origine.avi video_finale.mpg</p><p>将一段视频转换为DVD格式</p><p>ffmpeg -i source_video.avi -target pal-dvd -ps 2000000000 -aspect 16:9 finale_video.mpeg</p><p>（target pal-dvd : Output format ps 2000000000 maximum size for the output file, in bits (here, 2 Gb) aspect 16:9 : Widescreen）</p><p>转换一段视频为DivX格式</p><p>ffmpeg -i video_origine.avi -s 320x240 -vcodec msmpeg4v2 video_finale.avi</p><p>Turn X images to a video sequence</p><p>ffmpeg -f image2 -i image%d.jpg video.mpg</p><p>（This command will transform all the images from the current directory (named image1.jpg, image2.jpg, etc…) to a video file named video.mpg.）</p><p>Turn a video to X images</p><p>ffmpeg -i video.mpg image%d.jpg</p><p>（This command will generate the files named image1.jpg, image2.jpg, … ；The following image formats are also availables : PGM, PPM, PAM, PGMYUV, JPEG, GIF, PNG, TIFF, SGI.）</p><p>使用ffmpeg录像屏幕(仅限Linux平台)</p><p>ffmpeg -vcodec mpeg4 -b 1000 -r 10 -g 300 -vd x11:0,0 -s 1024x768 ~&#x2F;test.avi</p><p>（-vd x11:0,0 指录制所使用的偏移为 x&#x3D;0 和 y&#x3D;0，-s 1024×768 指录制视频的大小为 1024×768。录制的视频文件为 test.avi，将保存到用户主目录中；如果你只想录制一个应用程序窗口或者桌面上的一个固定区域，那么可以指定偏移位置和区域大小。使用xwininfo -frame命令可以完成查找上述参数。）</p><p>重新调整视频尺寸大小(仅限Linux平台)</p><p>ffmpeg -vcodec mpeg4 -b 1000 -r 10 -g 300 -i ~&#x2F;test.avi -s 800×600 ~&#x2F;test-800-600.avi</p><p>把摄像头的实时视频录制下来，存储为文件(仅限Linux平台)</p><p>ffmpeg  -f video4linux -s 320*240 -r 10 -i &#x2F;dev&#x2F;video0 test.asf</p><p>使用ffmpeg压制H.264视频</p><p>ffmpeg -threads 4 -i INPUT -r 29.97 -vcodec libx264 -s 480x272 -flags +loop -cmp chroma -deblockalpha 0 -deblockbeta 0 -crf 24 -bt 256k -refs 1 -coder 0 -me umh -me_range 16 -subq 5 -partitions parti4x4+parti8x8+partp8x8 -g 250 -keyint_min 25 -level 30 -qmin 10 -qmax 51 -trellis 2 -sc_threshold 40 -i_qfactor 0.71 -acodec libfaac -ab 128k -ar 48000 -ac 2 OUTPUT</p><p>（使用该指令可以压缩出比较清晰，而且文件转小的H.264视频文件）</p><p>3、网络推送<br>udp视频流的推送<br>ffmpeg -re  -i 1.ts  -c copy -f mpegts   udp:&#x2F;&#x2F;192.168.0.106:1234</p><p>4、视频拼接<br>裸码流的拼接，先拼接裸码流，再做容器的封装<br>ffmpeg -i “concat:test1.h264|test2.h264” -vcodec copy -f h264 out12.h264</p><p>5、图像相关<br>截取一张352x240尺寸大小的，格式为jpg的图片 </p><p>ffmpeg -i test.asf -y -f image2 -t 0.001 -s 352x240 a.jpg</p><p>把视频的前30帧转换成一个Animated Gif </p><p>ffmpeg -i test.asf -vframes 30 -y -f gif a.gif</p><p>截取指定时间的缩微图,-ss后跟的时间单位为秒 </p><p>ffmpeg -i test.avi -y -f image2 -ss 8 -t 0.001 -s 350x240 test.jpg</p><p>6、音频处理</p><p>转换wav到mp2格式</p><p>ffmpeg -i &#x2F;tmp&#x2F;a.wav -ab 64 &#x2F;tmp&#x2F;a.mp2 -ab 128 &#x2F;tmp&#x2F;b.mp2 -map 0:0 -map 0:0</p><p>（上面的命令行转换一个64Kbits 的a.wav到128kbits的a.mp2 ‘-map file:index’在输出流的顺序上定义了哪一路输入流是用于每一个输出流的。）<br>7、切割ts分片<br>ffmpeg -i input.mp4 -c:v libx264 -c:a aac -strict -2 -f hls -hls_list_size 6 -hls_time 5 output1.m3u8</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ffmpeg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java输入的三种方式</title>
      <link href="/2018/01/02/java%E8%BE%93%E5%85%A5%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
      <url>/2018/01/02/java%E8%BE%93%E5%85%A5%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>import java.io.BufferedReader;<br>import java.io.IOException;<br>import java.io.InputStreamReader;<br>import java.util.Scanner;    </p><p>public class EnterTest {   </p><p>  public static void main(String[] args) { &#x2F;&#x2F;主方法<br>    CharTest();  &#x2F;&#x2F;调用System.in方法<br>    ReadTest();  &#x2F;&#x2F;调用ReadTest方法<br>    ScannerTest();&#x2F;&#x2F;调用ScannerTest方法<br>  }<br>  &#x2F;**  </p><ul><li>System.in和System.out方法  </li><li>缺点一: 该方法能获取从键盘输入的字符，但只能针对一个字符的获取  </li><li>缺点二: 获取的只是char类型的。如果想获得int,float等类型的输入,比较麻烦。<br>   *&#x2F;<br>  public static void CharTest(){<br>try{<br> System.out.print(“Enter a Char:”);<br> char i &#x3D; (char)System.in.read();<br> System.out.println(“Yout Enter Char is:” + i);<br>}<br>catch(IOException e){<br> e.printStackTrace();<br>}</li></ul><p>  }<br>  &#x2F;**  </p><ul><li>InputStreamReader和BufferedReader方法  </li><li>优点: 可以获取键盘输入的字符串  </li><li>缺点: 如何要获取的是int,float等类型的仍然需要转换<br>   *&#x2F;<br>  public static void ReadTest(){<br>System.out.println(“ReadTest, Please Enter Data:”);<br>InputStreamReader is &#x3D; new InputStreamReader(System.in); &#x2F;&#x2F;new构造InputStreamReader对象<br>BufferedReader br &#x3D; new BufferedReader(is); &#x2F;&#x2F;拿构造的方法传到BufferedReader中<br>try{ &#x2F;&#x2F;该方法中有个IOExcepiton需要捕获<br> String name &#x3D; br.readLine();<br> System.out.println(“ReadTest Output:” + name);<br>}<br>catch(IOException e){<br> e.printStackTrace();<br>}</li></ul><p>  }<br>  &#x2F;**  </p><ul><li>Scanner类中的方法  </li><li>优点一: 可以获取键盘输入的字符串  </li><li>优点二: 有现成的获取int,float等类型数据，非常强大，也非常方便；<br>   *&#x2F;<br>  public static void ScannerTest(){<br>Scanner sc &#x3D; new Scanner(System.in);<br>System.out.println(“ScannerTest, Please Enter Name:”);<br>String name &#x3D; sc.nextLine();  &#x2F;&#x2F;读取字符串型输入<br>System.out.println(“ScannerTest, Please Enter Age:”);<br>int age &#x3D; sc.nextInt();    &#x2F;&#x2F;读取整型输入<br>System.out.println(“ScannerTest, Please Enter Salary:”);<br>float salary &#x3D; sc.nextFloat(); &#x2F;&#x2F;读取float型输入<br>System.out.println(“Your Information is as below:”);<br>System.out.println(“Name:” + name +”\n” + “Age:”+age + “\n”+”Salary:”+salary);<br>  }<br>}</li></ul>]]></content>
      
      
      <categories>
          
          <category> 程序语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌镜像网站</title>
      <link href="/2017/12/27/%E8%B0%B7%E6%AD%8C%E9%95%9C%E5%83%8F%E7%BD%91%E7%AB%99/"/>
      <url>/2017/12/27/%E8%B0%B7%E6%AD%8C%E9%95%9C%E5%83%8F%E7%BD%91%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<p>作为一个程序员，经常需要上谷歌查点资料，想在大局域网内上谷歌，也不是件容易的事情，给大家推荐一个谷歌镜像导航网站：<a href="http://dir.scmor.com/google/">http://dir.scmor.com/google/</a></p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Google </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>闭包、候选码、最小依赖集</title>
      <link href="/2017/12/18/%E9%97%AD%E5%8C%85%E5%92%8C%E5%80%99%E9%80%89%E7%A0%81/"/>
      <url>/2017/12/18/%E9%97%AD%E5%8C%85%E5%92%8C%E5%80%99%E9%80%89%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h3 id="闭包概念"><a href="#闭包概念" class="headerlink" title="闭包概念"></a>闭包概念</h3><p>　　以下是写的比较科学规范的闭包求解方法，设X和Y均为关系R的属性集的子集，F是R上的函数依赖集，若对R的任一属性集B，一旦X→B，必有B⊆Y，且对R的任一满足以上条件的属性集Y1 ，必有Y⊆Y1，此时称Y为属性集X在函数依赖集F下的闭包，记作X＋。<br>　　计算关系R的属性集X的闭包的步骤如下：<br>　　第一步：设最终将成为闭包的属性集是Y，把Y初始化为X；<br>　　第二步：检查F中的每一个函数依赖A→B，如果属性集A中所有属性均在Y中，而B中有的属性不在Y中，则将其加入到Y中；<br>　　第三步：重复第二步，直到没有属性可以添加到属性集Y中为止。 最后得到的Y就是X＋<br>      例（1）：   设有关系模式R(U，F)，其中U&#x3D;{A，B，C，D，E，I}，F&#x3D;{A→D，AB→E，BI→E，CD→I，E→C}，计算(AE)+<br>        解:  (1) 令X&#x3D;{AE}，X(0)&#x3D;AE<br>              (2)在F中寻找尚未使用过的左边是AE的子集的函数依赖，结果是: A→D， E→C；所以 X(1)&#x3D;X(0)DC&#x3D;ACDE， 显然 X(1)≠X(0).<br>              (3) 在F中寻找尚未使用过的左边是ACDE的子集的函数依赖， 结果是: CD→I；所以 X(2)&#x3D;X(1)I&#x3D;ACDEI。虽然X（2）≠X(1)，但F中寻找尚未使用过函数依赖的左边已经没有X（2）的子集，所以不必再计算下去，即(AE)+&#x3D;ACDEI。<br>　　　说白话一点：闭包就是由一个属性直接或间接推导出的所有属性的集合。<br>         例如：f&#x3D;{a-&gt;b，b-&gt;c，a-&gt;d，e-&gt;f}；由a可直接得到b和d，间接得到c，则a的闭包就是{a，b，c，d}</p><h3 id="候选码的求解理论和算法"><a href="#候选码的求解理论和算法" class="headerlink" title="候选码的求解理论和算法"></a>候选码的求解理论和算法</h3><p>　　对于给定的关系R（A1，A2，…An）和函数依赖集F，可将其属性分为4类：<br>　　　　L类  仅出现在函数依赖左部的属性。<br>　　　　R 类  仅出现在函数依赖右部的属性。<br>　　　　N 类  在函数依赖左右两边均未出现的属性。<br>　　　　LR类  在函数依赖左右两边均出现的属性。<br>　　定理：对于给定的关系模式R及其函数依赖集F，若X（X∈R）是L类属性，则X必为R的任一候选码的成员。<br>　　推论：对于给定的关系模式R及其函数依赖集F，若X（X∈R）是L类属性，且X+包含了R的全部属性；则X必为R的唯一候选码。<br>　　例（2）：设有关系模式R（A，B，C，D），其函数依赖集F&#x3D;{D→B，B →D，AD →B，AC →D}，求R的所有候选码。<br>　　       解：考察F发现，A，C两属性是L类属性，所以AC必是R的候选码成员，又因为（AC）+&#x3D;ABCD，所以AC是R的唯一候选码。<br>　　定理：对于给定的关系模式R及其函数依赖集F，若X（X∈R）是R类属性，则X不在任何候选码中。<br>　　定理：对于给定的关系模式R及其函数依赖集F，若X（X∈R）是N类属性，则X必包含在R的任一候选码中。<br>　　推论：对于给定的关系模式R及其函数依赖集F，若X（X∈R）是L类和N类组成的属性集，且X+包含了R的全部属性；则X是R的唯一候选码。</p><h3 id="最小依赖集的求解"><a href="#最小依赖集的求解" class="headerlink" title="最小依赖集的求解"></a>最小依赖集的求解</h3><p>步骤：① 用分解的法则，使F中的任何一个函数依赖的右部仅含有一个属性；<br>　　　② 去掉多余的函数依赖：从第一个函数依赖X→Y开始将其从F中去掉，然后在剩下的函数依赖中求X的闭包X+，看X+是否包含Y，若是，则去掉X→Y；否则不能去掉，依次做下去。直到找不到冗余的函数依赖；<br>　　　③ 去掉各依赖左部多余的属性。一个一个地检查函数依赖左部非单个属性的依赖。例如XY→A，若要判Y为多余的，则以X→A代替XY→A是否等价？若A属于(X)+，则Y是多余属性，可以去掉。<br>下面我们来解答以下楼主提出的这个问题：<br>1、利用分解规则，将所有的函数依赖变成右边都是单个属性的函数依赖。从题目来看，F中的任何一个函数依赖的右部仅含有一个属性：<br>{A→B,B→A,B→C,A→C,C→A}<br>2、去掉F中多余的函数依赖<br>（1）设A→B冗余，从F中去掉A→B,则F1&#x3D;{B→A,B→C,A→C,C→A}。计算(A)F1+：设X(0)&#x3D;A,计算X(1)：扫描F1中各个函数依赖，找到左部为A或A子集的函数依赖，A→C。故有X(1)&#x3D;X(0)U C&#x3D;AC;扫描F1中各个函数依赖，找到左部为AC或为AC子集的函数依赖，C→A,X(2)&#x3D;X(1)U C&#x3D;AC.但AC不包含B，故A-&gt;B不能从Ｆ中去掉。<br>（2）设B→A冗余，从F中去掉B→A，则F2&#x3D;{A→B,B→C,A→C,C→A}。计算(B)F2+:设X(0)&#x3D;B，计算X(1)：扫描F2中各个函数依赖，找到左部为B或者B子集的函数依赖，B→C.故有X(1)&#x3D;X(0)U C &#x3D;BC;扫描F2中各个函数依赖，找到左部为BC或为BC子集的函数依赖，C-&gt;A,X(2)&#x3D;X(1)U A&#x3D;ABC.X(2)包含所有属性，故B→A可从Ｆ中去掉。<br>（3）设B→C冗余，从F中去掉B→C，则F3&#x3D;{A→B,A→C,C→A}。计算(B)F3+：扫描F3中各个函数依赖，找不到左部为B或B子集的函数依赖,因为找不到这样的函数依赖，故有X(1)&#x3D;X(0)&#x3D;B，(B)F1+&#x3D; B不包含C，故B→C不是冗余的函数依赖，不能从F1中去掉。<br>（4）设A→C冗余，从F中去掉A→C，则F4&#x3D;{A→B,B→C，C→A}。计算(A)F4+：设X(0)&#x3D;A,计算X(1)：扫描F4中各个函数依赖，找到左部为A或A子集的函数依赖，A→B。故有X(1)&#x3D;X(0)U B&#x3D;AB;扫描F4中各个函数依赖，找到左部为AB或为AB子集的函数依赖，B→C,X(2)&#x3D;X(1)U C&#x3D;ABC.X(2)包含所有属性，故A→C可从Ｆ中去掉。<br>（5）设C→A冗余，从F中去掉C→A，则F4&#x3D;{A→B,B→C}。计算(C)F5+：设X(0)&#x3D;C,计算X(1)：扫描F5中各个函数依赖，找到左部为C或C子集的函数依赖，找不到左部为C或C子集的函数依赖,因为找不到这样的函数依赖，故有X(1)&#x3D;X(0)&#x3D;C，(B)F1+&#x3D; C不包含A，故C→A不是冗余的函数依赖，不能从F中去掉。<br>（6）至此，所有依赖均以验算完毕，故F最小（极小）函数依赖集合为：{A→B,B→C，C→A}</p>]]></content>
      
      
      <categories>
          
          <category> 数据库系统概论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>STRENGTH</title>
      <link href="/2017/12/03/strength/"/>
      <url>/2017/12/03/strength/</url>
      
        <content type="html"><![CDATA[<h3 id="当你真心想要做某件事的时候，全世界都会联合起来帮助你。"><a href="#当你真心想要做某件事的时候，全世界都会联合起来帮助你。" class="headerlink" title="当你真心想要做某件事的时候，全世界都会联合起来帮助你。"></a>当你真心想要做某件事的时候，全世界都会联合起来帮助你。</h3><p>爷爷奶奶牵挂着自己，爸爸妈妈关注着自己，姐姐呵护着你～亲人都在以你为自豪，以你为骄傲。<br>心底的力量是多么无穷的无尽呀～<br>今天很开心，想要唱一首歌给你听～<br><em>啦啦啦 啦啦啦 啦啦啦啦啦啦啦<br>我把我唱给你听 最最最亲爱的人呐<br>路途遥远我的心很温暖呐</em><br>世界都是美好的～温暖如故～我宜奋斗～</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ngrok内网映射到外网</title>
      <link href="/2017/11/29/ngrok%E5%86%85%E7%BD%91%E6%98%A0%E5%B0%84%E5%88%B0%E5%A4%96%E7%BD%91/"/>
      <url>/2017/11/29/ngrok%E5%86%85%E7%BD%91%E6%98%A0%E5%B0%84%E5%88%B0%E5%A4%96%E7%BD%91/</url>
      
        <content type="html"><![CDATA[<p>首先cd到ngrok所在文件夹，然后直接命令</p><ul><li>.&#x2F;ngrok help</li><li>.&#x2F;ngrok http localhost:8080</li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ngrok </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络--物理层&amp;数据链路层</title>
      <link href="/2017/11/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E7%89%A9%E7%90%86%E5%B1%82-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82-1/"/>
      <url>/2017/11/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E7%89%A9%E7%90%86%E5%B1%82-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82-1/</url>
      
        <content type="html"><![CDATA[<p>根据TCP&#x2F;IP体系，计算机网络分成4层体系结构。学习时作5层：物理层（比特流）、数据链路层（帧）、网络层（IP数据报(分组))、运输层（报文）、应用层（PDU）。其中关系：对等层-&gt;协议；上下层-&gt;服务。</p><h3 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h3><p>物理层的数据传输单元为位(比特bit)。是指为上一层(数据链路层)提供原始(数据)比特流传输服务的一个可靠的物理连接(即通信通道)的<strong>所有</strong>构件。<br>功能：</p><ul><li>为相邻节点设备提供传送数据的通路(信道)。</li><li>为相邻节点设备提供透明、可靠的比特流传输。</li></ul><p>ISO称OSI四个功能要素的某种协议&#x2F;规范为特性：机械特性、电气特性、功能特性、过程（规程）特性。<br>按信道属性分类信道，与传输的信号方向有关的信道类型</p><ul><li>单向—单工信道</li><li>交错双向—半双工信道</li><li>同时双向信道—全双工信道</li></ul><h4 id="信噪比"><a href="#信噪比" class="headerlink" title="信噪比"></a>信噪比</h4><p>码元传输的速率越高，信号传输的距离越远，信号波形的失真就越严重。<br>信噪比即信号的电压与噪声电压的比，常以 S&#x2F;N 表示，记为分贝（dB）。信噪比应该越高越好</p><ul><li>信噪比 &#x3D; 10log<small>10</small>（S&#x2F;N) （dB）</li></ul><h4 id="香农定理"><a href="#香农定理" class="headerlink" title="香农定理"></a>香农定理</h4><p>香农定理用信号处理和信息理论等推导出了带宽受限且有高斯白噪声（随机热噪声）干扰的信道的极限信息传输速率。<br>信道的极限信息传输速率 C 可表达为</p><ul><li>C &#x3D; W log<small>2</small>(1+S&#x2F;N)  b&#x2F;s 或 bps<br>W：信道带宽；<br>S ：为信道内所传信号的平均功率；<br>N ：为信道内部的高斯噪声功率。</li></ul><p>表明：只要信息传输速率低于信道的极限信息传输速率，就一定可以找到某种办法来实现无差错的传输。</p><h3 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h3><p>为网络层结点（主机&#x2F;路由器等）等相邻结点之间提供通过公共或本地 信道进行（可靠、有效）数据帧（位流组）传输服务(支持)。</p><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><ul><li>封装成帧(framing):将上层(网络层)交下来的数据包按照所采用协议分别添加首部和尾部协议控制信息，构成了一个帧。帧首部+帧的数据部分(IP数据报)+帧尾部</li><li>帧同步：产生和识别帧界，界定帧的开始和结束位置</li><li>透明传输，有字节填充、位填充</li><li>差错检验与控制（CRC校验）</li><li>流量控制</li></ul><h4 id="点对点协议PPP"><a href="#点对点协议PPP" class="headerlink" title="点对点协议PPP"></a>点对点协议PPP</h4><p>3个主要组件：</p><ul><li>HDLC(High-level Data Link Control)高级数据链路控制–<big>&gt;</big>帧格式</li><li>LCP(Link Control Proocol)链路控制协议–<big>&gt;</big>链路管理</li><li>NCP(Network Control Procol)网络控制协议–<big>&gt;</big>与网络层协商用什么协议</li></ul><h4 id="以太网MAC地址结构"><a href="#以太网MAC地址结构" class="headerlink" title="以太网MAC地址结构"></a>以太网MAC地址结构</h4><p>以太网是一种基带总线局域网，局域网的数据链路层分为LLC(Logical Link Control)逻辑链路控制子层和MAC(Medium Access Control)媒体接入控制子层。<br>在局域网中<strong>硬件地址</strong>又称为物理地址或<strong>MAC地址</strong>(因为这种地址在MAC层中)。<br>MAC帧有三种</p><ul><li>多播(multicast)地址(帧)   (一对多传输帧)</li><li>单播(unicast)地址(帧)     (一对一传输帧)</li><li>广播(broadcast)地址(帧)   (一对全体传输帧)</li></ul><p>有效的 MAC 帧长度为 <strong>64 ~ 1518</strong> 字节之间。</p><h4 id="CSMA-x2F-CD"><a href="#CSMA-x2F-CD" class="headerlink" title="CSMA&#x2F;CD"></a>CSMA&#x2F;CD</h4><p>载波监听多点接入&#x2F;碰撞检测 CSMA&#x2F;CD（Carrier Sense Multiple Access with Collision Detection）</p><ul><li>退避算法</li><li>最小帧长的计算(四个参数：最小帧长、发送速率、传播时延、组网距离)</li><li>争用期与碰撞检测</li></ul><p>常用：</p><ul><li>最小帧64字节512bit</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>筑梦--清华伯克利</title>
      <link href="/2017/11/27/%E7%AD%91%E6%A2%A6-%E6%B8%85%E5%8D%8E%E4%BC%AF%E5%85%8B%E5%88%A9/"/>
      <url>/2017/11/27/%E7%AD%91%E6%A2%A6-%E6%B8%85%E5%8D%8E%E4%BC%AF%E5%85%8B%E5%88%A9/</url>
      
        <content type="html"><![CDATA[<ul><li>清华伯克利深圳学院</li><li>加油</li><li>命运在自己的手里，而不在别人的嘴里。</li><li>坚持到底，永不放弃！</li><li>坚持到底，永不放弃！</li></ul>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>我眼里的笑傲江湖</title>
      <link href="/2017/11/23/%E8%B0%88%E7%AC%91%E5%82%B2%E6%B1%9F%E6%B9%96/"/>
      <url>/2017/11/23/%E8%B0%88%E7%AC%91%E5%82%B2%E6%B1%9F%E6%B9%96/</url>
      
        <content type="html"><![CDATA[<p>打算整理一下自己读过的书。今天刚读完笑傲江湖，从这篇开始吧。</p><p>这是第一次看笑傲江湖，毕竟是武侠小说，不是我喜好的类型吧，没仔细品味金庸大师笔下文字的优美，我还是喜欢细节描写，武侠还是有些单调，笑傲江湖之所以被奉为武侠经典，定有其道理，我初读而已，还尚未领略其真魅力。<br>笑傲江湖是武侠小说，我却看的是爱情线。想写的很多，却发现自己早已文思枯竭，无法下笔，已经很难用文字来表达自己的情感了。<br>摘抄几段喜欢的地方吧：</p><ul><li><em>令狐冲慢慢转过身来，只见岳灵珊苗条的背影在左，林平之高高的背影在右，二人并肩而行。岳灵珊穿件湖绿衫子，翠绿裙子。林平之穿的是件淡黄色长袍。两人衣褛鲜洁，单看背影，便是一双才貌相当的璧人。令狐冲胸口便如有甚么东西塞住了，几乎气也透不过来。他和岳灵珊一别数月，虽然思念不绝，但今日一件，才知对她相爱之深。</em></li></ul><p>其实这段文字最能打动我的不是令狐冲对小师妹的丝丝痴情，而是小师妹和林平之平凡而不俗套的爱情描写，如果是真情的话，寥寥几笔的描写，却显得好不恩爱，而这等单纯的爱情，欢乐幸福也是自己所向往的吧。小师妹对林平之的爱是大概是真心的吧，而令狐冲的爱注定是无结果的痴恋，因为小师妹和他的视角大概是不同的。小师妹和令狐冲与其说是青梅竹马，不如说是令狐冲带着小师妹玩大的，小师妹喜欢的是他这个哥哥而非情人，敬佩居多，爱恋居少，在那个初恋懵懂的季节，情窦初开，无法细分，等到成熟，所以她能如此洒脱的爱上了林平之，她对林平之的爱是真爱，只可怜她爱了一个不该爱的人，小师妹本是纯真俏皮可爱的，奈何有一个伪君子爹爹，注定遇上林平之呢，她是可怜的，但她又是幸运的，因为有一个爱她的大师兄，她至少曾是他的全世界。</p><ul><li><em>突然之间，四下里万籁无声。少林寺寺内寺外聚集豪士数千之众，少室山自山腰以至山脚，正教中人至少也有二三千人，竟不约而同的谁都没有出声，便有人想说话的，也为这寂静的气氛所慑，话到嘴边都缩了回去。似乎只听到雪花落在树叶和丛草之上，发出轻柔异常的声音。令狐冲心中忽想：“小师妹这时候不知在干甚么？”</em></li></ul><p>全书有很多处令狐冲对小师妹的念念不忘之处，常言道：念念不忘，必有回响。纵使你是万丈豪杰的英雄男儿，又怎敌她倾城会心的回眸一笑，周幽王烽火戏诸侯，只为博卿笑，又有何妨？无论何时七尺男儿心中都有那么一块最柔软的地方，而那里住着另一个世界，她。<br>可这是一段虐恋。</p><ul><li><em>岳灵珊紧紧握着令狐冲的手，道：“大师哥，多……多谢你……我……我这可放心……放心了。”她眼中忽然发出光采，嘴角边露出微笑，一副心满意足的模样。<br>令狐冲见到她这等神情，心想：“能见到她这般开心，不论多大的艰难困苦，也值得为她抵受。”<br>忽然之间，岳灵珊轻轻唱起歌来。令狐冲胸口如受重击，听她唱的正是福建山歌，听到她口中吐出了“姊妹，上山采茶去”的曲调，那是林平之教她的福建山歌。当日在思过崖上心痛如绞，便是为了听到她口唱这山歌。她这时又唱了起来，自是想着当日与林平之在华山两情相悦的甜蜜时光。<br>她歌声越来越低，渐渐松开了抓着令狐冲的手，终于手掌一张，慢慢闭上了眼睛。歌声止歇，也停住了呼吸。<br>令狐冲心中一沉，似乎整个世界忽然间都死了，想要放声大哭，却又哭不出来。他伸出双手，将岳灵珊的身子抱了起来，轻轻叫道：“小师妹，小师妹，你别怕！我抱你到你妈妈那里去，没有人再欺侮你了。”</em></li></ul><p>她对他挚爱真情，纵是因死，也不曾恨，她是傻的；他对她痴恋情切，纵是悲情，也不曾弃，他亦是傻的。她死了，他的整个世界似乎也死了。<br><strong>“小师妹，你别怕，别怕！我抱你去见师娘。”</strong><br>小师妹和令狐冲，一个青涩，一个成熟，命运的齿轮早已注定。</p><p>两情相悦，贵乎自然。</p><ul><li><em>任、向二人彷徨无计，相对又望了一眼，目光中便只三个字：“怎么办？”任我行转过头来，向盈盈低声道：“你到对面去。”盈盈明白父亲的意思，他是怕令狐冲顾念昔日师门之恩，这一场比试要故意相让，他叫自己到对面去，是要令狐冲见到自己之后，想到自己待他的情意，便会出力取胜。她轻轻嗯了一声，却不移动脚步。过了片刻，任我行见令狐冲不住后退，更是焦急，又向盈盈道：“到前面去。”盈盈仍是不动，连“嗯”的那一声也不答应。她心中在想：“我待你如何，你早已知道。你如以我为重，决意救我下山，你自会取胜。你如以师父为重，我便是拉住你衣袖哀哀求告，也是无用。我何必站到你的面前来提醒你？”深觉两情相悦，贵乎自然，倘要自己有所示意之后，令狐冲再为自己打算，那可无味之极了。</em></li></ul><p>想写的还很多，文笔有限，时间有限，先写这么多吧。</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>笑傲江湖</title>
      <link href="/2017/11/23/%E7%AC%91%E5%82%B2%E6%B1%9F%E6%B9%96/"/>
      <url>/2017/11/23/%E7%AC%91%E5%82%B2%E6%B1%9F%E6%B9%96/</url>
      
        <content type="html"><![CDATA[<p>不知道自己在害怕什么，懒。<br>一直在逃避，逃避什么？<br>害怕失败？！所谓的执行力呢？浑浑噩噩！一味的逃避又能得来什么？陷入一个万劫不复的深渊，似那沼泽，越陷越深！抛弃一切烦恼！不要总是唯唯诺诺，缺乏安全之感，玩～玩不尽兴，学～学不踏实，如行尸走肉般，凡事都没有底气，还是自己实力不够！还是害怕失败！试一试吧，突破自我，从小事开始，咬牙坚持一下，突破那心中的壁垒，不要再囚禁自己，释放心中的猛虎。<br>是的，自己能行，自己能够做到，你可是曹广硕啊，你可是独一无二的呀。<br>不要找借口，不要抱怨，不要停滞！为了自己一生的成就。</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 筑梦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>接下来的路</title>
      <link href="/2017/11/21/%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9A%84%E8%B7%AF/"/>
      <url>/2017/11/21/%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9A%84%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<p>“你有一颗做学霸的心，却没有学霸的行动”这是伪学霸？不，这是最傻的。</p><ul><li>“ never give up，never give up！”</li></ul><p>丘吉尔说的对。<br>纵使你满腔热血，却不付诸行动，得到的仍然只是后悔。每天列满的提醒事项，变成了一个扰人的闹钟，还有什么能让你坚持下去呢？</p><ul><li>山不过来，你就过去</li></ul><p>古兰经说的对。<br>你懒又不愿承认，要高分却不努力，我觉得这是最可悲的地方。逆袭？当然也不会存在。</p><ul><li>立即行动</li></ul><p>必须做出突破！行动起来！不要犹豫，不要发呆，不要幻想，每一个踏实的脚印才能走好接下来的路，不留遗憾，不会后悔。</p><p>从六级开始、run～<br>然后2018考研年，筑梦传奇<br>然后考证：软件测试师、软件设计师<br>考证：软件架构师</p><p>寒假前看完3本书《js应用程序设计》《高性能js》《Spring》<br>寒假《数据结构》《操作系统》</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>关系数据库</title>
      <link href="/2017/11/21/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
      <url>/2017/11/21/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<p>一些术语：</p><ul><li>域 域是一组具有相同数据类型的值的集合</li><li>候选码 某一属性组的值能唯一地标识一个元组，而其他子集不能</li><li>主码 从候选码中选定的一个</li><li>关系模式 关系的描述 R(U,D,DOM,F)其中R为关系名，U为组成该关系的属性名集合，D为U中属性所来自的域，DOM为属性向域的映像集合，F为属性间数据的依赖关系集合</li></ul><p>关系的完整性：</p><ul><li>实体完整性 主属性不能为空</li><li>参照完整性 关系R的非主码属性F是关系S的主码，F是R的外码，R为参照关系，S为被参照关系</li><li>用户自定义的完整性</li></ul><p>关系代数：<br> 传统的集合二目运算：并、差、交、笛卡尔积<br> 专门的关系运算包括选择、投影、连接、除运算</p>]]></content>
      
      
      <categories>
          
          <category> 数据库系统概论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络---概述</title>
      <link href="/2017/11/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E6%A6%82%E8%BF%B0/"/>
      <url>/2017/11/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<p>计算机网络的基本组件有：服务、消息、介质、设备、规则-协议<br>计算机网络的作用：</p><ul><li>信息交换</li><li>资源共享</li><li>协同工作</li></ul><p>交换技术和交换网：</p><ul><li>电路交换技术及其交换网</li><li>分组交换</li><li>报文交换</li></ul><h3 id="电路交换"><a href="#电路交换" class="headerlink" title="电路交换"></a>电路交换</h3><p>要点：数据传输前需要建立一条端到端的通路，数据沿通道顺序传输。<br>过程分为3个阶段：建立连接-&gt;数据传输-&gt;释放连接。<br>优点：独占通道资源，延时小，实时性好<br>缺点：计算机数据传输的突发性造成信道空闲，利用率低；建立和释放连接时间占用信道资源；异构信道终端间的速率差错不易控制。</p><h3 id="分组交换"><a href="#分组交换" class="headerlink" title="分组交换"></a>分组交换</h3><p>优点：</p><ul><li>灵活：以分组为单位选择路由和传送。</li><li>迅速：不必先建立连接就能向其他主机发送分组。</li><li>可靠：(容错能力)基于分布式的路由选择协议使网络可以有很好的生存性。  </li><li>高效： 动态分配传输带宽，对通信链路是逐段占用。分组逐个传输，可以使分组结点内和结点间并行、流水线传输方式减少了报文的传输时间，因分组,减少了出错机率和重发数据量<br>缺点：</li><li>分组在各结点存储转发时需要排队，这就会造成一定的时延。 </li><li>分组必须携带的首部（里面有必不可少的控制信息）也造成了一定的开销。 </li><li>可能出现失序、丢失或重复分组</li></ul><h3 id="报文交换"><a href="#报文交换" class="headerlink" title="报文交换"></a>报文交换</h3><p>要点：要发送的信息不分组，封装路径等控制信息，递交报文结点交换机独立选路存储转发式传输。<br>优点： </p><ul><li>不需要为通信双方预先建立一条专用的通信线路，不存在连接建立时延，用户可随时发送报文。 </li><li>采用存储转发的传输方式，可灵活选择路径传输数据，可靠性高，易速率匹配，提供多目标服务，即一个报文可以同时发送到多个目的地址</li><li>通信双方不是固定占有一条通信线路，而是在不同的时间地占用不同的物理通路，提高了通信线路的利用率。<br>缺点： </li><li>存储、转发引起转发时延，时延与网络的通信量有关，实时性差</li><li>由于报文长度没有限制，要求网络中每个结点有较大的缓冲区。 </li><li>差错恢复较分组交换代价高</li></ul><h3 id="体系结构"><a href="#体系结构" class="headerlink" title="体系结构"></a>体系结构</h3><p>计算机网络的经典结构模型<br>    — OSI&#x2F;RM 体系结构<br>    — TCP&#x2F;IP  体系结构</p><h3 id="时延"><a href="#时延" class="headerlink" title="时延"></a>时延</h3><p>总时延 &#x3D; 发送时延+传播时延+处理时延+处理时延</p><ul><li>发送时延 &#x3D; 数据块长度(bite)&#x2F;信道宽度(bite&#x2F;s)</li><li>传播时延 &#x3D; 信道长度(m)&#x2F;信号在信道上的传播速率(m&#x2F;s)</li></ul><p>时延带宽积 &#x3D; 传播时延 × 带宽<br>链路的时延带宽积又称为以比特为单位的链路长度。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据库---绪论</title>
      <link href="/2017/11/19/%E6%95%B0%E6%8D%AE%E5%BA%93-%E7%BB%AA%E8%AE%BA/"/>
      <url>/2017/11/19/%E6%95%B0%E6%8D%AE%E5%BA%93-%E7%BB%AA%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>数据(Data)：描述事物的符号记录。<br>数据库(DataBase,DB)是长期储存在计算机内、有组织的、可共享的大量数据的集合。数据库中的数据按照一定的数据模型组织、描述和储存，具有较小的冗余度(redundancy)、较高的数据独立性(data independency)和易扩展性(scalability),并可为各种用户共享。<br>数据库系统(DBS)是由数据库、数据库管理系统(及其应用开发工具)、应用程序和数据库管理员(DBA)组成的存储、管理、处理和维护数据的系统。</p><h3 id="数据库系统的特点"><a href="#数据库系统的特点" class="headerlink" title="数据库系统的特点"></a>数据库系统的特点</h3><ul><li>数据结构化</li><li>数据的共享性高、冗余度低且易扩充</li><li>数据独立性高</li><li>数据由数据库管理系统统一管理和控制</li></ul><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>数据模型是对现实世界数据特征的抽象，是数据库系统的核心和基础。data model应该满足三方面要求</p><ul><li>能比较真实地模拟现实世界 </li><li>容易为人理解</li><li>便于在计算机上实现</li></ul><p>有两类数据模型：第一类是概念模型主要用于数据库设计、第二类是逻辑模型和物理模型</p><h3 id="概念模型"><a href="#概念模型" class="headerlink" title="概念模型"></a>概念模型</h3><p>数据模型的组成要素</p><ul><li>数据结构 描述数据库的组成对象以及对象之间的联系（实体间的联系）</li><li>数据操作 指对数据库中各种对象的实例允许执行的操作的集合（增删改查）</li><li>数据的完整性约束条件</li></ul><p>一些术语概念：</p><ul><li>实体：客观存在并可以相互区别的事物</li><li>属性：实体所具有的某一特性</li><li>码：唯一标识实体的属性集</li><li>实体型：用实体名及其属性名集合来抽象和刻画同类实体。例：学生(学号，姓名，性别)就是一个实体型</li><li>实体集：同一类型实体的集合。例：全体学生</li><li>联系：实体之间的联系有一对一、一对多、多对多</li></ul><p>关系模型的一些术语：</p><ul><li>关系(relation): 一个关系对应通常说的一张表，关系的每一个分量必须是一个不可分的数据项</li><li>元组(tuple): 表中的一行即为一个元组</li><li>属性(attribute)：表中的一列即为一个属性</li><li>码(key)：表中的某个属性组，也称为码键</li><li>域(domain)：域是一组具有相同数据类型的值的集合。属性的取值范围来自于某个域</li><li>分量：元组中的一个属性值</li><li>关系模式：对关系的描述。关系名（属性1，属性2,…属性n)</li></ul><h3 id="数据库系统的三级模式结构"><a href="#数据库系统的三级模式结构" class="headerlink" title="数据库系统的三级模式结构"></a>数据库系统的三级模式结构</h3><p>三级模式指外模式、模式、内模式<br>提供了两层映像：外模式&#x2F;模式映像（保证了数据的逻辑独立性）、模式&#x2F;内模式映像（保证了数据的物理独立性）</p>]]></content>
      
      
      <categories>
          
          <category> 数据库系统概论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>筑梦</title>
      <link href="/2017/11/19/%E7%AD%91%E6%A2%A6/"/>
      <url>/2017/11/19/%E7%AD%91%E6%A2%A6/</url>
      
        <content type="html"><![CDATA[<p>筑梦，一步一步，加油。<br>当你真心想要去做某件事时，整个宇宙都会联合起来帮助你。</p>]]></content>
      
      
      <categories>
          
          <category> 热爱生活 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
