<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NPMM | Cgs☛go</title><meta name="keywords" content="Dynamic Clustering,生成模型,Clustering Topic model"><meta name="author" content="cgshuo"><meta name="copyright" content="cgshuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="A nonparametric model for online topic discovery with word embeddings2019 Information science Abstract most online clustering models determine the probability of producing a new topic by manually sett">
<meta property="og:type" content="article">
<meta property="og:title" content="NPMM">
<meta property="og:url" content="http://www.cgsgo.com/2020/11/19/NPMM/index.html">
<meta property="og:site_name" content="Cgs☛go">
<meta property="og:description" content="A nonparametric model for online topic discovery with word embeddings2019 Information science Abstract most online clustering models determine the probability of producing a new topic by manually sett">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg">
<meta property="article:published_time" content="2020-11-19T01:50:41.000Z">
<meta property="article:modified_time" content="2020-11-30T09:13:23.180Z">
<meta property="article:author" content="cgshuo">
<meta property="article:tag" content="Dynamic Clustering">
<meta property="article:tag" content="生成模型">
<meta property="article:tag" content="Clustering Topic model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://www.cgsgo.com/2020/11/19/NPMM/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: cgshuo","link":"链接: ","source":"来源: Cgs☛go","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-11-30 17:13:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" data-lazy-src="/img/touxiang.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">62</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Cgs☛go</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">NPMM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-11-19T01:50:41.000Z" title="发表于 2020-11-19 09:50:41">2020-11-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-11-30T09:13:23.180Z" title="更新于 2020-11-30 17:13:23">2020-11-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Papers/">Papers</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="A-nonparametric-model-for-online-topic-discovery-with-word-embeddings"><a href="#A-nonparametric-model-for-online-topic-discovery-with-word-embeddings" class="headerlink" title="A nonparametric model for online topic discovery with word embeddings"></a>A nonparametric model for online topic discovery with word embeddings</h2><p>2019 Information science</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>most online clustering models determine the probability of producing a new topic by manually setting some hyper-parameter&#x2F;threshold, which becomes barrier to achieve better topic discovery results. Moreover, topics generated by using existing models often involve a wide coverage of the vocabulary which is not suitable for online social me- dia analysis</p>
</blockquote>
<p>在线聚类模型产生一个新的主题都需要手动设置一些超参数，而且，现有主题生成但模型的主题生成经常涉及词汇表的广泛覆盖，而这不适用于在线媒体分析。</p>
<blockquote>
<p>Therefore, we propose a nonparametric model (NPMM) which exploits auxiliary word embeddings to infer the topic number and employs a “spike and slab” function to alleviate the sparsity problem of topic-word distributions in online short text analyses. NPMM can automatically decide whether a given document belongs to existing topics, measured by the squared Mahalanobis distance. </p>
</blockquote>
<p>因此我们提出了一个无参数模型（NPMM）</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><blockquote>
<p>Generally speaking, model-based stream clustering approaches can be categorized into two groups: batch-based models and DP-based models.（ Dirichlet process (DP)）</p>
</blockquote>
<p>现在，概率图模型被广泛用于短文本流聚类，一般基于短文本流模型聚类的方法可以分为两类：基于批处理的模型和基于DP的模型（batch-based models and DP-based models）。</p>
<h3 id="1-1-Batch-based-models"><a href="#1-1-Batch-based-models" class="headerlink" title="1.1 Batch-based models"></a>1.1 Batch-based models</h3><p>基于批处理的对于传统在线动态聚类模型，是应用于长文本的，不适用于短文本聚类，这些模型无法处理主题词分布中稀疏性问题；还有一些短文本聚类模型，假设每个文档仅与一种主题相关，而不与多个主题相关联，该模型迭代处理当前批次中的文档，并在新批次到达时将其丢弃。基于批处理的模型的共同限制不仅在于它们缺乏即时处理功能，而且还需要手动设置批处理的大小。此外，上述所有模型在主题推断期间都需要预定义的主题编号。而这对于动态变化主题的聚类是很难的。</p>
<h3 id="1-2-DP-based-models"><a href="#1-2-DP-based-models" class="headerlink" title="1.2 DP-based models"></a>1.2 DP-based models</h3><blockquote>
<p>To deal with the problem of the unknown topic number and the requirement of the instant processing in the short text stream clustering, topic models exploiting Dirichlet process (DP) are proposed.</p>
</blockquote>
<p>为了解决短文本聚类中未知主题数量和即时处理问题，提出了Dirichlet process。</p>
<blockquote>
<p>In essence, existing topic models cannot address the problem of inferring a proper topic number when performing the short text stream clustering. In this paper, we propose a nonparametric model (NPMM) with word embeddings for the online short document clustering. </p>
</blockquote>
<p>实质上，现有的主题模型不能address(处理)推断正确主题数量的问题。在本论文中，我们提出了NPMM，带有词嵌入的非参数模型。</p>
<blockquote>
<p>Our idea is based on the following observation: semantic relations among words  can be stable for a long period of time. Therefore, given a comprehensive semantic space learned from a large modern external corpus, we can easily infer the topics hidden in the sparse short texts of social media. More specifically, the semantic space of word embeddings can be regarded as the prior knowledge when we derive semantic relations among social media data in the stream.</p>
</blockquote>
<p>我们的idea来自于以下的观察：单词之间的语义关系是长期稳定的，因此，给定一个大型的外部现在语料库中学到的全面语义空间，我们可以轻松的推断出社交媒体稀疏短文本中的隐藏主题。具体的说，当我们从社交媒体数据流中去除语义关系时，可以将单词嵌入的语义空间视为先验知识。</p>
<h3 id="1-3-summarization"><a href="#1-3-summarization" class="headerlink" title="1.3 summarization"></a>1.3 summarization</h3><blockquote>
<p>batch-based models require manually setting both the batch size and the number of topics, and lack the instant processing capability; DP-based models need manually setting a proper concentration hyper-parameter γ to control the probability of a new latent topic generation.</p>
</blockquote>
<p>batch-based model需要手动设置batch size以及主题数量，并且缺乏立即处理的能力；Dirichlet Process-based model需要手动设置超参数来控制新的潜在主题生成的概率。而且由于短文本的长度限制，稀疏性问题存在于文档主题分布以及主题词分布中。</p>
<blockquote>
<p>In this paper we propose NPMM to address the above stated challenges, which is characterized as follows:</p>
</blockquote>
<ul>
<li>Online processing. NPMM has one-pass clustering process for each arriving document, which can naturally deal with the streaming data.</li>
<li>Nonparametric topic discovery. NPMM incorporates the word embeddings in the online topic discovery, to release the parameter settings for generating new topics in the processing.</li>
<li>Sparsity.</li>
</ul>
<p>NPMM 加入词嵌入用于主题发现，NPMM可以自动发现主题通过计算新到达的文档和现有主题之间的距离，并使用卡方分布计算生成新主题的概率。NPMM通过“spike and slab” function减少主题词分布的稀疏性。</p>
<h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><blockquote>
<p>According to the general survey on the online data clustering in the stream clustering methods can be categorized into the following two groups: model-based stream clustering and threshold-based stream clustering. Moreover, the model-based methods can be further divided into two branches: DP-based models and batch-based models.</p>
</blockquote>
<p>根据文献中关于在线数据聚类的一般调查，流聚类方法可分为以下两类：基于模型的流聚类和基于阈值的流聚类。 此外，基于模型的方法可以进一步分为两个分支：DP-based model 和batch-based model。</p>
<h3 id="2-1-Model-based-stream-clustering"><a href="#2-1-Model-based-stream-clustering" class="headerlink" title="2.1 Model-based stream clustering"></a>2.1 Model-based stream clustering</h3><blockquote>
<p>Model-based stream clustering methods assume that documents are generated from probabilistic graphical models.</p>
</blockquote>
<p>基于模型的流聚类方法产生与概率图模型。它们通过主题词分布和文档主题分布对潜在语义空间建模，然后使用推理方法估计模型参数。</p><p><br>batch-based model: 经典LDA变体模型，当存在一定的不足，即它们设置了固定的主题数量（fixed-number),固定数字设置不适用于数据流聚类，无法处理主题演化问题。因此提出了DP-based model。</p><p><br>DP-based model: 应用广泛，但当前基于DP的模型的局限性在于，潜在主题生成的参数是手动设置的，并且搜索该参数的适当值非常耗时。</p>
<h3 id="2-2-Threshold-based-stream-clustering"><a href="#2-2-Threshold-based-stream-clustering" class="headerlink" title="2.2 Threshold-based stream clustering"></a>2.2 Threshold-based stream clustering</h3><p>基于阈值的流聚类方法的主要局限性在于，可以手动设置阈值以确定在线文档的主题分配。例如，新到达的文档通过将距离与预定义的阈值进行比较来选择最近的群集或新的群集。但是，在不同的数据集中搜索适当的阈值是一项耗时的操作。</p>
<h2 id="3-NPMM"><a href="#3-NPMM" class="headerlink" title="3. NPMM"></a>3. NPMM</h2><h3 id="3-1-1-Representative-terms"><a href="#3-1-1-Representative-terms" class="headerlink" title="3.1.1 Representative terms"></a>3.1.1 Representative terms</h3><p>NPMM使用词嵌入在全局语义空间中构造发现主题的多元高斯分布。当出现新文档时，我们可以通过计算该文档与高斯分布之间的距离来获得生成新主题的概率。但是，仅使用现有主题中的所有单词来构建基于单词嵌入的全局高斯分布可能会带来很大的噪音，因为有些单词与其主题之间的联系较弱。</p>
<blockquote>
<p>microsoft sony battle gaming supremacy holiday season</p>
</blockquote>
<p>其中holiday, season也是主题单词，但是核心是sony battle gaming，Therefore, we select “microsoft sony battle gaming supremacy” as the representative terms of this sentence.<br>我们可以如下获得主题中的representative terms:<br>$$\beta_{z,w} \sim Bernoulli(\lambda_{z,w})$$  </p>
<p>$$\lambda_{z,w} &#x3D; \frac{p(w|z)}{max_{w’}p(w’｜z)}, \qquad {\forall} w’\in z \tag{1}$$</p>
<p>$\beta_{z,w}$ 表示单词w是否是主题z中的代表词，$\lambda_{z,w}$与<em>“给定单词w的条件单词概率与$p(w’| z)$中的最大值即主题z的最大单词概率的比值”</em>相关。</p>
<h3 id="3-1-2-Global-semantic-space"><a href="#3-1-2-Global-semantic-space" class="headerlink" title="3.1.2 Global semantic space"></a>3.1.2 Global semantic space</h3><blockquote>
<p>The global semantic space G is represented as a multivariate normal distribution in the word embedding space, which<br>is constructed by the representative terms from each topic during the clustering. </p>
</blockquote>
<p>全局语义空间G表示为单词嵌入空间中的多元正态分布.</p>
<blockquote>
<p>In the scenario of social media streams, the newly coming topic is often outside the convex of those discovered topics, to be free of parameters and make the computation simple, we use just only one multivariate Gaussian to represent those discovered topics. </p>
</blockquote>
<p>在社交媒体流的场景中，新出现的主题通常不在那些发现的主题的凸面之外，因为没有参数并且简化了计算，我们仅使用一个多元高斯来表示那些发现的主题。</p>
<p>When a new document d comes, the squared Mahalanobis distance between each word w in d and G is calculated as follows:<br><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F5.png" alt="global semantic space G"></p>
<h3 id="3-1-3-The-spike-and-slab-priors"><a href="#3-1-3-The-spike-and-slab-priors" class="headerlink" title="3.1.3 The spike and slab priors"></a>3.1.3 The spike and slab priors</h3><blockquote>
<p>The spike and slab priors introduced to topic modeling are able to address the sparsity of topic-word distribution by using auxiliary Bernoulli variables to present the “on” and “off” of the priors, which indicate whether or not a term is selected by a topic as the representative term (or called the focused word). The selection process is given in Eq. (1). As in the real-world scenarios of short texts, a topic covers a narrow range of terms instead of a wide coverage of the vocabulary. It is hard for the sampling algorithm to distinguish relevant words from the irrelevant ones due to the small difference in word frequencies of short texts.</p>
</blockquote>
<p>引入主题建模的spike and slab priors能够通过使用辅助Bernoulli变量来表示先验的“开”和“关”来解决主题词分布的稀疏性。</p>
<h3 id="3-1-4-The-cluster-feature-CF-vector"><a href="#3-1-4-The-cluster-feature-CF-vector" class="headerlink" title="3.1.4 The cluster feature (CF) vector"></a>3.1.4 The cluster feature (CF) vector</h3><p>The cluster feature vector is used to represent a cluster, along with an addible property(可添加属性) and a deletable property(可删除属性).</p>
<p>CF vector for a cluster z is defined as a tuple <strong>(${n_z^w}, m_z, n_z$)</strong>, $n_z^w$是单词w在cluster类z中出现的次数(number of occurrences of word w), $m_z$是cluster z中documents的数量, $n_z$是the number of words in cluster z.我们用$D$表示记录文档(recorded documents)的集合，$V$表示记录文档的词汇的集合(we denote $V$ as the set of vocabulary of the recorded documents).</p>
<ul>
<li>(a). Addible proberty</li>
</ul>
<blockquote>
<p>When a new document d comes, it will be firstly assigned to a topic z in the one-pass(遍历) clustering process, the detail is given in the later section. Then, the CF vector and the recorder sets($i.e.$, $D$ and $V$) are updated in the following way:</p>
</blockquote>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F3.png" alt="addible property"></p>
<p>当有一个新文档d时，将在一次遍历聚类过程中首先将其分配给主题z，详细信息在later section, 然后更新CF vector 和 $D$、$V$。</p>
<ul>
<li>(b). Deletable property.<blockquote>
<p>Accordingly, when a document d is deleted from z in the update clustering process, the CF vector and the recorded sets are updated as follows:</p>
</blockquote>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F4.png" alt="deletable property"></p>
<p>After defining the above auxiliary components, the major tasks of NPMM given a data stream in a certain time range can be defined as(即当给定一个确定时间段数据流时，NPMM的主要任务如下)：</p>
<ol>
<li>Sample a set of representative terms from the existing clusters(or called topics);从已有聚类（或主题）中采样有代表性的terms(术语)。</li>
<li>Construct a semantic space G with the representative terms using a multivariate normal distributions.使用多元正态分布构造具有representative terms的语义空间G。</li>
<li>Calculate the probability of a newly arriving document d belong to the semantic G. 计算新到达文档d属于语义空间G的概率。</li>
<li>Learned the topic-word distribution $\psi$ and document-topic distribution $\theta$, respectively(各自地). 分别学习主题词分布$\psi$和文档主题分布$\theta$。</li>
</ol>
<p>All the notations used in this paper are summarized in table 1.</p>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E7%AC%A6%E5%8F%B7%E8%A1%A8%E7%A4%BA.png" alt="符号表示 table 1"></p>
<h3 id="3-2-Generative-process"><a href="#3-2-Generative-process" class="headerlink" title="3.2 Generative process"></a>3.2 Generative process</h3><p>在实践中，each document only focuses on one topic in short texts很有效，所以本文模型也遵循该假设。The graphical model of the proposed NPMM is given in Fig.1. </p>
<blockquote>
<p>In the generative process, we use the inverse Wishart distribution<a href="#refer-anchor"><sup>1</sup></a> as the conjugate prior  (共轭先验)<a href="#refer-anchor"><sup>2</sup></a> for the covariance(协方差) matrix of a multivariate normal of distrubution.In Bayesian statistics, it is widely used for estimating a multivariate normal distribution with unknown mean and covariance matrix.</p>
</blockquote>
<p>在生成过程中，我们使用Wishart逆分布<a href="#refer-archor"><sup>1<sup></sup></sup></a>用作多元正态分布协方差矩阵的共轭先验。在贝叶斯统计中，它被广泛用于估计均值和协方差矩阵未知的多元正态分布。</p>
<blockquote>
<p>Since a document is just related to one topic, there are two possible statuses for each newly arriving document in the clustering, either relevant or irrelevant to the existing global semantic space G.This status variable is denoted by $r$. In addition, $λ_{z,w}$ is as defined in $Eq. (1)$ and other notations are summarized in Table 1.</p>
</blockquote>
<p>The generative process is presented as follows:<br><img src= "/img/loading.gif" data-lazy-src="/.com//model.png" alt="model"><br><img src= "/img/loading.gif" data-lazy-src="/.com//NPMM.png" alt="Fig. 1"></p>
<ol>
<li>$Draw \   \Sigma_0 \sim W^{-1}(\Psi, v)$.</li>
<li>$Draw \  \mu_0 \sim N(\mu, \frac{1}{\tau}\Sigma_0)$.</li>
<li>$Draw \  a \  topic \  distribution \ \theta \sim Dirichlet(x)$</li>
<li>$For \  each \  term \ z \in {1,2,…,|z|}$:<br>$\quad(a) \  For\ each \ term \ w \in {1,2,…,|V|}$:<br>$\qquad i.\ Draw \ a \ focused \ term \ \beta_{z,w} \sim Bernoulli(\lambda_{z,w})$<br>$\quad(b)\ Draw\ a\ word\ distribution\ \psi_z \sim\ Dirichlet(\delta\beta_z +\epsilon1),\beta_z &#x3D;{\beta_z,w}_{w&#x3D;1}^{|V|}.$</li>
<li>$For \ each \ document \ d \in {1,2,…}$:<br>$\quad (a)\ Draw \ \eta_d \sim \chi_{dim}^2(\mu_0, \Sigma_0)$.<br>$\quad (b)\  Draw \ relevance \ r\  base \  on\  the\  representative \ term \ indicator \ \pi \ and\  p(\eta_d)\ (refer \ to\ Eqs.(3)\ and\ (4)).$<br>$\quad (c)\ If\ the\ document\ is\ relevant\ to\ the\ global\ semantic\ space\ G,\ i.e.,\ r\ &#x3D;\ 1$:<br>$\qquad i.\ Draw\ a\ topic\ z\ \sim \ Multinomial(\theta).$<br>$\qquad ii.\ Emit\ a\ word\ w \sim Multinomial(\psi_z).$<br>$\quad (d)\ If\ the\ document\ is\ irrelevant\ to\ the\ global\ semantic\ space\ G,\ i.e.,\ r\ &#x3D;\ 0$:<br>$\qquad i.\ Create\ a\ new\ topic\ z.$</li>
</ol>
<p>Here is a example:</p>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//example.png" alt="example"></p>
<h3 id="3-3-The-spark-and-slab-priors"><a href="#3-3-The-spark-and-slab-priors" class="headerlink" title="3.3 The spark and slab priors"></a>3.3 The spark and slab priors</h3><blockquote>
<p>When document d is sampled as relevant to the global semantic space G, we draw a topic assignment for it and apply the spike and slab priors on its word generation. The key idea of employing the priors is to restrict the size of the word simplex over Dirichlet distribution to reduce the sparsity in the topic-word distribution by incorporating a Bernoulli variable.</p>
</blockquote>
<p>当文档d被采样为语义空间G相关时，我们为其绘制一个主题分配（topic assignment），并在单词生成过程中应用<strong>spark and slab priors</strong>。采用先验的关键思想是通过合并Bernoulli变量来限制单词在Dirichlet分布上的大小，以此来较少主题单词分布的稀疏性（the sparsity in the topic-word distribution)。</p>
<p>The spark prior $\delta$ is much larger than the slab prior $\epsilon$, that is <strong>$\delta \gg \epsilon$</strong>. The representative term indicator $\beta_{z,w}$ serves as a switch of “on” and “off” to determine which term is a representative term,即 <strong>$\beta_{z,w}$用作指示器，确定哪个term是代表性的</strong>。</p>
<p>In the previous case, when $r &#x3D; 1$, a topic $z$ is sampled from $\theta$, and a word is emitted from $\psi_z \sim Dirichlet(\delta\beta_{z}+\epsilon1)$. This does not violate(违反) the definition of a multinomial distribution(多项式分布) as $\sum_{w&#x3D;1}^{|V|}\psi_{z,w}&#x3D;1$ where $|V|$ is the size of the vocabulary of current recorded documents. 当r&#x3D;1时，从$\theta$采样$z$, 从$\psi_z \sim Dirichlet(\delta\beta_{z}+\epsilon1)$采样出单词。$|V|$是当前记录文档的词汇量。</p>
<blockquote>
<p>The spike-and-slab prior setting w&#x3D;1 enables the model to generate more coherent topics. Besides, when r &#x3D; 0 that d is sampled as irrelevant to G, we create a new topic for d and also update the global space G as well.</p>
</blockquote>
<p>spike-and-slab prior预先设置w &#x3D; 1使模型能够生成更连贯的主题。此外，当r &#x3D; 0时，采样到的d与G不相关，我们为d创建了一个新主题，并且还更新了全局空间G。</p>
<h3 id="3-4-The-NPMM-algorithms"><a href="#3-4-The-NPMM-algorithms" class="headerlink" title="3.4 The NPMM algorithms"></a>3.4 The NPMM algorithms</h3><blockquote>
<p>The details of the clustering process for the proposed NPMM model are shown in Algorithms 1 and 2 . The meanings of the notations are as given in Table 1. As mentioned in the introduction section, NPMM has one-pass clustering process and update clustering process. The one-pass scheme (lines 5–8 in Algorithm 1) grants NPMM the instant processing capability to handle the massive amount of short text streams. And the update clustering scheme (or called batch scheme) (lines 9–25 in Algorithm 1) enables NPMM to achieve a better performance through multiple iterations. In addition, Algorithm 2 is a common operation for sampling the latent variables (details in the following section) in NPMM.</p>
</blockquote>
<p>算法1和2中显示了所提出的NPMM模型的聚类过程的详细信息。 表1给出了这些符号的含义。如引言部分所述，NPMM具有一遍聚类过程和更新聚类过程。 one-pass方案（算法1中的第5-8行）授予NPMM即时处理能力，以处理大量的短文本流。 更新聚类方案（或称为批处理方案）（算法1中的9–25行）使NPMM可以通过多次迭代获得更好的性能。 另外，算法2是用于在NPMM中对潜在变量（下一节中的详细信息）进行采样的通用操作。</p>
<h2 id="4-Inference"><a href="#4-Inference" class="headerlink" title="4. Inference"></a>4. Inference</h2><p>对参数推理使用Gibbs sampling<a href="#refer-anchor"><sup>3</sup></a>。模型中有5个latent variables，包括topic assignments $z$, relevance indicator $r$, the posterior normal distribution parameters $\mu_0$ and $\Sigma_0$, and the focused terms $\beta_z$ in each topic.</p>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F7.png" alt="公式7"><br><img src= "/img/loading.gif" data-lazy-src="/.com//A1.png" alt="Algorithm1"><br><img src= "/img/loading.gif" data-lazy-src="/.com//S1.png" alt="Sampling1"> <img src= "/img/loading.gif" data-lazy-src="/.com//s1_2.png" alt="Sampling2"></p>
<h3 id="4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition"><a href="#4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition" class="headerlink" title="4.1. Reduction of the sampling complexity with cholesky decomposition"></a>4.1. Reduction of the sampling complexity with cholesky decomposition</h3><p><img src= "/img/loading.gif" data-lazy-src="/.com//4.png" alt="4"><br><img src= "/img/loading.gif" data-lazy-src="/.com//4_1.png" alt="4.1"></p>
<h3 id="4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step"><a href="#4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step" class="headerlink" title="4.2. Improved reduction of the sampling complexity with metropolis hastings step"></a>4.2. Improved reduction of the sampling complexity with metropolis hastings step</h3><blockquote>
<p>The Metropolis Hastings (MH) algorithm is a Markov chain Monte Carlo method for obtaining a certain number of random samples from a stale probability distribution. The precondition of using MH is that the objective distribution changes relatively slowly and its changing distribution is similar to the stale one. From the observation of the parameters ($\mu_0$ and $\Sigma_0$) in the global space G, they do not change drastically during the clustering. One possible explanation is that in the real world, the number and the content of topics are relatively stable in a certain time window (e.g., people always like to talk about the latest news on social media platforms). We can exploit this observation and employ MH step in our sampling process (i.e., obtaining a few samples with the stale distribution of the global space G).</p><p><br>Therefore, combining Cholesky decomposition with the MH step, the sampling complexity of calculation can be brought down from $O(IDn^2)$ to $O(I\frac{D}{H}n^2)$, where H represents the number of MH steps used in the clustering and $\frac{D}{H} \ll D$. The experiment results (i.e., sampling naively, with Cholesky decomposition (CH), with CH+MH step) of the running time will be demonstrated in the following experiment section.</p>
</blockquote>
<p>Metropolis Hastings（MH）<a href="#refer-anchor"><sup>4</sup></a>算法是马尔可夫链蒙特卡罗方法，用于从陈旧的概率分布中获取一定数量的随机样本。使用MH的前提是目标分布变化相对较慢，并且其变化分布与陈旧状态相似。从全局空间G中的参数（μ0和􏰉0）观察，它们在聚类期间不会发生剧烈变化。一种可能的解释是，在现实世界中，主题的数量和内容在一定的时间范围内相对稳定（例如，人们总是喜欢在社交媒体平台上谈论最新新闻）。我们可以利用这一观察结果并在我们的采样过程中采用MH步骤（即获取一些具有全球空间G的陈旧分布的样本）。<br>因此，将Cholesky分解与MH步结合起来，可以将计算的采样复杂度从O$O(IDn^2)$ 降低到$O(I\frac{D}{H}n^2)$，其中H代表在聚类中使用的MH步数和$\frac{D}{H} \ll D$。运行时间的实验结果（即，通过Cholesky分解（CH），CH + MH步进行天真采样）将在以下实验部分中演示。</p>
<h2 id="5-Experiment"><a href="#5-Experiment" class="headerlink" title="5. Experiment"></a>5. Experiment</h2><h3 id="5-1-Datasets"><a href="#5-1-Datasets" class="headerlink" title="5.1 Datasets"></a>5.1 Datasets</h3><p>Two real-world datasets from Google News and Twitter, and two variants of them are used in the experimental study.<br><img src= "/img/loading.gif" data-lazy-src="/.com//dataset.png" alt="dataset"></p>
<blockquote>
<p>Google News <a target="_blank" rel="noopener" href="https://news.google.com/news/">https://news.google.com/news/</a><br>TweetSet <a target="_blank" rel="noopener" href="https://trec.nist.gov/data/microblog.html">https://trec.nist.gov/data/microblog.html</a></p>
</blockquote>
<h2 id="6-Conclustion-and-future-work"><a href="#6-Conclustion-and-future-work" class="headerlink" title="6. Conclustion and future work"></a>6. Conclustion and future work</h2><blockquote>
<p>In this paper, we have proposed a nonparametric topic model (NPMM) with auxiliary word embeddings for online topic discovery. NPMM can discover a new topic by computing the probabilities of a document belonging to the existing topics and a new one. NPMM can achieve the state-of-the-art performance of online clustering with one-pass process, and can have even better performance with multiple iterations. Moreover, after obtaining the representative terms from each topic, NPMM can exploit the spike and slab priors function to amplify the contrast of word generation probabilities between the relevant words and the irrelevant ones, which alleviates the sparsity problem of the topic-word distribution in the short text clustering. In addition, in order to speed up the sampling process, we have proposed two improved sampling methods (i.e., NPMM with CH and NPMM with CH+MH step) to compare with the naive sampling one. Our extensive experimental study has shown that NPMM with CH+MH100 step can achieve similar performance in less time compared with the naive sampling method on real-life datasets.</p>
</blockquote>
<p>In the future work, we intend to exploit NPMM to improve the performance of other text mining applications, such as event detection, search result diversification, and text classification.</p>
<div id="refer-anchor"></div>

<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[0] A, Junyang Chen , Z. G. A , and W. L. B . “A nonparametric model for online topic discovery with word embeddings.” Information sciences 504(2019):32-47.<br><a href="/2020/11/30/Wishart-%E5%88%86%E5%B8%83%E5%8F%8A%E9%80%86%E5%88%86%E5%B8%83/">[1] Wishart分布与Wishart逆分布</a><br><a href="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/">[2] 共轭分布</a><br>[3] H. Amoualian, M. Clausel, E. Gaussier, M.-R. Amini, Streaming-LDA: A copula-based approach to modeling topic dependencies in document streams, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2016, pp. 695–704. (Gipps Sampling)<br>[4] S. Chib, E. Greenberg, Understanding the metropolis-hastings algorithm, Am. Stat. 49 (4) (1995) 327–335. (MH马尔可夫链蒙特卡罗方法)</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">cgshuo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.cgsgo.com/2020/11/19/NPMM/">http://www.cgsgo.com/2020/11/19/NPMM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.cgsgo.com" target="_blank">Cgs☛go</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Dynamic-Clustering/">Dynamic Clustering</a><a class="post-meta__tags" href="/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">生成模型</a><a class="post-meta__tags" href="/tags/Clustering-Topic-model/">Clustering Topic model</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" data-lazy-src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" data-lazy-src="/img/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/23/array%E4%B8%89%E7%BB%B4%E6%8B%BC%E4%BA%8C%E7%BB%B4/"><img class="prev-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">array三维拼二维</div></div></a></div><div class="next-post pull-right"><a href="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/"><img class="next-cover" data-lazy-src="https://i.loli.net/2020/11/30/t8TzPrwuCnA75mv.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Dynamic Clustering of Streaming Short</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/" title="Dynamic Clustering of Streaming Short"><img class="cover" data-lazy-src="https://i.loli.net/2020/11/30/t8TzPrwuCnA75mv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-16</div><div class="title">Dynamic Clustering of Streaming Short</div></div></a></div><div><a href="/2021/01/21/Householder-flow/" title="Householder flow"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-21</div><div class="title">Householder flow</div></div></a></div><div><a href="/2020/11/25/LDA/" title="LDA"><img class="cover" data-lazy-src="https://i.loli.net/2020/11/30/roijB1RqF47mtAy.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-25</div><div class="title">LDA</div></div></a></div><div><a href="/2021/01/23/lstm-vae调研/" title="lstm-vae调研"><img class="cover" data-lazy-src="https://i.loli.net/2020/11/30/J1OKpd6fINCUzPb.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-23</div><div class="title">lstm-vae调研</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" data-lazy-src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">cgshuo</div><div class="author-info__description">热爱生活、拥抱世界，脚踏实地、不忘初心。😊</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">62</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cgshuo"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cgshuo" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:cgshuo@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">快来和我一起学习啦！！！</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#A-nonparametric-model-for-online-topic-discovery-with-word-embeddings"><span class="toc-number">1.</span> <span class="toc-text">A nonparametric model for online topic discovery with word embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">2.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">3.</span> <span class="toc-text">1. Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Batch-based-models"><span class="toc-number">3.1.</span> <span class="toc-text">1.1 Batch-based models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-DP-based-models"><span class="toc-number">3.2.</span> <span class="toc-text">1.2 DP-based models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-summarization"><span class="toc-number">3.3.</span> <span class="toc-text">1.3 summarization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-work"><span class="toc-number">4.</span> <span class="toc-text">2. Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Model-based-stream-clustering"><span class="toc-number">4.1.</span> <span class="toc-text">2.1 Model-based stream clustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Threshold-based-stream-clustering"><span class="toc-number">4.2.</span> <span class="toc-text">2.2 Threshold-based stream clustering</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-NPMM"><span class="toc-number">5.</span> <span class="toc-text">3. NPMM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-Representative-terms"><span class="toc-number">5.1.</span> <span class="toc-text">3.1.1 Representative terms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-Global-semantic-space"><span class="toc-number">5.2.</span> <span class="toc-text">3.1.2 Global semantic space</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-The-spike-and-slab-priors"><span class="toc-number">5.3.</span> <span class="toc-text">3.1.3 The spike and slab priors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-The-cluster-feature-CF-vector"><span class="toc-number">5.4.</span> <span class="toc-text">3.1.4 The cluster feature (CF) vector</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Generative-process"><span class="toc-number">5.5.</span> <span class="toc-text">3.2 Generative process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-The-spark-and-slab-priors"><span class="toc-number">5.6.</span> <span class="toc-text">3.3 The spark and slab priors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-The-NPMM-algorithms"><span class="toc-number">5.7.</span> <span class="toc-text">3.4 The NPMM algorithms</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Inference"><span class="toc-number">6.</span> <span class="toc-text">4. Inference</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition"><span class="toc-number">6.1.</span> <span class="toc-text">4.1. Reduction of the sampling complexity with cholesky decomposition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step"><span class="toc-number">6.2.</span> <span class="toc-text">4.2. Improved reduction of the sampling complexity with metropolis hastings step</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiment"><span class="toc-number">7.</span> <span class="toc-text">5. Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Datasets"><span class="toc-number">7.1.</span> <span class="toc-text">5.1 Datasets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Conclustion-and-future-work"><span class="toc-number">8.</span> <span class="toc-text">6. Conclustion and future work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">8.1.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2022 By cgshuo</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script></div></body></html>