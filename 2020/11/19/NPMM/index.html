<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NPMM | Cgsâ˜›go</title><meta name="keywords" content="Dynamic Clustering,ç”Ÿæˆæ¨¡å‹,Clustering Topic model"><meta name="author" content="cgshuo"><meta name="copyright" content="cgshuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="A nonparametric model for online topic discovery with word embeddings2019 Information science Abstract most online clustering models determine the probability of producing a new topic by manually sett">
<meta property="og:type" content="article">
<meta property="og:title" content="NPMM">
<meta property="og:url" content="http://www.cgsgo.com/2020/11/19/NPMM/index.html">
<meta property="og:site_name" content="Cgsâ˜›go">
<meta property="og:description" content="A nonparametric model for online topic discovery with word embeddings2019 Information science Abstract most online clustering models determine the probability of producing a new topic by manually sett">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg">
<meta property="article:published_time" content="2020-11-19T01:50:41.000Z">
<meta property="article:modified_time" content="2020-11-30T09:13:23.180Z">
<meta property="article:author" content="cgshuo">
<meta property="article:tag" content="Dynamic Clustering">
<meta property="article:tag" content="ç”Ÿæˆæ¨¡å‹">
<meta property="article:tag" content="Clustering Topic model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://www.cgsgo.com/2020/11/19/NPMM/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: {"limitCount":50,"languages":{"author":"ä½œè€…: cgshuo","link":"é“¾æ¥: ","source":"æ¥æº: Cgsâ˜›go","info":"è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚"}},
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-11-30 17:13:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" data-lazy-src="/img/touxiang.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">84</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">62</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Cgsâ˜›go</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">NPMM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2020-11-19T01:50:41.000Z" title="å‘è¡¨äº 2020-11-19 09:50:41">2020-11-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2020-11-30T09:13:23.180Z" title="æ›´æ–°äº 2020-11-30 17:13:23">2020-11-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Papers/">Papers</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">4.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>21åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="A-nonparametric-model-for-online-topic-discovery-with-word-embeddings"><a href="#A-nonparametric-model-for-online-topic-discovery-with-word-embeddings" class="headerlink" title="A nonparametric model for online topic discovery with word embeddings"></a>A nonparametric model for online topic discovery with word embeddings</h2><p>2019 Information science</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>most online clustering models determine the probability of producing a new topic by manually setting some hyper-parameter&#x2F;threshold, which becomes barrier to achieve better topic discovery results. Moreover, topics generated by using existing models often involve a wide coverage of the vocabulary which is not suitable for online social me- dia analysis</p>
</blockquote>
<p>åœ¨çº¿èšç±»æ¨¡å‹äº§ç”Ÿä¸€ä¸ªæ–°çš„ä¸»é¢˜éƒ½éœ€è¦æ‰‹åŠ¨è®¾ç½®ä¸€äº›è¶…å‚æ•°ï¼Œè€Œä¸”ï¼Œç°æœ‰ä¸»é¢˜ç”Ÿæˆä½†æ¨¡å‹çš„ä¸»é¢˜ç”Ÿæˆç»å¸¸æ¶‰åŠè¯æ±‡è¡¨çš„å¹¿æ³›è¦†ç›–ï¼Œè€Œè¿™ä¸é€‚ç”¨äºåœ¨çº¿åª’ä½“åˆ†æã€‚</p>
<blockquote>
<p>Therefore, we propose a nonparametric model (NPMM) which exploits auxiliary word embeddings to infer the topic number and employs a â€œspike and slabâ€ function to alleviate the sparsity problem of topic-word distributions in online short text analyses. NPMM can automatically decide whether a given document belongs to existing topics, measured by the squared Mahalanobis distance. </p>
</blockquote>
<p>å› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— å‚æ•°æ¨¡å‹ï¼ˆNPMMï¼‰</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><blockquote>
<p>Generally speaking, model-based stream clustering approaches can be categorized into two groups: batch-based models and DP-based models.ï¼ˆ Dirichlet process (DP)ï¼‰</p>
</blockquote>
<p>ç°åœ¨ï¼Œæ¦‚ç‡å›¾æ¨¡å‹è¢«å¹¿æ³›ç”¨äºçŸ­æ–‡æœ¬æµèšç±»ï¼Œä¸€èˆ¬åŸºäºçŸ­æ–‡æœ¬æµæ¨¡å‹èšç±»çš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºæ‰¹å¤„ç†çš„æ¨¡å‹å’ŒåŸºäºDPçš„æ¨¡å‹ï¼ˆbatch-based models and DP-based modelsï¼‰ã€‚</p>
<h3 id="1-1-Batch-based-models"><a href="#1-1-Batch-based-models" class="headerlink" title="1.1 Batch-based models"></a>1.1 Batch-based models</h3><p>åŸºäºæ‰¹å¤„ç†çš„å¯¹äºä¼ ç»Ÿåœ¨çº¿åŠ¨æ€èšç±»æ¨¡å‹ï¼Œæ˜¯åº”ç”¨äºé•¿æ–‡æœ¬çš„ï¼Œä¸é€‚ç”¨äºçŸ­æ–‡æœ¬èšç±»ï¼Œè¿™äº›æ¨¡å‹æ— æ³•å¤„ç†ä¸»é¢˜è¯åˆ†å¸ƒä¸­ç¨€ç–æ€§é—®é¢˜ï¼›è¿˜æœ‰ä¸€äº›çŸ­æ–‡æœ¬èšç±»æ¨¡å‹ï¼Œå‡è®¾æ¯ä¸ªæ–‡æ¡£ä»…ä¸ä¸€ç§ä¸»é¢˜ç›¸å…³ï¼Œè€Œä¸ä¸å¤šä¸ªä¸»é¢˜ç›¸å…³è”ï¼Œè¯¥æ¨¡å‹è¿­ä»£å¤„ç†å½“å‰æ‰¹æ¬¡ä¸­çš„æ–‡æ¡£ï¼Œå¹¶åœ¨æ–°æ‰¹æ¬¡åˆ°è¾¾æ—¶å°†å…¶ä¸¢å¼ƒã€‚åŸºäºæ‰¹å¤„ç†çš„æ¨¡å‹çš„å…±åŒé™åˆ¶ä¸ä»…åœ¨äºå®ƒä»¬ç¼ºä¹å³æ—¶å¤„ç†åŠŸèƒ½ï¼Œè€Œä¸”è¿˜éœ€è¦æ‰‹åŠ¨è®¾ç½®æ‰¹å¤„ç†çš„å¤§å°ã€‚æ­¤å¤–ï¼Œä¸Šè¿°æ‰€æœ‰æ¨¡å‹åœ¨ä¸»é¢˜æ¨æ–­æœŸé—´éƒ½éœ€è¦é¢„å®šä¹‰çš„ä¸»é¢˜ç¼–å·ã€‚è€Œè¿™å¯¹äºåŠ¨æ€å˜åŒ–ä¸»é¢˜çš„èšç±»æ˜¯å¾ˆéš¾çš„ã€‚</p>
<h3 id="1-2-DP-based-models"><a href="#1-2-DP-based-models" class="headerlink" title="1.2 DP-based models"></a>1.2 DP-based models</h3><blockquote>
<p>To deal with the problem of the unknown topic number and the requirement of the instant processing in the short text stream clustering, topic models exploiting Dirichlet process (DP) are proposed.</p>
</blockquote>
<p>ä¸ºäº†è§£å†³çŸ­æ–‡æœ¬èšç±»ä¸­æœªçŸ¥ä¸»é¢˜æ•°é‡å’Œå³æ—¶å¤„ç†é—®é¢˜ï¼Œæå‡ºäº†Dirichlet processã€‚</p>
<blockquote>
<p>In essence, existing topic models cannot address the problem of inferring a proper topic number when performing the short text stream clustering. In this paper, we propose a nonparametric model (NPMM) with word embeddings for the online short document clustering. </p>
</blockquote>
<p>å®è´¨ä¸Šï¼Œç°æœ‰çš„ä¸»é¢˜æ¨¡å‹ä¸èƒ½address(å¤„ç†)æ¨æ–­æ­£ç¡®ä¸»é¢˜æ•°é‡çš„é—®é¢˜ã€‚åœ¨æœ¬è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NPMMï¼Œå¸¦æœ‰è¯åµŒå…¥çš„éå‚æ•°æ¨¡å‹ã€‚</p>
<blockquote>
<p>Our idea is based on the following observation: semantic relations among words  can be stable for a long period of time. Therefore, given a comprehensive semantic space learned from a large modern external corpus, we can easily infer the topics hidden in the sparse short texts of social media. More specifically, the semantic space of word embeddings can be regarded as the prior knowledge when we derive semantic relations among social media data in the stream.</p>
</blockquote>
<p>æˆ‘ä»¬çš„ideaæ¥è‡ªäºä»¥ä¸‹çš„è§‚å¯Ÿï¼šå•è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»æ˜¯é•¿æœŸç¨³å®šçš„ï¼Œå› æ­¤ï¼Œç»™å®šä¸€ä¸ªå¤§å‹çš„å¤–éƒ¨ç°åœ¨è¯­æ–™åº“ä¸­å­¦åˆ°çš„å…¨é¢è¯­ä¹‰ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾çš„æ¨æ–­å‡ºç¤¾äº¤åª’ä½“ç¨€ç–çŸ­æ–‡æœ¬ä¸­çš„éšè—ä¸»é¢˜ã€‚å…·ä½“çš„è¯´ï¼Œå½“æˆ‘ä»¬ä»ç¤¾äº¤åª’ä½“æ•°æ®æµä¸­å»é™¤è¯­ä¹‰å…³ç³»æ—¶ï¼Œå¯ä»¥å°†å•è¯åµŒå…¥çš„è¯­ä¹‰ç©ºé—´è§†ä¸ºå…ˆéªŒçŸ¥è¯†ã€‚</p>
<h3 id="1-3-summarization"><a href="#1-3-summarization" class="headerlink" title="1.3 summarization"></a>1.3 summarization</h3><blockquote>
<p>batch-based models require manually setting both the batch size and the number of topics, and lack the instant processing capability; DP-based models need manually setting a proper concentration hyper-parameter Î³ to control the probability of a new latent topic generation.</p>
</blockquote>
<p>batch-based modeléœ€è¦æ‰‹åŠ¨è®¾ç½®batch sizeä»¥åŠä¸»é¢˜æ•°é‡ï¼Œå¹¶ä¸”ç¼ºä¹ç«‹å³å¤„ç†çš„èƒ½åŠ›ï¼›Dirichlet Process-based modeléœ€è¦æ‰‹åŠ¨è®¾ç½®è¶…å‚æ•°æ¥æ§åˆ¶æ–°çš„æ½œåœ¨ä¸»é¢˜ç”Ÿæˆçš„æ¦‚ç‡ã€‚è€Œä¸”ç”±äºçŸ­æ–‡æœ¬çš„é•¿åº¦é™åˆ¶ï¼Œç¨€ç–æ€§é—®é¢˜å­˜åœ¨äºæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒä»¥åŠä¸»é¢˜è¯åˆ†å¸ƒä¸­ã€‚</p>
<blockquote>
<p>In this paper we propose NPMM to address the above stated challenges, which is characterized as follows:</p>
</blockquote>
<ul>
<li>Online processing. NPMM has one-pass clustering process for each arriving document, which can naturally deal with the streaming data.</li>
<li>Nonparametric topic discovery. NPMM incorporates the word embeddings in the online topic discovery, to release the parameter settings for generating new topics in the processing.</li>
<li>Sparsity.</li>
</ul>
<p>NPMM åŠ å…¥è¯åµŒå…¥ç”¨äºä¸»é¢˜å‘ç°ï¼ŒNPMMå¯ä»¥è‡ªåŠ¨å‘ç°ä¸»é¢˜é€šè¿‡è®¡ç®—æ–°åˆ°è¾¾çš„æ–‡æ¡£å’Œç°æœ‰ä¸»é¢˜ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨å¡æ–¹åˆ†å¸ƒè®¡ç®—ç”Ÿæˆæ–°ä¸»é¢˜çš„æ¦‚ç‡ã€‚NPMMé€šè¿‡â€œspike and slabâ€ functionå‡å°‘ä¸»é¢˜è¯åˆ†å¸ƒçš„ç¨€ç–æ€§ã€‚</p>
<h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><blockquote>
<p>According to the general survey on the online data clustering in the stream clustering methods can be categorized into the following two groups: model-based stream clustering and threshold-based stream clustering. Moreover, the model-based methods can be further divided into two branches: DP-based models and batch-based models.</p>
</blockquote>
<p>æ ¹æ®æ–‡çŒ®ä¸­å…³äºåœ¨çº¿æ•°æ®èšç±»çš„ä¸€èˆ¬è°ƒæŸ¥ï¼Œæµèšç±»æ–¹æ³•å¯åˆ†ä¸ºä»¥ä¸‹ä¸¤ç±»ï¼šåŸºäºæ¨¡å‹çš„æµèšç±»å’ŒåŸºäºé˜ˆå€¼çš„æµèšç±»ã€‚ æ­¤å¤–ï¼ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºä¸¤ä¸ªåˆ†æ”¯ï¼šDP-based model å’Œbatch-based modelã€‚</p>
<h3 id="2-1-Model-based-stream-clustering"><a href="#2-1-Model-based-stream-clustering" class="headerlink" title="2.1 Model-based stream clustering"></a>2.1 Model-based stream clustering</h3><blockquote>
<p>Model-based stream clustering methods assume that documents are generated from probabilistic graphical models.</p>
</blockquote>
<p>åŸºäºæ¨¡å‹çš„æµèšç±»æ–¹æ³•äº§ç”Ÿä¸æ¦‚ç‡å›¾æ¨¡å‹ã€‚å®ƒä»¬é€šè¿‡ä¸»é¢˜è¯åˆ†å¸ƒå’Œæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒå¯¹æ½œåœ¨è¯­ä¹‰ç©ºé—´å»ºæ¨¡ï¼Œç„¶åä½¿ç”¨æ¨ç†æ–¹æ³•ä¼°è®¡æ¨¡å‹å‚æ•°ã€‚</p><p><br>batch-based model: ç»å…¸LDAå˜ä½“æ¨¡å‹ï¼Œå½“å­˜åœ¨ä¸€å®šçš„ä¸è¶³ï¼Œå³å®ƒä»¬è®¾ç½®äº†å›ºå®šçš„ä¸»é¢˜æ•°é‡ï¼ˆfixed-number),å›ºå®šæ•°å­—è®¾ç½®ä¸é€‚ç”¨äºæ•°æ®æµèšç±»ï¼Œæ— æ³•å¤„ç†ä¸»é¢˜æ¼”åŒ–é—®é¢˜ã€‚å› æ­¤æå‡ºäº†DP-based modelã€‚</p><p><br>DP-based model: åº”ç”¨å¹¿æ³›ï¼Œä½†å½“å‰åŸºäºDPçš„æ¨¡å‹çš„å±€é™æ€§åœ¨äºï¼Œæ½œåœ¨ä¸»é¢˜ç”Ÿæˆçš„å‚æ•°æ˜¯æ‰‹åŠ¨è®¾ç½®çš„ï¼Œå¹¶ä¸”æœç´¢è¯¥å‚æ•°çš„é€‚å½“å€¼éå¸¸è€—æ—¶ã€‚</p>
<h3 id="2-2-Threshold-based-stream-clustering"><a href="#2-2-Threshold-based-stream-clustering" class="headerlink" title="2.2 Threshold-based stream clustering"></a>2.2 Threshold-based stream clustering</h3><p>åŸºäºé˜ˆå€¼çš„æµèšç±»æ–¹æ³•çš„ä¸»è¦å±€é™æ€§åœ¨äºï¼Œå¯ä»¥æ‰‹åŠ¨è®¾ç½®é˜ˆå€¼ä»¥ç¡®å®šåœ¨çº¿æ–‡æ¡£çš„ä¸»é¢˜åˆ†é…ã€‚ä¾‹å¦‚ï¼Œæ–°åˆ°è¾¾çš„æ–‡æ¡£é€šè¿‡å°†è·ç¦»ä¸é¢„å®šä¹‰çš„é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒæ¥é€‰æ‹©æœ€è¿‘çš„ç¾¤é›†æˆ–æ–°çš„ç¾¤é›†ã€‚ä½†æ˜¯ï¼Œåœ¨ä¸åŒçš„æ•°æ®é›†ä¸­æœç´¢é€‚å½“çš„é˜ˆå€¼æ˜¯ä¸€é¡¹è€—æ—¶çš„æ“ä½œã€‚</p>
<h2 id="3-NPMM"><a href="#3-NPMM" class="headerlink" title="3. NPMM"></a>3. NPMM</h2><h3 id="3-1-1-Representative-terms"><a href="#3-1-1-Representative-terms" class="headerlink" title="3.1.1 Representative terms"></a>3.1.1 Representative terms</h3><p>NPMMä½¿ç”¨è¯åµŒå…¥åœ¨å…¨å±€è¯­ä¹‰ç©ºé—´ä¸­æ„é€ å‘ç°ä¸»é¢˜çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚å½“å‡ºç°æ–°æ–‡æ¡£æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—è¯¥æ–‡æ¡£ä¸é«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥è·å¾—ç”Ÿæˆæ–°ä¸»é¢˜çš„æ¦‚ç‡ã€‚ä½†æ˜¯ï¼Œä»…ä½¿ç”¨ç°æœ‰ä¸»é¢˜ä¸­çš„æ‰€æœ‰å•è¯æ¥æ„å»ºåŸºäºå•è¯åµŒå…¥çš„å…¨å±€é«˜æ–¯åˆ†å¸ƒå¯èƒ½ä¼šå¸¦æ¥å¾ˆå¤§çš„å™ªéŸ³ï¼Œå› ä¸ºæœ‰äº›å•è¯ä¸å…¶ä¸»é¢˜ä¹‹é—´çš„è”ç³»è¾ƒå¼±ã€‚</p>
<blockquote>
<p>microsoft sony battle gaming supremacy holiday season</p>
</blockquote>
<p>å…¶ä¸­holiday, seasonä¹Ÿæ˜¯ä¸»é¢˜å•è¯ï¼Œä½†æ˜¯æ ¸å¿ƒæ˜¯sony battle gamingï¼ŒTherefore, we select â€œmicrosoft sony battle gaming supremacyâ€ as the representative terms of this sentence.<br>æˆ‘ä»¬å¯ä»¥å¦‚ä¸‹è·å¾—ä¸»é¢˜ä¸­çš„representative terms:<br>$$\beta_{z,w} \sim Bernoulli(\lambda_{z,w})$$  </p>
<p>$$\lambda_{z,w} &#x3D; \frac{p(w|z)}{max_{wâ€™}p(wâ€™ï½œz)}, \qquad {\forall} wâ€™\in z \tag{1}$$</p>
<p>$\beta_{z,w}$ è¡¨ç¤ºå•è¯wæ˜¯å¦æ˜¯ä¸»é¢˜zä¸­çš„ä»£è¡¨è¯ï¼Œ$\lambda_{z,w}$ä¸<em>â€œç»™å®šå•è¯wçš„æ¡ä»¶å•è¯æ¦‚ç‡ä¸$p(wâ€™| z)$ä¸­çš„æœ€å¤§å€¼å³ä¸»é¢˜zçš„æœ€å¤§å•è¯æ¦‚ç‡çš„æ¯”å€¼â€</em>ç›¸å…³ã€‚</p>
<h3 id="3-1-2-Global-semantic-space"><a href="#3-1-2-Global-semantic-space" class="headerlink" title="3.1.2 Global semantic space"></a>3.1.2 Global semantic space</h3><blockquote>
<p>The global semantic space G is represented as a multivariate normal distribution in the word embedding space, which<br>is constructed by the representative terms from each topic during the clustering. </p>
</blockquote>
<p>å…¨å±€è¯­ä¹‰ç©ºé—´Gè¡¨ç¤ºä¸ºå•è¯åµŒå…¥ç©ºé—´ä¸­çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒ.</p>
<blockquote>
<p>In the scenario of social media streams, the newly coming topic is often outside the convex of those discovered topics, to be free of parameters and make the computation simple, we use just only one multivariate Gaussian to represent those discovered topics. </p>
</blockquote>
<p>åœ¨ç¤¾äº¤åª’ä½“æµçš„åœºæ™¯ä¸­ï¼Œæ–°å‡ºç°çš„ä¸»é¢˜é€šå¸¸ä¸åœ¨é‚£äº›å‘ç°çš„ä¸»é¢˜çš„å‡¸é¢ä¹‹å¤–ï¼Œå› ä¸ºæ²¡æœ‰å‚æ•°å¹¶ä¸”ç®€åŒ–äº†è®¡ç®—ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨ä¸€ä¸ªå¤šå…ƒé«˜æ–¯æ¥è¡¨ç¤ºé‚£äº›å‘ç°çš„ä¸»é¢˜ã€‚</p>
<p>When a new document d comes, the squared Mahalanobis distance between each word w in d and G is calculated as follows:<br><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F5.png" alt="global semantic space G"></p>
<h3 id="3-1-3-The-spike-and-slab-priors"><a href="#3-1-3-The-spike-and-slab-priors" class="headerlink" title="3.1.3 The spike and slab priors"></a>3.1.3 The spike and slab priors</h3><blockquote>
<p>The spike and slab priors introduced to topic modeling are able to address the sparsity of topic-word distribution by using auxiliary Bernoulli variables to present the â€œonâ€ and â€œoffâ€ of the priors, which indicate whether or not a term is selected by a topic as the representative term (or called the focused word). The selection process is given in Eq. (1). As in the real-world scenarios of short texts, a topic covers a narrow range of terms instead of a wide coverage of the vocabulary. It is hard for the sampling algorithm to distinguish relevant words from the irrelevant ones due to the small difference in word frequencies of short texts.</p>
</blockquote>
<p>å¼•å…¥ä¸»é¢˜å»ºæ¨¡çš„spike and slab priorsèƒ½å¤Ÿé€šè¿‡ä½¿ç”¨è¾…åŠ©Bernoulliå˜é‡æ¥è¡¨ç¤ºå…ˆéªŒçš„â€œå¼€â€å’Œâ€œå…³â€æ¥è§£å†³ä¸»é¢˜è¯åˆ†å¸ƒçš„ç¨€ç–æ€§ã€‚</p>
<h3 id="3-1-4-The-cluster-feature-CF-vector"><a href="#3-1-4-The-cluster-feature-CF-vector" class="headerlink" title="3.1.4 The cluster feature (CF) vector"></a>3.1.4 The cluster feature (CF) vector</h3><p>The cluster feature vector is used to represent a cluster, along with an addible property(å¯æ·»åŠ å±æ€§) and a deletable property(å¯åˆ é™¤å±æ€§).</p>
<p>CF vector for a cluster z is defined as a tuple <strong>(${n_z^w}, m_z, n_z$)</strong>, $n_z^w$æ˜¯å•è¯wåœ¨clusterç±»zä¸­å‡ºç°çš„æ¬¡æ•°(number of occurrences of word w), $m_z$æ˜¯cluster zä¸­documentsçš„æ•°é‡, $n_z$æ˜¯the number of words in cluster z.æˆ‘ä»¬ç”¨$D$è¡¨ç¤ºè®°å½•æ–‡æ¡£(recorded documents)çš„é›†åˆï¼Œ$V$è¡¨ç¤ºè®°å½•æ–‡æ¡£çš„è¯æ±‡çš„é›†åˆ(we denote $V$ as the set of vocabulary of the recorded documents).</p>
<ul>
<li>(a). Addible proberty</li>
</ul>
<blockquote>
<p>When a new document d comes, it will be firstly assigned to a topic z in the one-pass(éå†) clustering process, the detail is given in the later section. Then, the CF vector and the recorder sets($i.e.$, $D$ and $V$) are updated in the following way:</p>
</blockquote>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F3.png" alt="addible property"></p>
<p>å½“æœ‰ä¸€ä¸ªæ–°æ–‡æ¡£dæ—¶ï¼Œå°†åœ¨ä¸€æ¬¡éå†èšç±»è¿‡ç¨‹ä¸­é¦–å…ˆå°†å…¶åˆ†é…ç»™ä¸»é¢˜zï¼Œè¯¦ç»†ä¿¡æ¯åœ¨later section, ç„¶åæ›´æ–°CF vector å’Œ $D$ã€$V$ã€‚</p>
<ul>
<li>(b). Deletable property.<blockquote>
<p>Accordingly, when a document d is deleted from z in the update clustering process, the CF vector and the recorded sets are updated as follows:</p>
</blockquote>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F4.png" alt="deletable property"></p>
<p>After defining the above auxiliary components, the major tasks of NPMM given a data stream in a certain time range can be defined as(å³å½“ç»™å®šä¸€ä¸ªç¡®å®šæ—¶é—´æ®µæ•°æ®æµæ—¶ï¼ŒNPMMçš„ä¸»è¦ä»»åŠ¡å¦‚ä¸‹)ï¼š</p>
<ol>
<li>Sample a set of representative terms from the existing clusters(or called topics);ä»å·²æœ‰èšç±»ï¼ˆæˆ–ä¸»é¢˜ï¼‰ä¸­é‡‡æ ·æœ‰ä»£è¡¨æ€§çš„terms(æœ¯è¯­)ã€‚</li>
<li>Construct a semantic space G with the representative terms using a multivariate normal distributions.ä½¿ç”¨å¤šå…ƒæ­£æ€åˆ†å¸ƒæ„é€ å…·æœ‰representative termsçš„è¯­ä¹‰ç©ºé—´Gã€‚</li>
<li>Calculate the probability of a newly arriving document d belong to the semantic G. è®¡ç®—æ–°åˆ°è¾¾æ–‡æ¡£då±äºè¯­ä¹‰ç©ºé—´Gçš„æ¦‚ç‡ã€‚</li>
<li>Learned the topic-word distribution $\psi$ and document-topic distribution $\theta$, respectively(å„è‡ªåœ°). åˆ†åˆ«å­¦ä¹ ä¸»é¢˜è¯åˆ†å¸ƒ$\psi$å’Œæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ$\theta$ã€‚</li>
</ol>
<p>All the notations used in this paper are summarized in table 1.</p>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E7%AC%A6%E5%8F%B7%E8%A1%A8%E7%A4%BA.png" alt="ç¬¦å·è¡¨ç¤º table 1"></p>
<h3 id="3-2-Generative-process"><a href="#3-2-Generative-process" class="headerlink" title="3.2 Generative process"></a>3.2 Generative process</h3><p>åœ¨å®è·µä¸­ï¼Œeach document only focuses on one topic in short textså¾ˆæœ‰æ•ˆï¼Œæ‰€ä»¥æœ¬æ–‡æ¨¡å‹ä¹Ÿéµå¾ªè¯¥å‡è®¾ã€‚The graphical model of the proposed NPMM is given in Fig.1. </p>
<blockquote>
<p>In the generative process, we use the inverse Wishart distribution<a href="#refer-anchor"><sup>1</sup></a> as the conjugate prior  (å…±è½­å…ˆéªŒ)<a href="#refer-anchor"><sup>2</sup></a> for the covariance(åæ–¹å·®) matrix of a multivariate normal of distrubution.In Bayesian statistics, it is widely used for estimating a multivariate normal distribution with unknown mean and covariance matrix.</p>
</blockquote>
<p>åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Wisharté€†åˆ†å¸ƒ<a href="#refer-archor"><sup>1<sup></sup></sup></a>ç”¨ä½œå¤šå…ƒæ­£æ€åˆ†å¸ƒåæ–¹å·®çŸ©é˜µçš„å…±è½­å…ˆéªŒã€‚åœ¨è´å¶æ–¯ç»Ÿè®¡ä¸­ï¼Œå®ƒè¢«å¹¿æ³›ç”¨äºä¼°è®¡å‡å€¼å’Œåæ–¹å·®çŸ©é˜µæœªçŸ¥çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒã€‚</p>
<blockquote>
<p>Since a document is just related to one topic, there are two possible statuses for each newly arriving document in the clustering, either relevant or irrelevant to the existing global semantic space G.This status variable is denoted by $r$. In addition, $Î»_{z,w}$ is as defined in $Eq. (1)$ and other notations are summarized in Table 1.</p>
</blockquote>
<p>The generative process is presented as follows:<br><img src= "/img/loading.gif" data-lazy-src="/.com//model.png" alt="model"><br><img src= "/img/loading.gif" data-lazy-src="/.com//NPMM.png" alt="Fig. 1"></p>
<ol>
<li>$Draw \   \Sigma_0 \sim W^{-1}(\Psi, v)$.</li>
<li>$Draw \  \mu_0 \sim N(\mu, \frac{1}{\tau}\Sigma_0)$.</li>
<li>$Draw \  a \  topic \  distribution \ \theta \sim Dirichlet(x)$</li>
<li>$For \  each \  term \ z \in {1,2,â€¦,|z|}$:<br>$\quad(a) \  For\ each \ term \ w \in {1,2,â€¦,|V|}$:<br>$\qquad i.\ Draw \ a \ focused \ term \ \beta_{z,w} \sim Bernoulli(\lambda_{z,w})$<br>$\quad(b)\ Draw\ a\ word\ distribution\ \psi_z \sim\ Dirichlet(\delta\beta_z +\epsilon1),\beta_z &#x3D;{\beta_z,w}_{w&#x3D;1}^{|V|}.$</li>
<li>$For \ each \ document \ d \in {1,2,â€¦}$:<br>$\quad (a)\ Draw \ \eta_d \sim \chi_{dim}^2(\mu_0, \Sigma_0)$.<br>$\quad (b)\  Draw \ relevance \ r\  base \  on\  the\  representative \ term \ indicator \ \pi \ and\  p(\eta_d)\ (refer \ to\ Eqs.(3)\ and\ (4)).$<br>$\quad (c)\ If\ the\ document\ is\ relevant\ to\ the\ global\ semantic\ space\ G,\ i.e.,\ r\ &#x3D;\ 1$:<br>$\qquad i.\ Draw\ a\ topic\ z\ \sim \ Multinomial(\theta).$<br>$\qquad ii.\ Emit\ a\ word\ w \sim Multinomial(\psi_z).$<br>$\quad (d)\ If\ the\ document\ is\ irrelevant\ to\ the\ global\ semantic\ space\ G,\ i.e.,\ r\ &#x3D;\ 0$:<br>$\qquad i.\ Create\ a\ new\ topic\ z.$</li>
</ol>
<p>Here is a example:</p>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//example.png" alt="example"></p>
<h3 id="3-3-The-spark-and-slab-priors"><a href="#3-3-The-spark-and-slab-priors" class="headerlink" title="3.3 The spark and slab priors"></a>3.3 The spark and slab priors</h3><blockquote>
<p>When document d is sampled as relevant to the global semantic space G, we draw a topic assignment for it and apply the spike and slab priors on its word generation. The key idea of employing the priors is to restrict the size of the word simplex over Dirichlet distribution to reduce the sparsity in the topic-word distribution by incorporating a Bernoulli variable.</p>
</blockquote>
<p>å½“æ–‡æ¡£dè¢«é‡‡æ ·ä¸ºè¯­ä¹‰ç©ºé—´Gç›¸å…³æ—¶ï¼Œæˆ‘ä»¬ä¸ºå…¶ç»˜åˆ¶ä¸€ä¸ªä¸»é¢˜åˆ†é…ï¼ˆtopic assignmentï¼‰ï¼Œå¹¶åœ¨å•è¯ç”Ÿæˆè¿‡ç¨‹ä¸­åº”ç”¨<strong>spark and slab priors</strong>ã€‚é‡‡ç”¨å…ˆéªŒçš„å…³é”®æ€æƒ³æ˜¯é€šè¿‡åˆå¹¶Bernoulliå˜é‡æ¥é™åˆ¶å•è¯åœ¨Dirichletåˆ†å¸ƒä¸Šçš„å¤§å°ï¼Œä»¥æ­¤æ¥è¾ƒå°‘ä¸»é¢˜å•è¯åˆ†å¸ƒçš„ç¨€ç–æ€§ï¼ˆthe sparsity in the topic-word distribution)ã€‚</p>
<p>The spark prior $\delta$ is much larger than the slab prior $\epsilon$, that is <strong>$\delta \gg \epsilon$</strong>. The representative term indicator $\beta_{z,w}$ serves as a switch of â€œonâ€ and â€œoffâ€ to determine which term is a representative term,å³ <strong>$\beta_{z,w}$ç”¨ä½œæŒ‡ç¤ºå™¨ï¼Œç¡®å®šå“ªä¸ªtermæ˜¯ä»£è¡¨æ€§çš„</strong>ã€‚</p>
<p>In the previous case, when $r &#x3D; 1$, a topic $z$ is sampled from $\theta$, and a word is emitted from $\psi_z \sim Dirichlet(\delta\beta_{z}+\epsilon1)$. This does not violate(è¿å) the definition of a multinomial distribution(å¤šé¡¹å¼åˆ†å¸ƒ) as $\sum_{w&#x3D;1}^{|V|}\psi_{z,w}&#x3D;1$ where $|V|$ is the size of the vocabulary of current recorded documents. å½“r&#x3D;1æ—¶ï¼Œä»$\theta$é‡‡æ ·$z$, ä»$\psi_z \sim Dirichlet(\delta\beta_{z}+\epsilon1)$é‡‡æ ·å‡ºå•è¯ã€‚$|V|$æ˜¯å½“å‰è®°å½•æ–‡æ¡£çš„è¯æ±‡é‡ã€‚</p>
<blockquote>
<p>The spike-and-slab prior setting w&#x3D;1 enables the model to generate more coherent topics. Besides, when r &#x3D; 0 that d is sampled as irrelevant to G, we create a new topic for d and also update the global space G as well.</p>
</blockquote>
<p>spike-and-slab prioré¢„å…ˆè®¾ç½®w &#x3D; 1ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´è¿è´¯çš„ä¸»é¢˜ã€‚æ­¤å¤–ï¼Œå½“r &#x3D; 0æ—¶ï¼Œé‡‡æ ·åˆ°çš„dä¸Gä¸ç›¸å…³ï¼Œæˆ‘ä»¬ä¸ºdåˆ›å»ºäº†ä¸€ä¸ªæ–°ä¸»é¢˜ï¼Œå¹¶ä¸”è¿˜æ›´æ–°äº†å…¨å±€ç©ºé—´Gã€‚</p>
<h3 id="3-4-The-NPMM-algorithms"><a href="#3-4-The-NPMM-algorithms" class="headerlink" title="3.4 The NPMM algorithms"></a>3.4 The NPMM algorithms</h3><blockquote>
<p>The details of the clustering process for the proposed NPMM model are shown in Algorithms 1 and 2 . The meanings of the notations are as given in Table 1. As mentioned in the introduction section, NPMM has one-pass clustering process and update clustering process. The one-pass scheme (lines 5â€“8 in Algorithm 1) grants NPMM the instant processing capability to handle the massive amount of short text streams. And the update clustering scheme (or called batch scheme) (lines 9â€“25 in Algorithm 1) enables NPMM to achieve a better performance through multiple iterations. In addition, Algorithm 2 is a common operation for sampling the latent variables (details in the following section) in NPMM.</p>
</blockquote>
<p>ç®—æ³•1å’Œ2ä¸­æ˜¾ç¤ºäº†æ‰€æå‡ºçš„NPMMæ¨¡å‹çš„èšç±»è¿‡ç¨‹çš„è¯¦ç»†ä¿¡æ¯ã€‚ è¡¨1ç»™å‡ºäº†è¿™äº›ç¬¦å·çš„å«ä¹‰ã€‚å¦‚å¼•è¨€éƒ¨åˆ†æ‰€è¿°ï¼ŒNPMMå…·æœ‰ä¸€éèšç±»è¿‡ç¨‹å’Œæ›´æ–°èšç±»è¿‡ç¨‹ã€‚ one-passæ–¹æ¡ˆï¼ˆç®—æ³•1ä¸­çš„ç¬¬5-8è¡Œï¼‰æˆäºˆNPMMå³æ—¶å¤„ç†èƒ½åŠ›ï¼Œä»¥å¤„ç†å¤§é‡çš„çŸ­æ–‡æœ¬æµã€‚ æ›´æ–°èšç±»æ–¹æ¡ˆï¼ˆæˆ–ç§°ä¸ºæ‰¹å¤„ç†æ–¹æ¡ˆï¼‰ï¼ˆç®—æ³•1ä¸­çš„9â€“25è¡Œï¼‰ä½¿NPMMå¯ä»¥é€šè¿‡å¤šæ¬¡è¿­ä»£è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ å¦å¤–ï¼Œç®—æ³•2æ˜¯ç”¨äºåœ¨NPMMä¸­å¯¹æ½œåœ¨å˜é‡ï¼ˆä¸‹ä¸€èŠ‚ä¸­çš„è¯¦ç»†ä¿¡æ¯ï¼‰è¿›è¡Œé‡‡æ ·çš„é€šç”¨æ“ä½œã€‚</p>
<h2 id="4-Inference"><a href="#4-Inference" class="headerlink" title="4. Inference"></a>4. Inference</h2><p>å¯¹å‚æ•°æ¨ç†ä½¿ç”¨Gibbs sampling<a href="#refer-anchor"><sup>3</sup></a>ã€‚æ¨¡å‹ä¸­æœ‰5ä¸ªlatent variablesï¼ŒåŒ…æ‹¬topic assignments $z$, relevance indicator $r$, the posterior normal distribution parameters $\mu_0$ and $\Sigma_0$, and the focused terms $\beta_z$ in each topic.</p>
<p><img src= "/img/loading.gif" data-lazy-src="/.com//%E5%85%AC%E5%BC%8F7.png" alt="å…¬å¼7"><br><img src= "/img/loading.gif" data-lazy-src="/.com//A1.png" alt="Algorithm1"><br><img src= "/img/loading.gif" data-lazy-src="/.com//S1.png" alt="Sampling1"> <img src= "/img/loading.gif" data-lazy-src="/.com//s1_2.png" alt="Sampling2"></p>
<h3 id="4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition"><a href="#4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition" class="headerlink" title="4.1. Reduction of the sampling complexity with cholesky decomposition"></a>4.1. Reduction of the sampling complexity with cholesky decomposition</h3><p><img src= "/img/loading.gif" data-lazy-src="/.com//4.png" alt="4"><br><img src= "/img/loading.gif" data-lazy-src="/.com//4_1.png" alt="4.1"></p>
<h3 id="4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step"><a href="#4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step" class="headerlink" title="4.2. Improved reduction of the sampling complexity with metropolis hastings step"></a>4.2. Improved reduction of the sampling complexity with metropolis hastings step</h3><blockquote>
<p>The Metropolis Hastings (MH) algorithm is a Markov chain Monte Carlo method for obtaining a certain number of random samples from a stale probability distribution. The precondition of using MH is that the objective distribution changes relatively slowly and its changing distribution is similar to the stale one. From the observation of the parameters ($\mu_0$ and $\Sigma_0$) in the global space G, they do not change drastically during the clustering. One possible explanation is that in the real world, the number and the content of topics are relatively stable in a certain time window (e.g., people always like to talk about the latest news on social media platforms). We can exploit this observation and employ MH step in our sampling process (i.e., obtaining a few samples with the stale distribution of the global space G).</p><p><br>Therefore, combining Cholesky decomposition with the MH step, the sampling complexity of calculation can be brought down from $O(IDn^2)$ to $O(I\frac{D}{H}n^2)$, where H represents the number of MH steps used in the clustering and $\frac{D}{H} \ll D$. The experiment results (i.e., sampling naively, with Cholesky decomposition (CH), with CH+MH step) of the running time will be demonstrated in the following experiment section.</p>
</blockquote>
<p>Metropolis Hastingsï¼ˆMHï¼‰<a href="#refer-anchor"><sup>4</sup></a>ç®—æ³•æ˜¯é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ–¹æ³•ï¼Œç”¨äºä»é™ˆæ—§çš„æ¦‚ç‡åˆ†å¸ƒä¸­è·å–ä¸€å®šæ•°é‡çš„éšæœºæ ·æœ¬ã€‚ä½¿ç”¨MHçš„å‰ææ˜¯ç›®æ ‡åˆ†å¸ƒå˜åŒ–ç›¸å¯¹è¾ƒæ…¢ï¼Œå¹¶ä¸”å…¶å˜åŒ–åˆ†å¸ƒä¸é™ˆæ—§çŠ¶æ€ç›¸ä¼¼ã€‚ä»å…¨å±€ç©ºé—´Gä¸­çš„å‚æ•°ï¼ˆÎ¼0å’Œô°‰0ï¼‰è§‚å¯Ÿï¼Œå®ƒä»¬åœ¨èšç±»æœŸé—´ä¸ä¼šå‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚ä¸€ç§å¯èƒ½çš„è§£é‡Šæ˜¯ï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œä¸»é¢˜çš„æ•°é‡å’Œå†…å®¹åœ¨ä¸€å®šçš„æ—¶é—´èŒƒå›´å†…ç›¸å¯¹ç¨³å®šï¼ˆä¾‹å¦‚ï¼Œäººä»¬æ€»æ˜¯å–œæ¬¢åœ¨ç¤¾äº¤åª’ä½“å¹³å°ä¸Šè°ˆè®ºæœ€æ–°æ–°é—»ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸€è§‚å¯Ÿç»“æœå¹¶åœ¨æˆ‘ä»¬çš„é‡‡æ ·è¿‡ç¨‹ä¸­é‡‡ç”¨MHæ­¥éª¤ï¼ˆå³è·å–ä¸€äº›å…·æœ‰å…¨çƒç©ºé—´Gçš„é™ˆæ—§åˆ†å¸ƒçš„æ ·æœ¬ï¼‰ã€‚<br>å› æ­¤ï¼Œå°†Choleskyåˆ†è§£ä¸MHæ­¥ç»“åˆèµ·æ¥ï¼Œå¯ä»¥å°†è®¡ç®—çš„é‡‡æ ·å¤æ‚åº¦ä»O$O(IDn^2)$ é™ä½åˆ°$O(I\frac{D}{H}n^2)$ï¼Œå…¶ä¸­Hä»£è¡¨åœ¨èšç±»ä¸­ä½¿ç”¨çš„MHæ­¥æ•°å’Œ$\frac{D}{H} \ll D$ã€‚è¿è¡Œæ—¶é—´çš„å®éªŒç»“æœï¼ˆå³ï¼Œé€šè¿‡Choleskyåˆ†è§£ï¼ˆCHï¼‰ï¼ŒCH + MHæ­¥è¿›è¡Œå¤©çœŸé‡‡æ ·ï¼‰å°†åœ¨ä»¥ä¸‹å®éªŒéƒ¨åˆ†ä¸­æ¼”ç¤ºã€‚</p>
<h2 id="5-Experiment"><a href="#5-Experiment" class="headerlink" title="5. Experiment"></a>5. Experiment</h2><h3 id="5-1-Datasets"><a href="#5-1-Datasets" class="headerlink" title="5.1 Datasets"></a>5.1 Datasets</h3><p>Two real-world datasets from Google News and Twitter, and two variants of them are used in the experimental study.<br><img src= "/img/loading.gif" data-lazy-src="/.com//dataset.png" alt="dataset"></p>
<blockquote>
<p>Google News <a target="_blank" rel="noopener" href="https://news.google.com/news/">https://news.google.com/news/</a><br>TweetSet <a target="_blank" rel="noopener" href="https://trec.nist.gov/data/microblog.html">https://trec.nist.gov/data/microblog.html</a></p>
</blockquote>
<h2 id="6-Conclustion-and-future-work"><a href="#6-Conclustion-and-future-work" class="headerlink" title="6. Conclustion and future work"></a>6. Conclustion and future work</h2><blockquote>
<p>In this paper, we have proposed a nonparametric topic model (NPMM) with auxiliary word embeddings for online topic discovery. NPMM can discover a new topic by computing the probabilities of a document belonging to the existing topics and a new one. NPMM can achieve the state-of-the-art performance of online clustering with one-pass process, and can have even better performance with multiple iterations. Moreover, after obtaining the representative terms from each topic, NPMM can exploit the spike and slab priors function to amplify the contrast of word generation probabilities between the relevant words and the irrelevant ones, which alleviates the sparsity problem of the topic-word distribution in the short text clustering. In addition, in order to speed up the sampling process, we have proposed two improved sampling methods (i.e., NPMM with CH and NPMM with CH+MH step) to compare with the naive sampling one. Our extensive experimental study has shown that NPMM with CH+MH100 step can achieve similar performance in less time compared with the naive sampling method on real-life datasets.</p>
</blockquote>
<p>In the future work, we intend to exploit NPMM to improve the performance of other text mining applications, such as event detection, search result diversification, and text classification.</p>
<div id="refer-anchor"></div>

<h3 id="å‚è€ƒèµ„æ–™"><a href="#å‚è€ƒèµ„æ–™" class="headerlink" title="å‚è€ƒèµ„æ–™"></a>å‚è€ƒèµ„æ–™</h3><p>[0] A, Junyang Chen , Z. G. A , and W. L. B . â€œA nonparametric model for online topic discovery with word embeddings.â€ Information sciences 504(2019):32-47.<br><a href="/2020/11/30/Wishart-%E5%88%86%E5%B8%83%E5%8F%8A%E9%80%86%E5%88%86%E5%B8%83/">[1] Wishartåˆ†å¸ƒä¸Wisharté€†åˆ†å¸ƒ</a><br><a href="/2020/11/30/%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83/">[2] å…±è½­åˆ†å¸ƒ</a><br>[3] H. Amoualian, M. Clausel, E. Gaussier, M.-R. Amini, Streaming-LDA: A copula-based approach to modeling topic dependencies in document streams, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2016, pp. 695â€“704. (Gipps Sampling)<br>[4]Â S. Chib, E. Greenberg, Understanding the metropolis-hastings algorithm, Am. Stat. 49 (4) (1995) 327â€“335. (MHé©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ–¹æ³•)</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="mailto:undefined">cgshuo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="http://www.cgsgo.com/2020/11/19/NPMM/">http://www.cgsgo.com/2020/11/19/NPMM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="http://www.cgsgo.com" target="_blank">Cgsâ˜›go</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Dynamic-Clustering/">Dynamic Clustering</a><a class="post-meta__tags" href="/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">ç”Ÿæˆæ¨¡å‹</a><a class="post-meta__tags" href="/tags/Clustering-Topic-model/">Clustering Topic model</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/11/30/8wo7MIT9BkfCQXt.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> æ‰“èµ<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" data-lazy-src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" data-lazy-src="/img/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/23/array%E4%B8%89%E7%BB%B4%E6%8B%BC%E4%BA%8C%E7%BB%B4/"><img class="prev-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">arrayä¸‰ç»´æ‹¼äºŒç»´</div></div></a></div><div class="next-post pull-right"><a href="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/"><img class="next-cover" data-lazy-src="https://i.loli.net/2020/11/30/t8TzPrwuCnA75mv.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">Dynamic Clustering of Streaming Short</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2020/11/16/Dynamic-Clustering-of-Streaming-Short/" title="Dynamic Clustering of Streaming Short"><img class="cover" data-lazy-src="https://i.loli.net/2020/11/30/t8TzPrwuCnA75mv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-16</div><div class="title">Dynamic Clustering of Streaming Short</div></div></a></div><div><a href="/2021/01/21/Householder-flow/" title="Householder flow"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-21</div><div class="title">Householder flow</div></div></a></div><div><a href="/2020/11/25/LDA/" title="LDA"><img class="cover" data-lazy-src="https://i.loli.net/2020/11/30/roijB1RqF47mtAy.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-25</div><div class="title">LDA</div></div></a></div><div><a href="/2021/01/23/lstm-vaeè°ƒç ”/" title="lstm-vaeè°ƒç ”"><img class="cover" data-lazy-src="https://i.loli.net/2020/11/30/J1OKpd6fINCUzPb.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-23</div><div class="title">lstm-vaeè°ƒç ”</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" data-lazy-src="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">cgshuo</div><div class="author-info__description">çƒ­çˆ±ç”Ÿæ´»ã€æ‹¥æŠ±ä¸–ç•Œï¼Œè„šè¸å®åœ°ã€ä¸å¿˜åˆå¿ƒã€‚ğŸ˜Š</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">84</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">62</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">17</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cgshuo"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cgshuo" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:cgshuo@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>å…¬å‘Š</span></div><div class="announcement_content">å¿«æ¥å’Œæˆ‘ä¸€èµ·å­¦ä¹ å•¦ï¼ï¼ï¼</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#A-nonparametric-model-for-online-topic-discovery-with-word-embeddings"><span class="toc-number">1.</span> <span class="toc-text">A nonparametric model for online topic discovery with word embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">2.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">3.</span> <span class="toc-text">1. Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Batch-based-models"><span class="toc-number">3.1.</span> <span class="toc-text">1.1 Batch-based models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-DP-based-models"><span class="toc-number">3.2.</span> <span class="toc-text">1.2 DP-based models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-summarization"><span class="toc-number">3.3.</span> <span class="toc-text">1.3 summarization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-work"><span class="toc-number">4.</span> <span class="toc-text">2. Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Model-based-stream-clustering"><span class="toc-number">4.1.</span> <span class="toc-text">2.1 Model-based stream clustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Threshold-based-stream-clustering"><span class="toc-number">4.2.</span> <span class="toc-text">2.2 Threshold-based stream clustering</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-NPMM"><span class="toc-number">5.</span> <span class="toc-text">3. NPMM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-Representative-terms"><span class="toc-number">5.1.</span> <span class="toc-text">3.1.1 Representative terms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-Global-semantic-space"><span class="toc-number">5.2.</span> <span class="toc-text">3.1.2 Global semantic space</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-The-spike-and-slab-priors"><span class="toc-number">5.3.</span> <span class="toc-text">3.1.3 The spike and slab priors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-The-cluster-feature-CF-vector"><span class="toc-number">5.4.</span> <span class="toc-text">3.1.4 The cluster feature (CF) vector</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Generative-process"><span class="toc-number">5.5.</span> <span class="toc-text">3.2 Generative process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-The-spark-and-slab-priors"><span class="toc-number">5.6.</span> <span class="toc-text">3.3 The spark and slab priors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-The-NPMM-algorithms"><span class="toc-number">5.7.</span> <span class="toc-text">3.4 The NPMM algorithms</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Inference"><span class="toc-number">6.</span> <span class="toc-text">4. Inference</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Reduction-of-the-sampling-complexity-with-cholesky-decomposition"><span class="toc-number">6.1.</span> <span class="toc-text">4.1. Reduction of the sampling complexity with cholesky decomposition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Improved-reduction-of-the-sampling-complexity-with-metropolis-hastings-step"><span class="toc-number">6.2.</span> <span class="toc-text">4.2. Improved reduction of the sampling complexity with metropolis hastings step</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiment"><span class="toc-number">7.</span> <span class="toc-text">5. Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Datasets"><span class="toc-number">7.1.</span> <span class="toc-text">5.1 Datasets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Conclustion-and-future-work"><span class="toc-number">8.</span> <span class="toc-text">6. Conclustion and future work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">8.1.</span> <span class="toc-text">å‚è€ƒèµ„æ–™</span></a></li></ol></li></ol></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2022 By cgshuo</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script></div></body></html>